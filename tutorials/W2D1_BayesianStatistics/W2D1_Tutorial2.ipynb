{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/master/tutorials/W2D1_BayesianStatistics/W2D1_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Neuromatch Academy: Week 3, Day 1, Tutorial 2\n",
    "# Bayesian inference and decisions with continuous hidden state\n",
    "\n",
    "__Content creators:__ Eric DeWitt, Xaq Pitkow, Saeed Salehi, Ella Betty\n",
    "\n",
    "__Content reviewers:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Tutorial Objectives\n",
    "\n",
    "This notebook introduces you to Gaussians and Bayes' rule for continuous distributions, allowing us to model simple put powerful combinations of prior information and new measurements. In this notebook you will work through the same ideas we explored in the binary state tutorial, but you will be introduced to a new problem: finding and guiding Astrocat! You will see this problem again in more complex ways in the following days.\n",
    "\n",
    "In this notebook, you will:\n",
    "1. Learn more about the problem setting, which we wil also use in Tutorial 3,\n",
    "2. Implement a mixture-of-Gaussian prior, and\n",
    "3. Explore how that prior produces more complex posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "colab_type": "code",
    "outputId": "422497ee-b213-4f13-db03-85476ca822b7"
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Introduction\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id='GdIwJWsW9-s', width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "##Setup  \n",
    "Please execute the cells below to initialize the notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "#@title Figure Settings\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "\n",
    "def my_gaussian(x_points, mu, sigma):\n",
    "    \"\"\"\n",
    "    DO NOT EDIT THIS FUNCTION !!!\n",
    "\n",
    "    Returns normalized Gaussian estimated at points `x_points`, with parameters `mu` and `sigma`\n",
    "\n",
    "    Args:\n",
    "      x_points (numpy array of floats) - points at which the gaussian is evaluated\n",
    "      mu (scalar) - mean of the Gaussian\n",
    "      sigma (scalar) - standard deviation of the gaussian\n",
    "    Returns:\n",
    "      (numpy array of floats): normalized Gaussian (i.e. without constant) evaluated at `x`\n",
    "    \"\"\"\n",
    "    px = np.exp(- 1/2/sigma**2 * (mu - x_points) ** 2)\n",
    "\n",
    "    px = px / px.sum() # this is the normalization part with a very strong assumption, that\n",
    "                       # x_points cover the big portion of probability mass around the mean.\n",
    "                       # Please think/discuss when this would be a dangerous assumption.\n",
    "\n",
    "    return px\n",
    "\n",
    "def plot_mixture_prior(x, gaussian1, gaussian2, combined):\n",
    "    \"\"\"\n",
    "    DO NOT EDIT THIS FUNCTION !!!\n",
    "\n",
    "    Plots a prior made of a mixture of gaussians\n",
    "\n",
    "    Args:\n",
    "      x (numpy array of floats):         points at which the likelihood has been evaluated\n",
    "      gaussian1 (numpy array of floats): normalized probabilities for Gaussian 1 evaluated at each `x`\n",
    "      gaussian2 (numpy array of floats): normalized probabilities for Gaussian 2 evaluated at each `x`\n",
    "      posterior (numpy array of floats): normalized probabilities for the posterior evaluated at each `x`\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, gaussian1, '--b', LineWidth=2, label='Gaussian 1')\n",
    "    ax.plot(x, gaussian2, '-.b', LineWidth=2, label='Gaussian 2')\n",
    "    ax.plot(x, combined, '-r', LineWidth=2, label='Gaussian Mixture')\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_xlabel('Orientation (Degrees)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Section 1: The Gaussian Distribution\n",
    "\n",
    "Bayesian analysis operates on probability distributions. Although these can take many forms, the Gaussian distribution is a very common choice. Because of the central limit theorem, many quantities are Gaussian-distributed. Gaussians also have some mathematical properties that permit simple closed-form solutions to several important problems. \n",
    "\n",
    "In this exercise, you will implement a Gaussian by filling in the missing portion of `my_gaussian` below. Gaussians have two parameters. The **mean** $\\mu$, which sets the location of its center. Its \"scale\" or spread is controlled by its **standard deviation** $\\sigma$ or its square, the **variance** $\\sigma^2$. (Be careful not to use one when the other is required). \n",
    "\n",
    "The equation for a Gaussian is:\n",
    "$$\n",
    "\\mathcal{N}(\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "Also, don't forget that this is a probability distribution and should therefore sum to one. While this happens \"automatically\" when integrated from $-\\infty$ to $\\infty$, your version will only be computed over a finite number of points. You therefore need to explicitly normalize it yourself. \n",
    "\n",
    "Test out your implementation with a $\\mu = -1$ and $\\sigma = 1$. After you have it working, play with the  parameters to develop an intuition for how changing $\\mu$ and $\\sigma$ alter the shape of the Gaussian. This is important, because subsequent exercises will be built out of Gaussians. "
   ]
  },
  {
   "source": [
    "# Section 1: The Cost Functions\n",
    "\n",
    "Next, we will implement the cost functions. \n",
    "A cost function determines the \"cost\" (or penalty) of estimating $\\hat{x}$ when the true or correct quantity is really $x$ (this is essentially the cost of the error between the true stimulus value: $x$ and our estimate: $\\hat x$ -- Note that the error can be defined in different ways):\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\textrm{Mean Squared Error} &=& (x - \\hat{x})^2 \\\\ \n",
    "\\textrm{Absolute Error} &=& \\big|x - \\hat{x}\\big| \\\\ \n",
    "\\textrm{Zero-One Loss} &=& \\begin{cases}\n",
    "                            0,& \\text{if } x = \\hat{x} \\\\\n",
    "                            1,              & \\text{otherwise}\n",
    "                            \\end{cases}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "In the cell below, fill in the body of these cost function. Each function should take one single value for $x$ (the true stimulus value : $x$) and one or more possible value estimates: $\\hat{x}$. \n",
    "\n",
    "Return an array containing the costs associated with predicting $\\hat{x}$ when the true value is $x$. Once you have written all three functions, uncomment the final line to visulize your results.\n",
    "\n",
    " _Hint:_ These functions are easy to write (1 line each!) but be sure *all* three functions return arrays of `np.float` rather than another data type."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "vA posterior distribution tells us about the confidence or credibility we assign to different choices. A cost function describes the penalty we incur when choosing an incorrect option. These concepts can be combined into an *expected loss* function. Expected loss is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "    \\mathbb{E}[\\text{Loss} | \\hat{x}] = \\int L[\\hat{x},x] \\odot  p(x|\\tilde{x}) dx\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where $L[ \\hat{x}, x]$ is the loss function, $p(x|\\tilde{x})$ is the posterior, and $\\odot$ represents the [Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) (i.e., elementwise multiplication), and $\\mathbb{E}[\\text{Loss} | \\hat{x}]$ is the expected loss. \n",
    "\n",
    "In this exercise, we will calculate the expected loss for the: means-squared error, the absolute error, and the zero-one loss over our bimodal posterior $p(x | \\tilde x)$. \n",
    "\n",
    "**Suggestions:**\n",
    "* We already pre-completed the code (commented-out) to calculate the mean-squared error, absolute error, and zero-one loss between $x$ and an estimate $\\hat x$ using the functions you created in exercise 1\n",
    "* Calculate the expected loss ($\\mathbb{E}[MSE Loss]$) using your posterior (imported above as `posterior`) & each of the loss functions described above (MSELoss, ABSELoss, and Zero-oneLoss).\n",
    "* Find the x position that minimizes the expected loss for each cost function and plot them using the `loss_plot` function provided (commented-out)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Section 2: Information and marginalization\n",
    "\n",
    "In this section we will explore correlated Gaussin variables to think about information sharing. Then we will explore how we can marginalize over such a distribution. Finally, we will think about marginalization when two distributions are multiplied."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Section 3: Bayes' theorem for continuous distributions\n",
    "\n",
    "The Gausian example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "\n",
    "Bayes' rule tells us how to combine two sources of information: the prior (e.g., a noisy representation of our expectations about where the stimulus might come from) and the likelihood (e.g., a noisy representation of the stimulus position on a given trial), to obtain a posterior distribution taking into account both pieces of information. Bayes' rule states:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\text{Posterior} = \\frac{ \\text{Likelihood} \\times \\text{Prior}}{ \\text{Normalization constant}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "When both the prior and likelihood are Gaussians, this translates into the following form:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\text{Likelihood} &=& \\mathcal{N}(\\mu_{likelihood},\\sigma_{likelihood}^2) \\\\\n",
    "\\text{Prior} &=& \\mathcal{N}(\\mu_{prior},\\sigma_{prior}^2) \\\\\n",
    "\\text{Posterior} &\\propto& \\mathcal{N}(\\mu_{likelihood},\\sigma_{likelihood}^2) \\times \\mathcal{N}(\\mu_{prior},\\sigma_{prior}^2) \\\\\n",
    "&&= \\mathcal{N}\\left( \\frac{\\sigma^2_{likelihood}\\mu_{prior}+\\sigma^2_{prior}\\mu_{likelihood}}{\\sigma^2_{likelihood}+\\sigma^2_{prior}}, \\frac{\\sigma^2_{likelihood}\\sigma^2_{prior}}{\\sigma^2_{likelihood}+\\sigma^2_{prior}} \\right) \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In these equations, $\\mathcal{N}(\\mu,\\sigma^2)$ denotes a Gaussian distribution with parameters $\\mu$ and $\\sigma^2$:\n",
    "$$\n",
    "\\mathcal{N}(\\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\; \\exp \\bigg( \\frac{-(x-\\mu)^2}{2\\sigma^2} \\bigg)\n",
    "$$\n",
    "\n",
    "In Exercise 2A, we will use the first form of the posterior, where the two distributions are combined via pointwise multiplication.  Although this method requires more computation, it works for any type of probability distribution. In Exercise 2B, we will see that the closed-form solution shown on the line below produces the same result. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Section 3: Exploring Priors\n",
    "\n",
    "In the previous tutorial, you learned how to create a single Gaussian prior that could represent one of these possibilties. A broad Gaussian with a large $\\sigma$ could represent sounds originating from nearly anywhere, while a narrow Gaussian with $\\mu$ near zero could represent sounds orginating from the puppet. \n",
    "\n",
    "Here, we will combine those into a mixure-of-Gaussians probability density function (PDF) that captures both possibilties. We will control how the Gaussians are mixed by summing them together with a 'mixing' or weight parameter $p_{common}$, set to a value between 0 and 1, like so:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    \\text{Mixture} = \\bigl[\\; p_{common} \\times \\mathcal{N}(\\mu_{common},\\sigma_{common}) \\; \\bigr] + \\bigl[ \\;\\underbrace{(1-p_{common})}_{p_{independent}} \\times \\mathcal{N}(\\mu_{independent},\\sigma_{independent}) \\; \\bigr]\n",
    "\\end{eqnarray}\n",
    "\n",
    "$p_{common}$ denotes the probability that auditory stimulus shares a \"common\" source with the learnt visual input; in other words, the probability that the \"puppet\" is speaking. You might think that we need to include a separate weight for the possibility that sound is \"independent\" from the puppet. nHowever, since there are only two, mutually-exclusive possibilties, we can replace $p_{independent}$ with $(1 - p_{common})$ since, by the law of total probability, $p_{common} + p_{independent}$ must equal one. \n",
    "\n",
    "Using the formula above, complete the code to build this mixture-of-Gaussians PDF: \n",
    "* Generate a Gaussian with mean 0 and standard deviation 0.5 to be the 'common' part of the Gaussian mixture prior. (This is already done for you below).\n",
    "* Generate another Gaussian with mean 0 and standard deviation 3 to serve as the 'independent' part. \n",
    "* Combine the two Gaussians to make a new prior by mixing the two Gaussians with mixing parameter $p_{common}$ = 0.75 so that the peakier \"common-cause\" Gaussian has 75% of the weight. Don't forget to normalize afterwards! \n",
    "\n",
    "Hints:\n",
    "* Code for the `my_gaussian` function from Tutorial 1 is available for you to use. Its documentation is below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Helper function(s)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "outputId": "5e888a13-7abb-4cbc-a623-ae600d677ead"
   },
   "outputs": [],
   "source": [
    "help(my_gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Exercise 1: Implement the prior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def mixture_prior(x, mean=0, sigma_common=0.5, sigma_independent=3, p_common=0.75):\n",
    "\n",
    "  ###############################################################################\n",
    "  ## Insert your code here to:\n",
    "  #   * Create a second gaussian representing the independent-cause component\n",
    "  #   * Combine the two priors, using the mixing weight p_common. Don't forget\n",
    "  #      to normalize the result so it remains a proper probability density function\n",
    "  #\n",
    "  #   * Comment the line below to test out your function\n",
    "  raise NotImplementedError(\"Please complete Exercise 1\")\n",
    "  ###############################################################################\n",
    "\n",
    "  gaussian_common = my_gaussian(x, mean, sigma_common)\n",
    "  gaussian_independent = ...\n",
    "  mixture = ...\n",
    "\n",
    "  return gaussian_common, gaussian_independent, mixture\n",
    "\n",
    "\n",
    "x = np.arange(-10, 11, 0.1)\n",
    "\n",
    "# Uncomment the lines below to visualize out your solution\n",
    "# common, independent, mixture = mixture_prior(x)\n",
    "# plot_mixture_prior(x, common, independent, mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "colab_type": "code",
    "outputId": "371c906d-ddd0-42f4-bd7d-6170c8818f94"
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "def mixture_prior(x, mean=0, sigma_common=0.5, sigma_independent=3, p_common=0.75):\n",
    "\n",
    "  gaussian_common = my_gaussian(x, mean, sigma_common)\n",
    "  ###############################################################################\n",
    "  ## Insert your code here to:\n",
    "  ##   * Create a second gaussian representing the independent-cause component\n",
    "  ##   * Combine the two priors, using the mixing weight p_common. Don't forget\n",
    "  #      to normalize the result so it remains a proper probability density function\n",
    "  #\n",
    "  #    * Comment the line below to test out your function\n",
    "  #raise NotImplementedError(\"Please complete Exercise 1\")\n",
    "  ###############################################################################\n",
    "  gaussian_independent = my_gaussian(x, mean, sigma_independent)\n",
    "\n",
    "  mixture = p_common * gaussian_common + ((1-p_common) * gaussian_independent)\n",
    "  mixture = mixture / np.sum(mixture)\n",
    "\n",
    "  return gaussian_common, gaussian_independent, mixture\n",
    "\n",
    "\n",
    "x = np.arange(-10, 11, 0.1)\n",
    "\n",
    "# Uncomment the lines below to visualize out your solution\n",
    "common, independent, mixture = mixture_prior(x)\n",
    "with plt.xkcd():\n",
    "  plot_mixture_prior(x, common, independent, mixture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "\n",
    "# Section 3: Bayes Theorem with Complex Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "colab_type": "code",
    "outputId": "79136cf7-30de-475f-83ba-29887aef24fb"
   },
   "outputs": [],
   "source": [
    "#@title Video 2: Mixture-of-Gaussians and Bayes' Theorem\n",
    "video = YouTubeVideo(id='LWKM35te0WI', width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now that we have created a mixture of Gaussians prior that embodies the participants' expectations about sound location, we want to compute the posterior probability, which represents the subjects' beliefs about a specific sound's origin. \n",
    "\n",
    "To do so we will compute the posterior by using *Bayes Theorem* to combine the mixture-of-gaussians prior and varying auditory Gaussian likelihood. This works exactly the same as in Tutorial 1: we simply multiply the prior and likelihood pointwise, then normalize the resulting distribution so it sums to 1. (The closed-form solution from Exercise 2B, however, no longer applies to this more complicated prior). \n",
    "\n",
    "Here, we provide you with the code mentioned in the video (lucky!). Instead, use the interactive demo to explore how a mixture-of-Gaussians prior and Gaussian likelihood interact. For simplicity, we have fixed the prior mean to be zero. We also recommend starting with same other prior parameters used in Exercise 1: $\\sigma_{common} = 0.5, \\sigma_{independent} = 3, p_{common}=0.75$; vary the likelihood instead. \n",
    "\n",
    "Unlike the demo in Tutorial 1, you should see several qualitatively different effects on the posterior, depending on the relative position and width of likelihood. Pay special attention to both the overall shape of the posterior and the location of the peak. What do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Interactive Demo 1: Mixture-of-Gaussian prior and the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456,
     "referenced_widgets": [
      "2ae86f7825fa42089f00b630d0408324",
      "6f60bf5e489b432fb238f743a1092427",
      "3e2a788493a34bb3b5343a621de06716",
      "b1d6aa7aa27143b7bed7d2131f93315c",
      "ce3f0d132c0348cb9a4cdbe1a1a55617",
      "33aceea223704d10a3f953895c208fef",
      "818c70bbc9f04aac8e319e0b74d7a402",
      "8e60f63452ac4122aa23a52ed6c7a31e",
      "54528ae871ba42f0aafb59a6d4a743cd",
      "e7aab8621eaf473fb04c9048ffc70158",
      "cc50a31c3bb644b48d2a68140b7a92ff",
      "6b62f20ee2204461b4fc2e6058ee8100",
      "abac0add4423425c8892ef4f985c7e06",
      "c7628a32462b47f8bd8a3d7c4f2b5cfe",
      "b74e7dd69dfc4dd882d38e020b51898f",
      "8e96eb6720c9422f9ba320457afe1df0",
      "39b28819bf8a4c0bb051c91b718200e5",
      "9dedcc20a79845779a6447ceb77051e9",
      "e22f2c8ff4ec4e37a6a856d5ccbed3c9"
     ]
    },
    "colab_type": "code",
    "outputId": "12577f11-30f0-484f-c043-d409104ab8d5"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#@markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "fig_domain = np.arange(-10, 11, 0.1)\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def refresh(sigma_common=0.5, sigma_independent=3, p_common=0.75, mu_auditory=3, sigma_auditory=1.5):\n",
    "    _, _, prior = mixture_prior(fig_domain, 0, sigma_common, sigma_independent, p_common)\n",
    "    likelihood = my_gaussian(fig_domain, mu_auditory, sigma_auditory)\n",
    "\n",
    "    posterior = prior * likelihood\n",
    "    posterior /= posterior.sum()\n",
    "\n",
    "    plt.plot(fig_domain, prior, label=\"Mixture Prior\")\n",
    "    plt.plot(fig_domain, likelihood, label=\"Likelihood\")\n",
    "    plt.plot(fig_domain, posterior, label=\"Posterior\")\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "\n",
    "_ = widgets.interact(refresh,\n",
    "    sigma_common=widgets.FloatSlider(value=0.5, min=0.01, max=10, step=0.5, description=\"sigma_common\", style=style),\n",
    "    sigma_independent=widgets.FloatSlider(value=3, min=0.01, max=10, step=0.5, description=\"sigma_independent:\", style=style),\n",
    "    p_common=widgets.FloatSlider(value=0.75, min=0, max=1, steps=0.01, description=\"p_common\"),\n",
    "    mu_auditory=widgets.FloatSlider(value=2, min=-10, max=10, step=0.1, description=\"mu_auditory:\", style=style),\n",
    "    sigma_auditory=widgets.FloatSlider(value=0.5, min=0.01, max=10, step=0.5, description=\"sigma_auditory:\", style=style),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100
    },
    "colab_type": "code",
    "outputId": "17a71ae3-6fd7-47ba-ce59-34e72ad104b2"
   },
   "outputs": [],
   "source": [
    "#to_remove explanation\n",
    "\"\"\"\n",
    "The mixture of Gaussian prior creates some interesting behaviour:\n",
    "  1. We observe multiple modes (i.e. peaks) in our posterior\n",
    "  (the common and independent causes).\n",
    "  2. The mode of the posterior jumps between stimulus locations. These\n",
    "  correspond to the participant switching from the independent to the common\n",
    "  parts (i.e. causes) of the prior.\n",
    "\n",
    "A similar discontinuity (ie. 'jump') in the posterior mode would happen in the\n",
    "case of cue combination illusion with the puppet and puppeteer voice.\n",
    "The illusion that the puppet is generating the speech would break-down when the\n",
    "voice stimulus is presented too far away from the visual input (the puppet's\n",
    "location).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Section 4: Bayesian decisions with compled utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "colab_type": "code",
    "outputId": "f4d716db-7d2b-44d8-ee9e-1191a3b4b5e6"
   },
   "outputs": [],
   "source": [
    "#@title Video 3: Outro\n",
    "video = YouTubeVideo(id='UgeAtE8xZT8', width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In this tutorial, we introduced the ventriloquism setting that will form the basis of Tutorials 3 and 4 as well. We built a mixture-of-Gaussians prior that captures the participants' subjective experiences. In the next tutorials, we will use these to perform causal inference and predict the subject's responses to indvidual stimuli. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D1_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}