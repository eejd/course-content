{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W2D1_Tutorial2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3710jvsc74a57bd03e19903e646247cead5404f55ff575624523d45cf244c3f93aaf5fa10367032a",
      "display_name": "Python 3.7.10 64-bit ('nma': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eejd/course-content/blob/2021-bayes/tutorials/W2D1_BayesianStatistics/W2D1_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajm0vbDOa9tB"
      },
      "source": [
        "# Neuromatch Academy: Week 3, Day 1, Tutorial 2\n",
        "# Bayesian inference and decisions with continuous hidden state\n",
        "\n",
        "__Content creators:__ Eric DeWitt, Xaq Pitkow, Saeed Salehi, Ella Betty\n",
        "\n",
        "__Content reviewers:__ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISohim-Ta9tC"
      },
      "source": [
        "# Tutorial Objectives\n",
        "\n",
        "This notebook introduces you to Gaussians and Bayes' rule for continuous distributions, allowing us to model simple put powerful combinations of prior information and new measurements. In this notebook you will work through the same ideas we explored in the binary state tutorial, but you will be introduced to a new problem: finding and guiding Astrocat! You will see this problem again in more complex ways in the following days.\n",
        "\n",
        "In this notebook, you will:\n",
        "\n",
        "1. Learn about the Gaussian distribution and it's nice properies\n",
        "2. Explore how we can extend the ideas from the binary hidden tutorial to continuous distributions\n",
        "3. Explore how different priors can produce more complex posteriors.\n",
        "4. Explore Loss functions often used in inference and complex utility functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "eR8sfGi_a9tD"
      },
      "source": [
        "# @title Video 1: Introduction\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id='GdIwJWsW9-s', width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upESZk9ga9tD"
      },
      "source": [
        "---\n",
        "##Setup  \n",
        "Please execute the cells below to initialize the notebook environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOzWD_Pga9tE"
      },
      "source": [
        "# imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import multivariate_normal\n",
        "from scipy.stats import gamma as gamma_distribution\n",
        "from matplotlib.transforms import Affine2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "EBLHwDYla9tE"
      },
      "source": [
        "#@title Figure Settings\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import FloatSlider\n",
        "from ipywidgets import interact, fixed, HBox, Layout, VBox, interactive, Label\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmFeqjqa9tF"
      },
      "source": [
        "def gaussian(x, μ, σ):\n",
        "    return np.exp(-((x - μ) / σ)**2 / 2) / np.sqrt(2 * np.pi * σ**2)\n",
        "\n",
        "\n",
        "def gamma_pdf(x, α, β):\n",
        "    return gamma_distribution.pdf(x, a=α, scale=1/β)\n",
        "\n",
        "\n",
        "def mvn2d(x, y, mu1, mu2, sigma1, sigma2, cov12):\n",
        "    mvn = multivariate_normal([mu1, mu2], [[sigma1**2, cov12], [cov12, sigma2**2]])\n",
        "    return mvn.pdf(np.dstack((x, y)))\n",
        "\n",
        "\n",
        "def product_guassian(mu1, mu2, sigma1, sigma2):\n",
        "    J_1, J_2 = 1/sigma1**2, 1/sigma2**2\n",
        "    J_3 = J_1 + J_2\n",
        "    mu_prod = (J_1*mu1/J_3) + (J_2*mu2/J_3)\n",
        "    sigma_prod = np.sqrt(1/J_3)\n",
        "    return mu_prod, sigma_prod\n",
        "\n",
        "\n",
        "def reverse_product(mu3, sigma3, mu1, mu2):\n",
        "    J_3 = 1/sigma3**2\n",
        "    J_1 = J_3 * (mu3 - mu2) / (mu1 - mu2)\n",
        "    J_2 = J_3 * (mu3 - mu1) / (mu2 - mu1)\n",
        "    sigma1, sigma2 = 1/np.sqrt(J_1), 1/np.sqrt(J_2)\n",
        "    return sigma1, sigma2\n",
        "\n",
        "\n",
        "def plot_gaussian(μ, σ):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    y = gaussian(x, μ, σ)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(x, y, c='blue')\n",
        "    plt.fill_between(x, y, color='b', alpha=0.2)\n",
        "    plt.ylabel('$\\mathcal{N}(x, \\mu, \\sigma^2)$')\n",
        "    plt.xlabel('x')\n",
        "    plt.yticks([])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_mvn2d(mu1, mu2, sigma1, sigma2, corr):\n",
        "    x, y = np.mgrid[-2:2:.02, -2:2:.02]\n",
        "    cov12 = corr * sigma1 * sigma2\n",
        "    z = mvn2d(x, y, mu1, mu2, sigma1, sigma2, cov12)\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.contourf(x, y, z, cmap='Reds')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_marginal(sigma1, sigma2, c_x, c_y, corr):\n",
        "    mu1, mu2 = 0.0, 0.0\n",
        "    cov12 = corr * sigma1 * sigma2\n",
        "    xx, yy = np.mgrid[-2:2:.02, -2:2:.02]\n",
        "    x, y = xx[:, 0], yy[0]\n",
        "    p_x = gaussian(x, mu1, sigma1)\n",
        "    p_y = gaussian(y, mu2, sigma2)\n",
        "    zz = mvn2d(xx, yy, mu1, mu2, sigma1, sigma2, cov12)\n",
        "\n",
        "    mu_x_y = mu1+cov12*(c_y-mu2)/sigma2**2\n",
        "    mu_y_x = mu2+cov12*(c_x-mu1)/sigma1**2\n",
        "    sigma_x_y = np.sqrt(sigma2**2 - cov12**2/sigma1**2)\n",
        "    sigma_y_x = np.sqrt(sigma1**2-cov12**2/sigma2**2)\n",
        "    p_x_y = gaussian(x, mu_x_y, sigma_x_y)\n",
        "    p_y_x = gaussian(x, mu_y_x, sigma_y_x)\n",
        "\n",
        "    p_c_y = gaussian(mu_x_y-sigma_x_y, mu_x_y, sigma_x_y)\n",
        "    p_c_x = gaussian(mu_y_x-sigma_y_x, mu_y_x, sigma_y_x)\n",
        "\n",
        "    # definitions for the axes\n",
        "    left, width = 0.1, 0.65\n",
        "    bottom, height = 0.1, 0.65\n",
        "    spacing = 0.01\n",
        "\n",
        "    rect_z = [left, bottom, width, height]\n",
        "    rect_x = [left, bottom + height + spacing, width, 0.2]\n",
        "    rect_y = [left + width + spacing, bottom, 0.2, height]\n",
        "\n",
        "    # start with a square Figure\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "\n",
        "    ax_z = fig.add_axes(rect_z)\n",
        "    ax_x = fig.add_axes(rect_x, sharex=ax_z)\n",
        "    ax_y = fig.add_axes(rect_y, sharey=ax_z)\n",
        "\n",
        "    ax_z.set_axis_off()\n",
        "    ax_x.set_axis_off()\n",
        "    ax_y.set_axis_off()\n",
        "    ax_x.set_xlim(np.min(x), np.max(x))\n",
        "    ax_y.set_ylim(np.min(y), np.max(y))\n",
        "\n",
        "    ax_z.contourf(xx, yy, zz, cmap='Greys')\n",
        "    ax_z.hlines(c_y, mu_x_y-sigma_x_y, mu_x_y+sigma_x_y, color='c', zorder=9, linewidth=3)\n",
        "    ax_z.vlines(c_x, mu_y_x-sigma_y_x, mu_y_x+sigma_y_x, color='m', zorder=9, linewidth=3)\n",
        "\n",
        "    ax_x.plot(x, p_x, label='$p(x)$', c = 'b', linewidth=3)\n",
        "    ax_x.plot(x, p_x_y, label='$p(x|y = C_y)$', c = 'c', linestyle='dashed', linewidth=3)\n",
        "    ax_x.hlines(p_c_y, mu_x_y-sigma_x_y, mu_x_y+sigma_x_y, color='c', linestyle='dashed', linewidth=3)\n",
        "\n",
        "    ax_y.plot(p_y, y, label='$p(y)$', c = 'r', linewidth=3)\n",
        "    ax_y.plot(p_y_x, y, label='$p(y|x = C_x)$', c = 'm', linestyle='dashed', linewidth=3)\n",
        "    ax_y.vlines(p_c_x, mu_y_x-sigma_y_x, mu_y_x+sigma_y_x, color='m', linestyle='dashed', linewidth=3)\n",
        "\n",
        "\n",
        "    ax_x.legend(loc=\"upper left\", frameon=False)\n",
        "    ax_y.legend(loc=\"lower right\", frameon=False)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_bayes(mu1, mu2, sigma1, sigma2):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    prior = gaussian(x, mu1, sigma1)\n",
        "    likelihood = gaussian(x, mu2, sigma2)\n",
        "\n",
        "    mu_post, sigma_post = product_guassian(mu1, mu2, sigma1, sigma2)\n",
        "    posterior = gaussian(x, mu_post, sigma_post)\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, prior, c='b', label='prior')\n",
        "    plt.fill_between(x, prior, color='b', alpha=0.2)\n",
        "    plt.plot(x, likelihood, c='r', label='likelihood')\n",
        "    plt.fill_between(x, likelihood, color='r', alpha=0.2)\n",
        "    plt.plot(x, posterior, c='k', label='posterior')\n",
        "    plt.fill_between(x, posterior, color='k', alpha=0.5)\n",
        "    plt.yticks([])\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.ylabel('$\\mathcal{N}(x, \\mu, \\sigma^2)$')\n",
        "    plt.xlabel('x')\n",
        "    plt.show()\n",
        "\n",
        "def plot_information(mu1, sigma1, mu2, sigma2):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    mu3, sigma3 = product_guassian(mu1, mu2, sigma1, sigma2)\n",
        "    prior = gaussian(x, mu1, sigma1)\n",
        "    likelihood = gaussian(x, mu2, sigma2)\n",
        "    posterior = gaussian(x, mu3, sigma3)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, prior, c='b', label='prior')\n",
        "    plt.fill_between(x, prior, color='b', alpha=0.2)\n",
        "    plt.plot(x, likelihood, c='r', label='likelihood')\n",
        "    plt.fill_between(x, likelihood, color='r', alpha=0.2)\n",
        "    plt.plot(x, posterior, c='k', label='posterior')\n",
        "    plt.fill_between(x, posterior, color='k', alpha=0.5)\n",
        "    plt.yticks([])\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.ylabel('$\\mathcal{N}(x, \\mu, \\sigma^2)$')\n",
        "    plt.xlabel('x')\n",
        "    plt.show()\n",
        "\n",
        "def plot_information_global(mu3, sigma3, mu1, mu2):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    sigma1, sigma2 = reverse_product(mu3, sigma3, mu1, mu2)\n",
        "    prior = gaussian(x, mu1, sigma1)\n",
        "    likelihood = gaussian(x, mu2, sigma2)\n",
        "    posterior = gaussian(x, mu3, sigma3)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, prior, c='b', label='prior')\n",
        "    plt.fill_between(x, prior, color='b', alpha=0.2)\n",
        "    plt.plot(x, likelihood, c='r', label='likelihood')\n",
        "    plt.fill_between(x, likelihood, color='r', alpha=0.2)\n",
        "    plt.plot(x, posterior, c='k', label='posterior')\n",
        "    plt.fill_between(x, posterior, color='k', alpha=0.5)\n",
        "    plt.yticks([])\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.ylabel('$\\mathcal{N}(x, \\mu, \\sigma^2)$')\n",
        "    plt.xlabel('x')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_simple_utility_gaussian(mu, sigma, mu_g, mu_c, sigma_g, sigma_c):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    posterior = gaussian(x, mu, sigma)\n",
        "    gain = gaussian(x, mu_g, sigma_g)\n",
        "    loss = gaussian(x, mu_c, sigma_c)\n",
        "    utility = np.multiply(posterior, gain) - np.multiply(posterior, loss)\n",
        "\n",
        "    plt.figure(figsize=(18, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"Posterior distribution\")\n",
        "    plt.plot(x, posterior, c='k', label='posterior')\n",
        "    plt.fill_between(x, posterior, color='k', alpha=0.5)\n",
        "    plt.yticks([])\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.xlabel('x')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"utility function\")\n",
        "    plt.plot(x, gain, c='m', label='gain')\n",
        "    plt.fill_between(x, gain, color='m', alpha=0.2)\n",
        "    plt.plot(x, -loss, c='c', label='loss')\n",
        "    plt.fill_between(x, -loss, color='c', alpha=0.2)\n",
        "    plt.legend(loc=\"upper left\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"expected utility\")\n",
        "    plt.plot(x, utility, c='y', label='utility')\n",
        "    plt.fill_between(x, utility, color='y', alpha=0.2)\n",
        "    plt.legend(loc=\"upper left\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_utility_gaussian(mu1, mu2, sigma1, sigma2, mu_g, mu_c, sigma_g, sigma_c):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    prior = gaussian(x, mu1, sigma1)\n",
        "    likelihood = gaussian(x, mu2, sigma2)\n",
        "    gain = gaussian(x, mu_g, sigma_g)\n",
        "    loss = gaussian(x, mu_c, sigma_c)\n",
        "\n",
        "    mu_post, sigma_post = product_guassian(mu1, mu2, sigma1, sigma2)\n",
        "    posterior = gaussian(x, mu_post, sigma_post)\n",
        "\n",
        "    utility = np.multiply(posterior, gain) - np.multiply(posterior, loss)\n",
        "\n",
        "    plot_utility(x, prior, likelihood, posterior, gain, loss, utility)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def plot_utility_uniform(mu, sigma, mu_g, mu_c, sigma_g, sigma_c):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    prior = np.ones_like(x) / (x.max() - x.min())\n",
        "    likelihood = gaussian(x, mu, sigma)\n",
        "    gain = gaussian(x, mu_g, sigma_g)\n",
        "    loss = gaussian(x, mu_c, sigma_c)\n",
        "\n",
        "    posterior = likelihood\n",
        "    # posterior = np.multiply(prior, likelihood)\n",
        "    # posterior = posterior / (posterior.sum() * (x[1] - x[0]))\n",
        "\n",
        "    utility = np.multiply(posterior, gain) - np.multiply(posterior, loss)\n",
        "\n",
        "    plot_utility(x, prior, likelihood, posterior, gain, loss, utility)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def plot_utility_gamma(alpha, beta, offset, mu, sigma, mu_g, mu_c, sigma_g, sigma_c):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    prior = gamma_pdf(x-offset, alpha, beta)\n",
        "    likelihood = gaussian(x, mu, sigma)\n",
        "    gain = gaussian(x, mu_g, sigma_g)\n",
        "    loss = gaussian(x, mu_c, sigma_c)\n",
        "\n",
        "    posterior = np.multiply(prior, likelihood)\n",
        "    posterior = posterior / (posterior.sum() * (x[1] - x[0]))\n",
        "\n",
        "    utility = np.multiply(posterior, gain) - np.multiply(posterior, loss)\n",
        "\n",
        "    plot_utility(x, prior, likelihood, posterior, gain, loss, utility)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def plot_utility(x, prior, likelihood, posterior, gain, loss, utility):\n",
        "    plt.figure(figsize=(18, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"Posterior distribution\")\n",
        "    plt.plot(x, prior, c='b', label='prior')\n",
        "    plt.fill_between(x, prior, color='b', alpha=0.2)\n",
        "    plt.plot(x, likelihood, c='r', label='likelihood')\n",
        "    plt.fill_between(x, likelihood, color='r', alpha=0.2)\n",
        "    plt.plot(x, posterior, c='k', label='posterior')\n",
        "    plt.fill_between(x, posterior, color='k', alpha=0.5)\n",
        "    plt.yticks([])\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    # plt.ylabel('$f(x)$')\n",
        "    plt.xlabel('x')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"utility function\")\n",
        "    plt.plot(x, gain, c='m', label='gain')\n",
        "    plt.fill_between(x, gain, color='m', alpha=0.2)\n",
        "    plt.plot(x, -loss, c='c', label='loss')\n",
        "    plt.fill_between(x, -loss, color='c', alpha=0.2)\n",
        "    plt.legend(loc=\"upper left\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"expected utility\")\n",
        "    plt.plot(x, utility, c='y', label='utility')\n",
        "    plt.fill_between(x, utility, color='y', alpha=0.2)\n",
        "    plt.legend(loc=\"upper left\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def gaussian_mixture(mu1, mu2, sigma1, sigma2, factor):\n",
        "    assert 0.0 < factor < 1.0\n",
        "    x = np.linspace(-7.0, 7.0, 1000, endpoint=True)\n",
        "    y_1 = gaussian(x, mu1, sigma1)\n",
        "    y_2 = gaussian(x, mu2, sigma2)\n",
        "    mixture = y_1 * factor + y_2 * (1.0 - factor)\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, y_1, c='deepskyblue', label='p(x)', linewidth=3.0)\n",
        "    plt.fill_between(x, y_1, color='deepskyblue', alpha=0.2)\n",
        "    plt.plot(x, y_2, c='aquamarine', label='p(y)', linewidth=3.0)\n",
        "    plt.fill_between(x, y_2, color='aquamarine', alpha=0.2)\n",
        "    plt.plot(x, mixture, c='b', label='$\\pi \\cdot p(x) + (1-\\pi) \\cdot p(y)$',  linewidth=3.0)\n",
        "    plt.fill_between(x, mixture, color='b', alpha=0.5)\n",
        "    plt.yticks([])\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    # plt.ylabel('$f(x)$')\n",
        "    plt.xlabel('x')\n",
        "    plt.show()\n",
        "\n",
        "def plot_switcher(what_to_plot):\n",
        "    if what_to_plot == \"Gaussian\":\n",
        "        widget = interact(plot_utility_gaussian,\n",
        "                  mu1 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ_prior\", continuous_update=False),\n",
        "                  mu2 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.5, description=\"µ_likelihood\", continuous_update=False),\n",
        "                  sigma1 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_prior\", continuous_update=False),\n",
        "                  sigma2 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_likelihood\", continuous_update=False),\n",
        "                  mu_g = FloatSlider(min=-4.0, max=4.0, step=0.01, value=1.0, description=\"µ_gain\", continuous_update=False),\n",
        "                  mu_c = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-1.0, description=\"µ_cost\", continuous_update=False),\n",
        "                  sigma_g = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_gain\", continuous_update=False),\n",
        "                  sigma_c = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_cost\", continuous_update=False))\n",
        "    elif what_to_plot == \"Uniform\":\n",
        "        widget = interact(plot_utility_uniform,\n",
        "                  mu = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.5, description=\"µ_likelihood\", continuous_update=False),\n",
        "                  sigma = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_likelihood\", continuous_update=False),\n",
        "                  mu_g = FloatSlider(min=-4.0, max=4.0, step=0.01, value=1.0, description=\"µ_gain\", continuous_update=False),\n",
        "                  mu_c = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-1.0, description=\"µ_cost\", continuous_update=False),\n",
        "                  sigma_g = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_gain\", continuous_update=False),\n",
        "                  sigma_c = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_cost\", continuous_update=False))\n",
        "    elif what_to_plot == \"Gamma\":\n",
        "        widget = interact(plot_utility_gamma,\n",
        "                  alpha = FloatSlider(min=1.0, max=10.0, step=0.1, value=2.0, description=\"α_prior\", continuous_update=False),\n",
        "                  beta = FloatSlider(min=0.5, max=2.0, step=0.01, value=1.0, description=\"β_prior\", continuous_update=False),\n",
        "                  offset = FloatSlider(min=-6.0, max=2.0, step=0.1, value=0.0, description=\"offset\", continuous_update=False),\n",
        "                  mu = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.5, description=\"µ_likelihood\", continuous_update=False),\n",
        "                  sigma = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_likelihood\", continuous_update=False),\n",
        "                  mu_g = FloatSlider(min=-4.0, max=4.0, step=0.01, value=1.0, description=\"µ_gain\", continuous_update=False),\n",
        "                  mu_c = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-1.0, description=\"µ_cost\", continuous_update=False),\n",
        "                  sigma_g = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_gain\", continuous_update=False),\n",
        "                  sigma_c = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_cost\", continuous_update=False))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVNOYHWRq2CN"
      },
      "source": [
        "# New Widgets\n",
        "\n",
        "widget = interact(plot_simple_utility_gaussian,\n",
        "                  mu = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ_posterior\", continuous_update=False),\n",
        "                  sigma = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_posterior\", continuous_update=False),\n",
        "                  mu_g = FloatSlider(min=-4.0, max=4.0, step=0.01, value=1.0, description=\"µ_gain\", continuous_update=False),\n",
        "                  mu_c = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-1.0, description=\"µ_cost\", continuous_update=False),\n",
        "                  sigma_g = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_gain\", continuous_update=False),\n",
        "                  sigma_c = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_cost\", continuous_update=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "N_qgI8rza9tG"
      },
      "source": [
        "#@title Helper functions\n",
        "# --- maybe we allow them to implement gaussian and cost?\n",
        "def my_gaussian(x_points, mu, sigma):\n",
        "    \"\"\"\n",
        "    DO NOT EDIT THIS FUNCTION !!!\n",
        "\n",
        "    Returns normalized Gaussian estimated at points `x_points`, with parameters `mu` and `sigma`\n",
        "\n",
        "    Args:\n",
        "      x_points (numpy array of floats) - points at which the gaussian is evaluated\n",
        "      mu (scalar) - mean of the Gaussian\n",
        "      sigma (scalar) - standard deviation of the gaussian\n",
        "    Returns:\n",
        "      (numpy array of floats): normalized Gaussian (i.e. without constant) evaluated at `x`\n",
        "    \"\"\"\n",
        "    px = np.exp(- 1/2/sigma**2 * (mu - x_points) ** 2)\n",
        "\n",
        "    px = px / px.sum() # this is the normalization part with a very strong assumption, that\n",
        "                       # x_points cover the big portion of probability mass around the mean.\n",
        "                       # Please think/discuss when this would be a dangerous assumption.\n",
        "\n",
        "    return px\n",
        "\n",
        "def plot_mixture_prior(x, gaussian1, gaussian2, combined):\n",
        "    \"\"\"\n",
        "    DO NOT EDIT THIS FUNCTION !!!\n",
        "\n",
        "    Plots a prior made of a mixture of gaussians\n",
        "\n",
        "    Args:\n",
        "      x (numpy array of floats):         points at which the likelihood has been evaluated\n",
        "      gaussian1 (numpy array of floats): normalized probabilities for Gaussian 1 evaluated at each `x`\n",
        "      gaussian2 (numpy array of floats): normalized probabilities for Gaussian 2 evaluated at each `x`\n",
        "      posterior (numpy array of floats): normalized probabilities for the posterior evaluated at each `x`\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(x, gaussian1, '--b', LineWidth=2, label='Gaussian 1')\n",
        "    ax.plot(x, gaussian2, '-.b', LineWidth=2, label='Gaussian 2')\n",
        "    ax.plot(x, combined, '-r', LineWidth=2, label='Gaussian Mixture')\n",
        "    ax.legend()\n",
        "    ax.set_ylabel('Probability')\n",
        "    ax.set_xlabel('Orientation (Degrees)')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AILoNnT-a9tH"
      },
      "source": [
        "# @title Video 2: Astrocat!\n",
        "from IPython.display import YouTubeVideo\n",
        "# video = YouTubeVideo(id='GdIwJWsW9-s', width=854, height=480, fs=1)\n",
        "# print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "# video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4N2Qr7ra9tH"
      },
      "source": [
        "# Section 1: Likelihoods and utility - continuous distributions\n",
        "\n",
        "Now that you have been introduced to Astrocat, we need to extend the ideas from tutorial 1 to help ground control determine where to move him! Remember, in this example, you can think of us as scientists taking measurements of Astrocat's location, along with measurements of the satellite and the space mouse. Your goal as a scientist would be to determine your *belief* about the location of Astrocat, how best to choose an estimate of his precise location, and where to tell his jet pack to go given what you know about the gains and losses associate with correctly (or incorrectly) positioning him. In fact, this is the kind of problem real scientists working to control remote equipment face! But you could also think of Astrocat as a the activity in the brain, measured with a noisy instrument, where you want to know the most likely true value. And, finally, we can also think of this as what your brain has to do when it wants to determine where to move your body! A number of classic experiments use this kind of framing to study how *optimal* human decisions or movements are! Some examples are in the reading ?reference to where?.\n",
        "\n",
        "The important concepts intoduced in this section are:\n",
        "\n",
        "1. The Gaussian distribution\n",
        "2. Combining Gaussians and relative information\n",
        "2. The use of Loss functions when reducing a distribution to an estimate of the true value\n",
        "3. The use of Utility functions to describe the gains and loses for taking an action"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U7Ci7Foa9tI"
      },
      "source": [
        "# @title Video 3: Likelihoods and utility - continuous distributions\n",
        "from IPython.display import YouTubeVideo\n",
        "# video = YouTubeVideo(id='GdIwJWsW9-s', width=854, height=480, fs=1)\n",
        "# print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "# video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGKqnqANa9tI"
      },
      "source": [
        "## The Gaussian Distribution\n",
        "\n",
        "We know the Bayesian approach can be used on any probability distribution. Although many things in the world require representation using complex or unknown (e.g. emperical) distributions, the Gaussian distribution is special. Because of the Central Limit Theorem, many quantities are Gaussian--or *normally distributed*--or measurements on them are Gaussian. Gaussians also have some mathematical properties that permit simple closed-form solutions to several important problems. And we can use *mixtures* of Gaussians to approximate other distributions. In short, the Gaussian is probably the most important continous distribution to understand and use.\n",
        "\n",
        "?? if we implement \n",
        "\n",
        "### Exercise 1:\n",
        "First, you will implement a Gaussian by filling in the missing portion of `my_gaussian` below. Gaussians have two parameters. The **mean** $\\mu$, which sets the location of its center. Its \"scale\" or spread is controlled by its **standard deviation** $\\sigma$ or its square, the **variance** $\\sigma^2$. (Be careful not to use one when the other is required). \n",
        "\n",
        "The equation for a Gaussian is:\n",
        "$$\n",
        "\\mathcal{N}(\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right)\n",
        "$$\n",
        "Also, don't forget that this is a probability distribution and should therefore sum to one. While this happens \"automatically\" when integrated from $-\\infty$ to $\\infty$, your version will only be computed over a finite number of points. You therefore need to explicitly normalize it yourself.\n",
        "\n",
        "Test out your implementation with a $\\mu = -1$ and $\\sigma = 1$. After you have it working, play with the  parameters to develop an intuition for how changing $\\mu$ and $\\sigma$ alter the shape of the Gaussian. This is important, because subsequent exercises will be built out of Gaussians. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLlFK11La9tI"
      },
      "source": [
        "?? if we have them code the gaussian, it goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmitvcV6a9tJ"
      },
      "source": [
        "### Exercise2:\n",
        "\n",
        "Play with the Gaussian you made. Remember that we explained that the information (about the mean, $\\mu$) is $\\frac{1}{\\sigma^2}$.\n",
        "\n",
        "1. If you wanted to know the probabilty of an event happing at $0$, can you find two different $\\mu$ and $\\sigma$ values that produce the same probabilty of an event at $0$?\n",
        "2. How many Gaussian's could produce the same probabilty at $0$?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OI1jBhLa9tJ"
      },
      "source": [
        "widget = interact(plot_gaussian,\n",
        "                     μ = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.0, continuous_update=False),\n",
        "                     σ = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, continuous_update=False))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfsjNwKMa9tJ"
      },
      "source": [
        "## Multiplying Gaussians - interactive demo\n",
        "\n",
        "We have implemented the multiplication of two Gaussians for you. Using the following widget, we are going to think about the information and combination of two Gaussians. It is important to remember, this isn't combining the underlying random variables! In our case, imagine we want to find the location least likely to contain the satellite or space mouse. This would be the center (average) of the two locations. Because we have uncertainty, we need to weight our uncertianty in thinking about the most likely place. Or imagine you want to know how much information is gained combining (averaging) the response of two neurons that represent locations in sensory space (think: how much information is shared by their receptive fields). In any case where we need to multiply two Gaussian distributions, we will also be weighting the information they each have about their center.\n",
        "\n",
        "Remember:\n",
        "\n",
        "$$\n",
        "\\mu_{3} = a\\mu_{1} + (1-a)\\mu_{2}\n",
        "$$\n",
        "$$\n",
        "\\sigma_{3}^{-2} = \\sigma_{1}^{-2} + \\sigma_{2}^{-2}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "a = \\frac{\\sigma_{1}^{-2}}{\\sigma_{1}^{-2} + \\sigma_{2}^{-2}}\n",
        "$$\n",
        "\n",
        "Questions:\n",
        "\n",
        "1. What is your uncertainty (how much information) do you have about $\\mu_{3}$ with $\\mu_{1} = -1, \\mu_{2} = 1, \\sigma_{1} = \\sigma_{2} = 0.5$?\n",
        "2. What happens to your estimate of $\\mu_{3}$ as $\\sigma_{2} \\to \\infty$? (In this case, $\\sigma$ only goes to 11... but that should be loud enough.)\n",
        "3. What is the differene in your estimate of $\\mu_{3} if $\\sigma_{1} = \\sigma_{2} = 11$? What has changed from the first example?\n",
        "4. Set $\\mu_{1} = -4, \\mu_{2} = 4$ and change the $\\sigma$s so that $\\mu_{3}$ is close to $2$. How many $\\sigma$s will produce the same $\\mu_{3}$?\n",
        "5. Continuing, if you set $\\mu_{1} = 0$, what $\\sigma$ do you need to change so $\\mu_{3} ~= 2$?\n",
        "6. If $\\sigma_{1} = \\sigma_{2} = 0.1$, how much information do you have about the average?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUaAFKMaa9tK"
      },
      "source": [
        "widget = interact(plot_information,\n",
        "                  mu1 = FloatSlider(min=-6.0, max=6.0, step=0.01, value=-1.0, description=\"µ_1\", continuous_update=False),\n",
        "                  sigma1 = FloatSlider(min=0.1, max=11.0, step=0.01, value=0.5, description=\"σ_1\", continuous_update=False),\n",
        "                  mu2 = FloatSlider(min=-6.0, max=6.0, step=0.01, value=1.0, description=\"µ_2\",continuous_update=False),\n",
        "                  sigma2 = FloatSlider(min=0.1, max=11.0, step=0.01, value=0.5, description=\"σ_2\", continuous_update=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAFMx5nMa9tK"
      },
      "source": [
        "## Mixture's of Gaussians - interactive demo\n",
        "\n",
        "We can also combine Gaussian's using the idea of a *mixture distribution*. In a Gaussian mixture distribution, you use a random variable to draw from each of the the Guassian's you are mixing! We will not cover the derivation here but you can work it out as a bonux exercise!\n",
        "\n",
        "Mixture distributions are useful if you want to represent a multi-model distribution (i.e. where the distribution isn't goes up and down more than once). They are a common tool in Bayesian modeling and an important tool in general.\n",
        "\n",
        "Use the following widget to experiment with the parameters of each Gaussian and the mixing weight ($\\pi$) to undersand how the mixture of Gaussian distribution behaves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoUoeB5Ka9tL"
      },
      "source": [
        "widget = interact(gaussian_mixture,\n",
        "            mu1 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=1.0, description=\"µ_1\", continuous_update=False),\n",
        "            mu2 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-1.0, description=\"µ_2\", continuous_update=False),\n",
        "            sigma1 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_1\", continuous_update=False),\n",
        "            sigma2 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_2\", continuous_update=False),\n",
        "            factor = FloatSlider(min=0.1, max=0.9, step=0.01, value=0.5, description=\"π\", continuous_update=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnYT7EA2a9tL"
      },
      "source": [
        "## Utility functions: Loss and estimation - interactive demo\n",
        "\n",
        "A utility function, as we have seen, represents a gain or loss (also called a cost). When we are asked for a single estimate from a probability distribution, we are explicitly or implicitly defining a specifc utility function--the *Loss* associated with being 'wrong' in our 'choice' with respect to the probabilities. Think about our case, we want to know where Astrocat is. If we were asked to provide the coordinates, for example to display them for Ground Control or two note them in a log, we are not going to provide the whole probability distribution! So, we use an estimator or, equivilantly, we define a Loss function. We are going to explore this for the Gaussian distribution, where our estimate is $\\hat{\\mu}$ and the true hidden state we are interested in is $\\mu$. ?? could extend to mixture ??\n",
        "\n",
        "A Loss function determines the \"cost\" (or penalty) of estimating $\\hat \\mu$ when the true or correct quantity is really $\\mu$ (this is essentially the cost of the error between the true hidden state we are interested in: $\\mu$ and our estimate: $\\hat \\mu$ -- Note that the error can be defined in different ways):\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\textrm{Mean Squared Error} &=& (\\mu - \\hat{\\mu})^2 \\\\ \n",
        "\\textrm{Absolute Error} &=& \\big|\\mu - \\hat{\\mu}\\big| \\\\ \n",
        "\\textrm{Zero-One Loss} &=& \\begin{cases}\n",
        "                            0,& \\textrm{if } \\mu = \\hat{\\mu} \\\\\n",
        "                            1,              & \\textrm{otherwise}\n",
        "                            \\end{cases}\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "?? we could have them implement these ??\n",
        "In the cell below, fill in the body of these cost function. Each function should take one single value for $x$ (the true stimulus value : $x$) and one or more possible value estimates: $\\hat{x}$. \n",
        "\n",
        "Return an array containing the costs associated with predicting $\\hat{x}$ when the true value is $x$. Once you have written all three functions, uncomment the final line to visulize your results.\n",
        "\n",
        " _Hint:_ These functions are easy to write (1 line each!) but be sure *all* three functions return arrays of `np.float` rather than another data type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DXebTeCa9tL"
      },
      "source": [
        "# if implementing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G15pqroZa9tL"
      },
      "source": [
        "### Exploring Loss functions\n",
        "\n",
        "The Mean Squared Error Loss function produces the Mean estiamte\n",
        "The Absolute Error Loss function produces the Median estiamte\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkjK51i9a9tM"
      },
      "source": [
        "widget = interact(plot_utility_gaussian,\n",
        "                  mu1 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ_prior\", continuous_update=False),\n",
        "                  mu2 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.5, description=\"µ_likelihood\", continuous_update=False),\n",
        "                  sigma1 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_prior\", continuous_update=False),\n",
        "                  sigma2 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_likelihood\", continuous_update=False),\n",
        "                  mu_g = FloatSlider(min=-4.0, max=4.0, step=0.01, value=1.0, description=\"µ_gain\", continuous_update=False),\n",
        "                  mu_c = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-1.0, description=\"µ_cost\", continuous_update=False),\n",
        "                  sigma_g = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_gain\", continuous_update=False),\n",
        "                  sigma_c = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_cost\", continuous_update=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0to3dQjna9tM"
      },
      "source": [
        "# Section 2: Information and marginalization\n",
        "\n",
        "In this section we will explore correlated Gaussin variables to think about information sharing. Then we will explore how we can marginalize over such a distribution. Finally, we will think about marginalization when two distributions are multiplied."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpOhf8aia9tM"
      },
      "source": [
        "vA posterior distribution tells us about the confidence or credibility we assign to different choices. A cost function describes the penalty we incur when choosing an incorrect option. These concepts can be combined into an *expected loss* function. Expected loss is defined as:\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "    \\mathbb{E}[\\text{Loss} | \\hat{x}] = \\int L[\\hat{x},x] \\odot  p(x|\\tilde{x}) dx\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "where $L[ \\hat{x}, x]$ is the loss function, $p(x|\\tilde{x})$ is the posterior, and $\\odot$ represents the [Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) (i.e., elementwise multiplication), and $\\mathbb{E}[\\text{Loss} | \\hat{x}]$ is the expected loss. \n",
        "\n",
        "In this exercise, we will calculate the expected loss for the: means-squared error, the absolute error, and the zero-one loss over our bimodal posterior $p(x | \\tilde x)$. \n",
        "\n",
        "**Suggestions:**\n",
        "* We already pre-completed the code (commented-out) to calculate the mean-squared error, absolute error, and zero-one loss between $x$ and an estimate $\\hat x$ using the functions you created in exercise 1\n",
        "* Calculate the expected loss ($\\mathbb{E}[MSE Loss]$) using your posterior (imported above as `posterior`) & each of the loss functions described above (MSELoss, ABSELoss, and Zero-oneLoss).\n",
        "* Find the x position that minimizes the expected loss for each cost function and plot them using the `loss_plot` function provided (commented-out)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlxISLZCa9tN"
      },
      "source": [
        "widget = interact(plot_mvn2d,\n",
        "                  mu1 = FloatSlider(min=-1.0, max=1.0, step=0.01, value=0.0, description=\"µ_1\", continuous_update=False),\n",
        "                  mu2 = FloatSlider(min=-1.0, max=1.0, step=0.01, value=0.0, description=\"µ_2\", continuous_update=False),\n",
        "                  sigma1 = FloatSlider(min=0.1, max=1.5, step=0.01, value=0.5, description=\"σ_1\", continuous_update=False),\n",
        "                  sigma2 = FloatSlider(min=0.1, max=1.5, step=0.01, value=0.5, description=\"σ_2\", continuous_update=False),\n",
        "                  corr = FloatSlider(min=-0.99, max=0.99, step=0.01, value=0.0, description=\"ρ\", continuous_update=False))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv8Y-n3Ma9tN"
      },
      "source": [
        "widget = interact(plot_marginal,\n",
        "                  sigma1 = FloatSlider(min=0.1, max=1.1, step=0.01, value=0.5, description=\"σ_1\", continuous_update=False),\n",
        "                  sigma2 = FloatSlider(min=0.1, max=1.1, step=0.01, value=0.5, description=\"σ_2\", continuous_update=False),\n",
        "                  c_x = FloatSlider(min=-1.0, max=1.0, step=0.01, value=0.0, description=\"Cx\", continuous_update=False),\n",
        "                  c_y = FloatSlider(min=-1.0, max=1.0, step=0.01, value=0.0, description=\"Cy\", continuous_update=False),\n",
        "                  corr = FloatSlider(min=-1.0, max=1.0, step=0.01, value=0.0, description=\"ρ\", continuous_update=False))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TorW6wKga9tO"
      },
      "source": [
        "# Section 3: Bayes' theorem for continuous distributions\n",
        "\n",
        "The Gausian example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DQCv8Zma9tO"
      },
      "source": [
        "\n",
        "\n",
        "Bayes' rule tells us how to combine two sources of information: the prior (e.g., a noisy representation of our expectations about where the stimulus might come from) and the likelihood (e.g., a noisy representation of the stimulus position on a given trial), to obtain a posterior distribution taking into account both pieces of information. Bayes' rule states:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\text{Posterior} = \\frac{ \\text{Likelihood} \\times \\text{Prior}}{ \\text{Normalization constant}}\n",
        "\\end{eqnarray}\n",
        "\n",
        "When both the prior and likelihood are Gaussians, this translates into the following form:\n",
        "\n",
        "$$\n",
        "\\begin{array}{rcl}\n",
        "\\text{Likelihood} &=& \\mathcal{N}(\\mu_{likelihood},\\sigma_{likelihood}^2) \\\\\n",
        "\\text{Prior} &=& \\mathcal{N}(\\mu_{prior},\\sigma_{prior}^2) \\\\\n",
        "\\text{Posterior} &\\propto& \\mathcal{N}(\\mu_{likelihood},\\sigma_{likelihood}^2) \\times \\mathcal{N}(\\mu_{prior},\\sigma_{prior}^2) \\\\\n",
        "&&= \\mathcal{N}\\left( \\frac{\\sigma^2_{likelihood}\\mu_{prior}+\\sigma^2_{prior}\\mu_{likelihood}}{\\sigma^2_{likelihood}+\\sigma^2_{prior}}, \\frac{\\sigma^2_{likelihood}\\sigma^2_{prior}}{\\sigma^2_{likelihood}+\\sigma^2_{prior}} \\right) \n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "In these equations, $\\mathcal{N}(\\mu,\\sigma^2)$ denotes a Gaussian distribution with parameters $\\mu$ and $\\sigma^2$:\n",
        "$$\n",
        "\\mathcal{N}(\\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\; \\exp \\bigg( \\frac{-(x-\\mu)^2}{2\\sigma^2} \\bigg)\n",
        "$$\n",
        "\n",
        "In Exercise 2A, we will use the first form of the posterior, where the two distributions are combined via pointwise multiplication.  Although this method requires more computation, it works for any type of probability distribution. In Exercise 2B, we will see that the closed-form solution shown on the line below produces the same result. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18XfQvk2a9tO"
      },
      "source": [
        "widget = interact(plot_bayes,\n",
        "                  mu1 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ_prior\", continuous_update=False),\n",
        "                  mu2 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.5, description=\"µ_likelihood\", continuous_update=False),\n",
        "                  sigma1 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_prior\", continuous_update=False),\n",
        "                  sigma2 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_likelihood\", continuous_update=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDuI85NOa9tP"
      },
      "source": [
        "# Section 3: Exploring Priors\n",
        "\n",
        "In the previous tutorial, you learned how to create a single Gaussian prior that could represent one of these possibilties. A broad Gaussian with a large $\\sigma$ could represent sounds originating from nearly anywhere, while a narrow Gaussian with $\\mu$ near zero could represent sounds orginating from the puppet. \n",
        "\n",
        "Here, we will combine those into a mixure-of-Gaussians probability density function (PDF) that captures both possibilties. We will control how the Gaussians are mixed by summing them together with a 'mixing' or weight parameter $p_{common}$, set to a value between 0 and 1, like so:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "    \\text{Mixture} = \\bigl[\\; p_{common} \\times \\mathcal{N}(\\mu_{common},\\sigma_{common}) \\; \\bigr] + \\bigl[ \\;\\underbrace{(1-p_{common})}_{p_{independent}} \\times \\mathcal{N}(\\mu_{independent},\\sigma_{independent}) \\; \\bigr]\n",
        "\\end{eqnarray}\n",
        "\n",
        "$p_{common}$ denotes the probability that auditory stimulus shares a \"common\" source with the learnt visual input; in other words, the probability that the \"puppet\" is speaking. You might think that we need to include a separate weight for the possibility that sound is \"independent\" from the puppet. nHowever, since there are only two, mutually-exclusive possibilties, we can replace $p_{independent}$ with $(1 - p_{common})$ since, by the law of total probability, $p_{common} + p_{independent}$ must equal one. \n",
        "\n",
        "Using the formula above, complete the code to build this mixture-of-Gaussians PDF: \n",
        "* Generate a Gaussian with mean 0 and standard deviation 0.5 to be the 'common' part of the Gaussian mixture prior. (This is already done for you below).\n",
        "* Generate another Gaussian with mean 0 and standard deviation 3 to serve as the 'independent' part. \n",
        "* Combine the two Gaussians to make a new prior by mixing the two Gaussians with mixing parameter $p_{common}$ = 0.75 so that the peakier \"common-cause\" Gaussian has 75% of the weight. Don't forget to normalize afterwards! \n",
        "\n",
        "Hints:\n",
        "* Code for the `my_gaussian` function from Tutorial 1 is available for you to use. Its documentation is below. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U_CVkE_a9tP"
      },
      "source": [
        "## Exercise 1: Implement the prior "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "a5_veQn1a9tQ"
      },
      "source": [
        "def mixture_prior(x, mean=0, sigma_common=0.5, sigma_independent=3, p_common=0.75):\n",
        "\n",
        "  ###############################################################################\n",
        "  ## Insert your code here to:\n",
        "  #   * Create a second gaussian representing the independent-cause component\n",
        "  #   * Combine the two priors, using the mixing weight p_common. Don't forget\n",
        "  #      to normalize the result so it remains a proper probability density function\n",
        "  #\n",
        "  #   * Comment the line below to test out your function\n",
        "  raise NotImplementedError(\"Please complete Exercise 1\")\n",
        "  ###############################################################################\n",
        "\n",
        "  gaussian_common = my_gaussian(x, mean, sigma_common)\n",
        "  gaussian_independent = ...\n",
        "  mixture = ...\n",
        "\n",
        "  return gaussian_common, gaussian_independent, mixture\n",
        "\n",
        "\n",
        "x = np.arange(-10, 11, 0.1)\n",
        "\n",
        "# Uncomment the lines below to visualize out your solution\n",
        "# common, independent, mixture = mixture_prior(x)\n",
        "# plot_mixture_prior(x, common, independent, mixture)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu10PWz1a9tQ"
      },
      "source": [
        "# to_remove solution\n",
        "def mixture_prior(x, mean=0, sigma_common=0.5, sigma_independent=3, p_common=0.75):\n",
        "\n",
        "  gaussian_common = my_gaussian(x, mean, sigma_common)\n",
        "  ###############################################################################\n",
        "  ## Insert your code here to:\n",
        "  ##   * Create a second gaussian representing the independent-cause component\n",
        "  ##   * Combine the two priors, using the mixing weight p_common. Don't forget\n",
        "  #      to normalize the result so it remains a proper probability density function\n",
        "  #\n",
        "  #    * Comment the line below to test out your function\n",
        "  #raise NotImplementedError(\"Please complete Exercise 1\")\n",
        "  ###############################################################################\n",
        "  gaussian_independent = my_gaussian(x, mean, sigma_independent)\n",
        "\n",
        "  mixture = p_common * gaussian_common + ((1-p_common) * gaussian_independent)\n",
        "  mixture = mixture / np.sum(mixture)\n",
        "\n",
        "  return gaussian_common, gaussian_independent, mixture\n",
        "\n",
        "\n",
        "x = np.arange(-10, 11, 0.1)\n",
        "\n",
        "# Uncomment the lines below to visualize out your solution\n",
        "common, independent, mixture = mixture_prior(x)\n",
        "with plt.xkcd():\n",
        "  plot_mixture_prior(x, common, independent, mixture)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GEIqk7ga9tR"
      },
      "source": [
        "widget = interact(plot_utility_gaussian,\n",
        "                  mu1 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ_prior\", continuous_update=False),\n",
        "                  mu2 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.5, description=\"µ_likelihood\", continuous_update=False),\n",
        "                  sigma1 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_prior\", continuous_update=False),\n",
        "                  sigma2 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_likelihood\", continuous_update=False),\n",
        "                  mu_g = FloatSlider(min=-4.0, max=4.0, step=0.01, value=1.0, description=\"µ_gain\", continuous_update=False),\n",
        "                  mu_c = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-1.0, description=\"µ_cost\", continuous_update=False),\n",
        "                  sigma_g = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_gain\", continuous_update=False),\n",
        "                  sigma_c = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_cost\", continuous_update=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-tRwSgUa9tR"
      },
      "source": [
        "\n",
        "# Section 3: Bayes Theorem with Complex Posteriors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "goDkAnTTa9tS"
      },
      "source": [
        "#@title Video 2: Mixture-of-Gaussians and Bayes' Theorem\n",
        "video = YouTubeVideo(id='LWKM35te0WI', width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlnOtfSva9tS"
      },
      "source": [
        "widget = interact(plot_switcher,\n",
        "                  what_to_plot = widgets.Dropdown(\n",
        "                      options=[\"Gaussian\", \"Uniform\", \"Gamma\"], \n",
        "                      value=\"Gaussian\", description=\"Prior: \"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_y-W4oia9tS"
      },
      "source": [
        "Now that we have created a mixture of Gaussians prior that embodies the participants' expectations about sound location, we want to compute the posterior probability, which represents the subjects' beliefs about a specific sound's origin. \n",
        "\n",
        "To do so we will compute the posterior by using *Bayes Theorem* to combine the mixture-of-gaussians prior and varying auditory Gaussian likelihood. This works exactly the same as in Tutorial 1: we simply multiply the prior and likelihood pointwise, then normalize the resulting distribution so it sums to 1. (The closed-form solution from Exercise 2B, however, no longer applies to this more complicated prior). \n",
        "\n",
        "Here, we provide you with the code mentioned in the video (lucky!). Instead, use the interactive demo to explore how a mixture-of-Gaussians prior and Gaussian likelihood interact. For simplicity, we have fixed the prior mean to be zero. We also recommend starting with same other prior parameters used in Exercise 1: $\\sigma_{common} = 0.5, \\sigma_{independent} = 3, p_{common}=0.75$; vary the likelihood instead. \n",
        "\n",
        "Unlike the demo in Tutorial 1, you should see several qualitatively different effects on the posterior, depending on the relative position and width of likelihood. Pay special attention to both the overall shape of the posterior and the location of the peak. What do you see?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPWSQPNra9tS"
      },
      "source": [
        "## Interactive Demo 1: Mixture-of-Gaussian prior and the posterior"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq_tGJEDa9tT"
      },
      "source": [
        "widget = interact(plot_switcher,\n",
        "                  what_to_plot = widgets.Dropdown(\n",
        "                      options=[\"Gaussian\", \"Uniform\", \"Gamma\"], \n",
        "                      value=\"Gaussian\", description=\"Prior: \"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjcPWF9ma9tT"
      },
      "source": [
        "#to_remove explanation\n",
        "\"\"\"\n",
        "The mixture of Gaussian prior creates some interesting behaviour:\n",
        "  1. We observe multiple modes (i.e. peaks) in our posterior\n",
        "  (the common and independent causes).\n",
        "  2. The mode of the posterior jumps between stimulus locations. These\n",
        "  correspond to the participant switching from the independent to the common\n",
        "  parts (i.e. causes) of the prior.\n",
        "\n",
        "A similar discontinuity (ie. 'jump') in the posterior mode would happen in the\n",
        "case of cue combination illusion with the puppet and puppeteer voice.\n",
        "The illusion that the puppet is generating the speech would break-down when the\n",
        "voice stimulus is presented too far away from the visual input (the puppet's\n",
        "location).\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToCxtWwha9tT"
      },
      "source": [
        "# Section 4: Bayesian decisions with compled utility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "uiKmKaJua9tU"
      },
      "source": [
        "#@title Video 3: Outro\n",
        "video = YouTubeVideo(id='UgeAtE8xZT8', width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNo6jiJqa9tU"
      },
      "source": [
        "widget = interact(plot_utility_gaussian,\n",
        "                  mu1 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ_prior\", continuous_update=False),\n",
        "                  mu2 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.5, description=\"µ_likelihood\", continuous_update=False),\n",
        "                  sigma1 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_prior\", continuous_update=False),\n",
        "                  sigma2 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_likelihood\", continuous_update=False),\n",
        "                  mu_g = FloatSlider(min=-4.0, max=4.0, step=0.01, value=1.0, description=\"µ_gain\", continuous_update=False),\n",
        "                  mu_c = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-1.0, description=\"µ_cost\", continuous_update=False),\n",
        "                  sigma_g = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_gain\", continuous_update=False),\n",
        "                  sigma_c = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_cost\", continuous_update=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U19yZlWXa9tU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9klJ8TmUa9tU"
      },
      "source": [
        "widget = interact(plot_switcher,\n",
        "                  what_to_plot = widgets.Dropdown(\n",
        "                      options=[\"Gaussian\", \"Uniform\", \"Gamma\"], \n",
        "                      value=\"Gaussian\", description=\"Prior: \"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qnVR5uQa9tU"
      },
      "source": [
        "In this tutorial, we introduced the ventriloquism setting that will form the basis of Tutorials 3 and 4 as well. We built a mixture-of-Gaussians prior that captures the participants' subjective experiences. In the next tutorials, we will use these to perform causal inference and predict the subject's responses to indvidual stimuli. "
      ]
    }
  ]
}