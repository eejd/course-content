{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W2D1_Tutorial2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3710jvsc74a57bd03e19903e646247cead5404f55ff575624523d45cf244c3f93aaf5fa10367032a",
      "display_name": "Python 3.7.10 64-bit ('nma': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eejd/course-content/blob/2021-bayes/tutorials/W2D1_BayesianStatistics/W2D1_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajm0vbDOa9tB"
      },
      "source": [
        "# Neuromatch Academy: Week 3, Day 1, Tutorial 2\n",
        "# Bayesian inference and decisions with continuous hidden state\n",
        "\n",
        "__Content creators:__ Eric DeWitt, Xaq Pitkow, Saeed Salehi, Ella Betty\n",
        "\n",
        "__Content reviewers:__ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISohim-Ta9tC"
      },
      "source": [
        "# Tutorial Objectives\n",
        "\n",
        "This notebook introduces you to Gaussians and Bayes' rule for continuous distributions, allowing us to model simple put powerful combinations of prior information and new measurements. In this notebook you will work through the same ideas we explored in the binary state tutorial, but you will be introduced to a new problem: finding and guiding Astrocat! You will see this problem again in more complex ways in the following days.\n",
        "\n",
        "In this notebook, you will:\n",
        "\n",
        "1. Learn about the Gaussian distribution and its nice properies\n",
        "2. Explore how we can extend the ideas from the binary hidden tutorial to continuous distributions\n",
        "3. Explore how different priors can produce more complex posteriors.\n",
        "4. Explore Loss functions often used in inference and complex utility functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upESZk9ga9tD"
      },
      "source": [
        "---\n",
        "##Setup  \n",
        "Please execute the cells below to initialize the notebook environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOzWD_Pga9tE"
      },
      "source": [
        "# imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import multivariate_normal\n",
        "from scipy.stats import gamma as gamma_distribution\n",
        "from matplotlib.transforms import Affine2D"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBLHwDYla9tE"
      },
      "source": [
        "#@title Figure Settings\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import FloatSlider\n",
        "from ipywidgets import interact, fixed, HBox, Layout, VBox, interactive, Label\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDKX2K8VcisP"
      },
      "source": [
        "# @title Plotting functions\n",
        "\n",
        "def plot_gaussian(μ, σ):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    y = gaussian(x, μ, σ)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(x, y, c='blue')\n",
        "    plt.fill_between(x, y, color='b', alpha=0.2)\n",
        "    plt.ylabel('$\\mathcal{N}(x, \\mu, \\sigma^2)$')\n",
        "    plt.xlabel('x')\n",
        "    plt.yticks([])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_losses(μ, σ):\n",
        "    x = np.linspace(-2, 2, 400, endpoint=True)\n",
        "    y = gaussian(x, μ, σ)\n",
        "    error = x - μ\n",
        "\n",
        "    mse_loss = (error)**2\n",
        "    abs_loss = np.abs(error)\n",
        "    zero_one_loss = (np.abs(error) >= 0.02).astype(np.float)\n",
        "\n",
        "    fig, (ax_gaus, ax_error) = plt.subplots(2, 1, figsize=(6, 8))\n",
        "    ax_gaus.plot(x, y, color='blue', label='true distribution')\n",
        "    ax_gaus.fill_between(x, y, color='blue', alpha=0.2)\n",
        "    ax_gaus.set_ylabel('$\\\\mathcal{N}(x, \\\\mu, \\\\sigma^2)$')\n",
        "    ax_gaus.set_xlabel('x')\n",
        "    ax_gaus.set_yticks([])\n",
        "    ax_gaus.legend(loc='upper right')\n",
        "\n",
        "    ax_error.plot(x, mse_loss, color='c', label='Mean Squared Error', linewidth=3)\n",
        "    ax_error.plot(x, abs_loss, color='m', label='Absolute Error', linewidth=3)\n",
        "    ax_error.plot(x, zero_one_loss, color='y', label='Zero-One Loss', linewidth=3)\n",
        "    ax_error.legend(loc='upper right')\n",
        "    ax_error.set_xlabel('$\\\\hat{\\\\mu}$')\n",
        "    ax_error.set_ylabel('Error')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_mvn2d(mu1, mu2, sigma1, sigma2, corr):\n",
        "    x, y = np.mgrid[-2:2:.02, -2:2:.02]\n",
        "    cov12 = corr * sigma1 * sigma2\n",
        "    z = mvn2d(x, y, mu1, mu2, sigma1, sigma2, cov12)\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.contourf(x, y, z, cmap='Reds')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_marginal(sigma1, sigma2, c_x, c_y, corr):\n",
        "    mu1, mu2 = 0.0, 0.0\n",
        "    cov12 = corr * sigma1 * sigma2\n",
        "    xx, yy = np.mgrid[-2:2:.02, -2:2:.02]\n",
        "    x, y = xx[:, 0], yy[0]\n",
        "    p_x = gaussian(x, mu1, sigma1)\n",
        "    p_y = gaussian(y, mu2, sigma2)\n",
        "    zz = mvn2d(xx, yy, mu1, mu2, sigma1, sigma2, cov12)\n",
        "\n",
        "    mu_x_y = mu1+cov12*(c_y-mu2)/sigma2**2\n",
        "    mu_y_x = mu2+cov12*(c_x-mu1)/sigma1**2\n",
        "    sigma_x_y = np.sqrt(sigma2**2 - cov12**2/sigma1**2)\n",
        "    sigma_y_x = np.sqrt(sigma1**2-cov12**2/sigma2**2)\n",
        "    p_x_y = gaussian(x, mu_x_y, sigma_x_y)\n",
        "    p_y_x = gaussian(x, mu_y_x, sigma_y_x)\n",
        "\n",
        "    p_c_y = gaussian(mu_x_y-sigma_x_y, mu_x_y, sigma_x_y)\n",
        "    p_c_x = gaussian(mu_y_x-sigma_y_x, mu_y_x, sigma_y_x)\n",
        "\n",
        "    # definitions for the axes\n",
        "    left, width = 0.1, 0.65\n",
        "    bottom, height = 0.1, 0.65\n",
        "    spacing = 0.01\n",
        "\n",
        "    rect_z = [left, bottom, width, height]\n",
        "    rect_x = [left, bottom + height + spacing, width, 0.2]\n",
        "    rect_y = [left + width + spacing, bottom, 0.2, height]\n",
        "\n",
        "    # start with a square Figure\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "\n",
        "    ax_z = fig.add_axes(rect_z)\n",
        "    ax_x = fig.add_axes(rect_x, sharex=ax_z)\n",
        "    ax_y = fig.add_axes(rect_y, sharey=ax_z)\n",
        "\n",
        "    ax_z.set_axis_off()\n",
        "    ax_x.set_axis_off()\n",
        "    ax_y.set_axis_off()\n",
        "    ax_x.set_xlim(np.min(x), np.max(x))\n",
        "    ax_y.set_ylim(np.min(y), np.max(y))\n",
        "\n",
        "    ax_z.contourf(xx, yy, zz, cmap='Greys')\n",
        "    ax_z.hlines(c_y, mu_x_y-sigma_x_y, mu_x_y+sigma_x_y, color='c', zorder=9, linewidth=3)\n",
        "    ax_z.vlines(c_x, mu_y_x-sigma_y_x, mu_y_x+sigma_y_x, color='m', zorder=9, linewidth=3)\n",
        "\n",
        "    ax_x.plot(x, p_x, label='$p(x)$', c = 'b', linewidth=3)\n",
        "    ax_x.plot(x, p_x_y, label='$p(x|y = C_y)$', c = 'c', linestyle='dashed', linewidth=3)\n",
        "    ax_x.hlines(p_c_y, mu_x_y-sigma_x_y, mu_x_y+sigma_x_y, color='c', linestyle='dashed', linewidth=3)\n",
        "\n",
        "    ax_y.plot(p_y, y, label='$p(y)$', c = 'r', linewidth=3)\n",
        "    ax_y.plot(p_y_x, y, label='$p(y|x = C_x)$', c = 'm', linestyle='dashed', linewidth=3)\n",
        "    ax_y.vlines(p_c_x, mu_y_x-sigma_y_x, mu_y_x+sigma_y_x, color='m', linestyle='dashed', linewidth=3)\n",
        "\n",
        "    ax_x.legend(loc=\"upper left\", frameon=False)\n",
        "    ax_y.legend(loc=\"lower right\", frameon=False)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_bayes(mu1, mu2, sigma1, sigma2):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    prior = gaussian(x, mu1, sigma1)\n",
        "    likelihood = gaussian(x, mu2, sigma2)\n",
        "\n",
        "    mu_post, sigma_post = product_guassian(mu1, mu2, sigma1, sigma2)\n",
        "    posterior = gaussian(x, mu_post, sigma_post)\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, prior, c='b', label='prior')\n",
        "    plt.fill_between(x, prior, color='b', alpha=0.2)\n",
        "    plt.plot(x, likelihood, c='r', label='likelihood')\n",
        "    plt.fill_between(x, likelihood, color='r', alpha=0.2)\n",
        "    plt.plot(x, posterior, c='k', label='posterior')\n",
        "    plt.fill_between(x, posterior, color='k', alpha=0.2)\n",
        "    plt.yticks([])\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.ylabel('$\\mathcal{N}(x, \\mu, \\sigma^2)$')\n",
        "    plt.xlabel('x')\n",
        "    plt.show()\n",
        "\n",
        "def plot_information(mu1, sigma1, mu2, sigma2):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    mu3, sigma3 = product_guassian(mu1, mu2, sigma1, sigma2)\n",
        "    prior = gaussian(x, mu1, sigma1)\n",
        "    likelihood = gaussian(x, mu2, sigma2)\n",
        "    posterior = gaussian(x, mu3, sigma3)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, prior, c='b', label='Satellite')\n",
        "    plt.fill_between(x, prior, color='b', alpha=0.2)\n",
        "    plt.plot(x, likelihood, c='r', label='Space Mouse')\n",
        "    plt.fill_between(x, likelihood, color='r', alpha=0.2)\n",
        "    plt.plot(x, posterior, c='k', label='Center')\n",
        "    plt.fill_between(x, posterior, color='k', alpha=0.2)\n",
        "    plt.yticks([])\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.ylabel('$\\mathcal{N}(x, \\mu, \\sigma^2)$')\n",
        "    plt.xlabel('x')\n",
        "    plt.show()\n",
        "\n",
        "def plot_information_global(mu3, sigma3, mu1, mu2):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    sigma1, sigma2 = reverse_product(mu3, sigma3, mu1, mu2)\n",
        "    prior = gaussian(x, mu1, sigma1)\n",
        "    likelihood = gaussian(x, mu2, sigma2)\n",
        "    posterior = gaussian(x, mu3, sigma3)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, prior, c='b', label='Satellite')\n",
        "    plt.fill_between(x, prior, color='b', alpha=0.2)\n",
        "    plt.plot(x, likelihood, c='r', label='Space Mouse')\n",
        "    plt.fill_between(x, likelihood, color='r', alpha=0.2)\n",
        "    plt.plot(x, posterior, c='k', label='Center')\n",
        "    plt.fill_between(x, posterior, color='k', alpha=0.2)\n",
        "    plt.yticks([])\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.ylabel('$\\mathcal{N}(x, \\mu, \\sigma^2)$')\n",
        "    plt.xlabel('x')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_loss_utility_gaussian(loss_f, mu, sigma, mu_true):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    posterior = gaussian(x, mu, sigma)\n",
        "\n",
        "    plot_loss_utility(x, posterior, loss_f, mu_true)\n",
        "\n",
        "\n",
        "def plot_loss_utility_mixture(loss_f, mu1, mu2, sigma1, sigma2, factor, mu_true):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    y_1 = gaussian(x, mu1, sigma1)\n",
        "    y_2 = gaussian(x, mu2, sigma2)\n",
        "    posterior = y_1 * factor + y_2 * (1.0 - factor)\n",
        "\n",
        "    plot_loss_utility(x, posterior, loss_f, mu_true)\n",
        "\n",
        "\n",
        "def plot_loss_utility(x, posterior, loss_f, mu_true):\n",
        "    mean, median, mode = calc_mean_mode_median(x, posterior)\n",
        "\n",
        "    loss = calc_loss_func(loss_f, mu_true, x)\n",
        "\n",
        "    utility = calc_expected_loss(loss_f, posterior, x)\n",
        "    min_expected_loss = x[np.argmin(utility)]\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.title(\"Probability\")\n",
        "    plt.plot(x, posterior, c='b')\n",
        "    plt.fill_between(x, posterior, color='b', alpha=0.2)\n",
        "    plt.yticks([])\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('$\\pi \\cdot p(x) + (1-\\pi) \\cdot p(y)$')\n",
        "    plt.axvline(mean, ls='dashed', color='red', label='Mean')\n",
        "    plt.axvline(median, ls='dashdot', color='blue', label='Median')\n",
        "    plt.axvline(mode, ls='dotted', color='green', label='Mode')\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    \n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.title(loss_f)\n",
        "    plt.plot(x, loss, c='c', label=loss_f)\n",
        "    # plt.fill_between(x, loss, color='c', alpha=0.2)\n",
        "    plt.ylabel('loss')\n",
        "    # plt.legend(loc=\"upper left\")\n",
        "    plt.xlabel('x')\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.title(\"Expected Loss\")\n",
        "    plt.plot(x, utility, c='y', label='$\\mathbb{E}[L]$')\n",
        "    plt.axvline(min_expected_loss, ls='dashed', color='red', label='$Min~ \\mathbb{E}[Loss]$')\n",
        "    # plt.fill_between(x, utility, color='y', alpha=0.2)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('$\\mathbb{E}[L]$')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_loss_utility_bayes(mu1, mu2, sigma1, sigma2, mu_true, loss_f):\n",
        "    x = np.linspace(-4, 4, 1000, endpoint=True)\n",
        "\n",
        "    prior = gaussian(x, mu1, sigma1)\n",
        "    likelihood = gaussian(x, mu2, sigma2)\n",
        "\n",
        "    mu_post, sigma_post = product_guassian(mu1, mu2, sigma1, sigma2)\n",
        "    posterior = gaussian(x, mu_post, sigma_post)\n",
        "\n",
        "    loss = calc_loss_func(loss_f, mu_true, x)\n",
        "\n",
        "    utility = - calc_expected_loss(loss_f, posterior, x)\n",
        "\n",
        "    plt.figure(figsize=(18, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "\n",
        "    plt.title(\"Posterior distribution\")\n",
        "    plt.plot(x, prior, c='b', label='prior')\n",
        "    plt.fill_between(x, prior, color='b', alpha=0.2)\n",
        "    plt.plot(x, likelihood, c='r', label='likelihood')\n",
        "    plt.fill_between(x, likelihood, color='r', alpha=0.2)\n",
        "    plt.plot(x, posterior, c='k', label='posterior')\n",
        "    plt.fill_between(x, posterior, color='k', alpha=0.2)\n",
        "    plt.yticks([])\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    # plt.ylabel('$f(x)$')\n",
        "    plt.xlabel('x')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(loss_f)\n",
        "    plt.plot(x, loss, c='c')\n",
        "    # plt.fill_between(x, loss, color='c', alpha=0.2)\n",
        "    plt.ylabel('loss')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"Expected utility\")\n",
        "    plt.plot(x, utility, c='y', label='utility')\n",
        "    # plt.fill_between(x, utility, color='y', alpha=0.2)\n",
        "    plt.legend(loc=\"upper left\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_simple_utility_gaussian(mu, sigma, mu_g, mu_c, sigma_g, sigma_c):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    posterior = gaussian(x, mu, sigma)\n",
        "    gain = gaussian(x, mu_g, sigma_g)\n",
        "    loss = gaussian(x, mu_c, sigma_c)\n",
        "    utility = np.multiply(posterior, gain) - np.multiply(posterior, loss)\n",
        "\n",
        "    plt.figure(figsize=(18, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"Probability\")\n",
        "    plt.plot(x, posterior, c='b', label='posterior')\n",
        "    plt.fill_between(x, posterior, color='b', alpha=0.2)\n",
        "    plt.yticks([])\n",
        "    # plt.legend(loc=\"upper left\")\n",
        "    plt.xlabel('x')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"utility function\")\n",
        "    plt.plot(x, gain, c='m', label='gain')\n",
        "    # plt.fill_between(x, gain, color='m', alpha=0.2)\n",
        "    plt.plot(x, -loss, c='c', label='loss')\n",
        "    # plt.fill_between(x, -loss, color='c', alpha=0.2)\n",
        "    plt.legend(loc=\"upper left\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"expected utility\")\n",
        "    plt.plot(x, utility, c='y', label='utility')\n",
        "    # plt.fill_between(x, utility, color='y', alpha=0.2)\n",
        "    plt.legend(loc=\"upper left\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_utility_gaussian(mu1, mu2, sigma1, sigma2, mu_g, mu_c, sigma_g, sigma_c, plot_utility_row=True):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    prior = gaussian(x, mu1, sigma1)\n",
        "    likelihood = gaussian(x, mu2, sigma2)\n",
        "\n",
        "    mu_post, sigma_post = product_guassian(mu1, mu2, sigma1, sigma2)\n",
        "    posterior = gaussian(x, mu_post, sigma_post)\n",
        "\n",
        "    if plot_utility_row:\n",
        "        gain = gaussian(x, mu_g, sigma_g)\n",
        "        loss = gaussian(x, mu_c, sigma_c)\n",
        "        utility = np.multiply(posterior, gain) - np.multiply(posterior, loss)\n",
        "        plot_bayes_utility_rows(x, prior, likelihood, posterior, gain, loss, utility)\n",
        "    else:\n",
        "        plot_bayes_row(x, prior, likelihood, posterior)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def plot_utility_mixture(mu_m1, mu_m2, sigma_m1, sigma_m2, factor,\n",
        "                         mu, sigma, mu_g, mu_c, sigma_g, sigma_c, plot_utility_row=True):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    y_1 = gaussian(x, mu_m1, sigma_m1)\n",
        "    y_2 = gaussian(x, mu_m2, sigma_m2)\n",
        "    prior = y_1 * factor + y_2 * (1.0 - factor)\n",
        "\n",
        "    likelihood = gaussian(x, mu, sigma)\n",
        "\n",
        "    posterior = np.multiply(prior, likelihood)\n",
        "    posterior = posterior / (posterior.sum() * (x[1] - x[0]))\n",
        "\n",
        "    if plot_utility_row:\n",
        "        gain = gaussian(x, mu_g, sigma_g)\n",
        "        loss = gaussian(x, mu_c, sigma_c)\n",
        "        utility = np.multiply(posterior, gain) - np.multiply(posterior, loss)\n",
        "        plot_bayes_utility_rows(x, prior, likelihood, posterior, gain, loss, utility)\n",
        "    else:\n",
        "        plot_bayes_row(x, prior, likelihood, posterior)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def plot_utility_uniform(mu, sigma, mu_g, mu_c, sigma_g, sigma_c, plot_utility_row=True):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    prior = np.ones_like(x) / (x.max() - x.min())\n",
        "    likelihood = gaussian(x, mu, sigma)\n",
        "\n",
        "    posterior = likelihood\n",
        "    # posterior = np.multiply(prior, likelihood)\n",
        "    # posterior = posterior / (posterior.sum() * (x[1] - x[0]))\n",
        "\n",
        "    if plot_utility_row:\n",
        "        gain = gaussian(x, mu_g, sigma_g)\n",
        "        loss = gaussian(x, mu_c, sigma_c)\n",
        "        utility = np.multiply(posterior, gain) - np.multiply(posterior, loss)\n",
        "        plot_bayes_utility_rows(x, prior, likelihood, posterior, gain, loss, utility)\n",
        "    else:\n",
        "        plot_bayes_row(x, prior, likelihood, posterior)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def plot_utility_gamma(alpha, beta, offset, mu, sigma, mu_g, mu_c, sigma_g, sigma_c, plot_utility_row=True):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    prior = gamma_pdf(x-offset, alpha, beta)\n",
        "    likelihood = gaussian(x, mu, sigma)\n",
        "\n",
        "    posterior = np.multiply(prior, likelihood)\n",
        "    posterior = posterior / (posterior.sum() * (x[1] - x[0]))\n",
        "\n",
        "    if plot_utility_row:\n",
        "        gain = gaussian(x, mu_g, sigma_g)\n",
        "        loss = gaussian(x, mu_c, sigma_c)\n",
        "        utility = np.multiply(posterior, gain) - np.multiply(posterior, loss)\n",
        "        plot_bayes_utility_rows(x, prior, likelihood, posterior, gain, loss, utility)\n",
        "    else:\n",
        "        plot_bayes_row(x, prior, likelihood, posterior)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def plot_bayes_row(x, prior, likelihood, posterior):\n",
        "\n",
        "    mean, median, mode = calc_mean_mode_median(x, posterior)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Prior and likelihood distribution\")\n",
        "    plt.plot(x, prior, c='b', label='prior')\n",
        "    plt.fill_between(x, prior, color='b', alpha=0.2)\n",
        "    plt.plot(x, likelihood, c='r', label='likelihood')\n",
        "    plt.fill_between(x, likelihood, color='r', alpha=0.2)\n",
        "    # plt.plot(x, posterior, c='k', label='posterior')\n",
        "    # plt.fill_between(x, posterior, color='k', alpha=0.2)\n",
        "    plt.yticks([])\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    # plt.ylabel('$f(x)$')\n",
        "    plt.xlabel('x')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Posterior distribution\")\n",
        "    plt.plot(x, posterior, c='k', label='posterior')\n",
        "    plt.fill_between(x, posterior, color='k', alpha=0.1)\n",
        "    plt.axvline(mean, ls='dashed', color='red', label='Mean')\n",
        "    plt.axvline(median, ls='dashdot', color='blue', label='Median')\n",
        "    plt.axvline(mode, ls='dotted', color='green', label='Mode')\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.yticks([])\n",
        "    plt.xlabel('x')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_bayes_utility_rows(x, prior, likelihood, posterior, gain, loss, utility):\n",
        "\n",
        "    mean, median, mode = calc_mean_mode_median(x, posterior)\n",
        "    max_utility = x[np.argmax(utility)]\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.title(\"Prior and likelihood distribution\")\n",
        "    plt.plot(x, prior, c='b', label='prior')\n",
        "    plt.fill_between(x, prior, color='b', alpha=0.2)\n",
        "    plt.plot(x, likelihood, c='r', label='likelihood')\n",
        "    plt.fill_between(x, likelihood, color='r', alpha=0.2)\n",
        "    # plt.plot(x, posterior, c='k', label='posterior')\n",
        "    # plt.fill_between(x, posterior, color='k', alpha=0.2)\n",
        "    plt.yticks([])\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    # plt.ylabel('$f(x)$')\n",
        "    plt.xlabel('x')\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.title(\"Posterior distribution\")\n",
        "    plt.plot(x, posterior, c='k', label='posterior')\n",
        "    plt.fill_between(x, posterior, color='k', alpha=0.1)\n",
        "    plt.axvline(mean, ls='dashed', color='red', label='Mean')\n",
        "    plt.axvline(median, ls='dashdot', color='blue', label='Median')\n",
        "    plt.axvline(mode, ls='dotted', color='green', label='Mode')\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.yticks([])\n",
        "    plt.xlabel('x')\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.title(\"utility function\")\n",
        "    plt.plot(x, gain, c='m', label='gain')\n",
        "    # plt.fill_between(x, gain, color='m', alpha=0.2)\n",
        "    plt.plot(x, -loss, c='c', label='loss')\n",
        "    # plt.fill_between(x, -loss, color='c', alpha=0.2)\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.xlabel('x')\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.title(\"expected utility\")\n",
        "    plt.plot(x, utility, c='y', label='utility')\n",
        "    # plt.fill_between(x, utility, color='y', alpha=0.2)\n",
        "    plt.axvline(max_utility, ls='dashed', color='red', label='Max utility')\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('utility')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def gaussian_mixture(mu1, mu2, sigma1, sigma2, factor):\n",
        "    assert 0.0 < factor < 1.0\n",
        "    x = np.linspace(-7.0, 7.0, 1000, endpoint=True)\n",
        "    y_1 = gaussian(x, mu1, sigma1)\n",
        "    y_2 = gaussian(x, mu2, sigma2)\n",
        "    mixture = y_1 * factor + y_2 * (1.0 - factor)\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, y_1, c='deepskyblue', label='p(x)', linewidth=3.0)\n",
        "    plt.fill_between(x, y_1, color='deepskyblue', alpha=0.2)\n",
        "    plt.plot(x, y_2, c='aquamarine', label='p(y)', linewidth=3.0)\n",
        "    plt.fill_between(x, y_2, color='aquamarine', alpha=0.2)\n",
        "    plt.plot(x, mixture, c='b', label='$\\pi \\cdot p(x) + (1-\\pi) \\cdot p(y)$',  linewidth=3.0)\n",
        "    plt.fill_between(x, mixture, color='b', alpha=0.2)\n",
        "    plt.yticks([])\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    # plt.ylabel('$f(x)$')\n",
        "    plt.xlabel('x')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_bayes_loss_utility_gaussian(loss_f, mu_true, mu1, mu2, sigma1, sigma2):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "\n",
        "    prior = gaussian(x, mu1, sigma1)\n",
        "    likelihood = gaussian(x, mu2, sigma2)\n",
        "    mu_post, sigma_post = product_guassian(mu1, mu2, sigma1, sigma2)\n",
        "    posterior = gaussian(x, mu_post, sigma_post)\n",
        "\n",
        "    loss = calc_loss_func(loss_f, mu_true, x)\n",
        "\n",
        "    plot_bayes_loss_utility(x, prior, likelihood, posterior, loss, loss_f)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def plot_bayes_loss_utility_uniform(loss_f, mu_true, mu, sigma):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "\n",
        "    prior = np.ones_like(x) / (x.max() - x.min())\n",
        "    likelihood = gaussian(x, mu, sigma)\n",
        "    posterior = likelihood\n",
        "\n",
        "    loss = calc_loss_func(loss_f, mu_true, x)\n",
        "\n",
        "    plot_bayes_loss_utility(x, prior, likelihood, posterior, loss, loss_f)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def plot_bayes_loss_utility_gamma(loss_f, mu_true, alpha, beta, offset, mu, sigma):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    prior = gamma_pdf(x-offset, alpha, beta)\n",
        "    likelihood = gaussian(x, mu, sigma)\n",
        "    posterior = np.multiply(prior, likelihood)\n",
        "    posterior = posterior / (posterior.sum() * (x[1] - x[0]))\n",
        "\n",
        "    loss = calc_loss_func(loss_f, mu_true, x)\n",
        "\n",
        "    plot_bayes_loss_utility(x, prior, likelihood, posterior, loss, loss_f)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def plot_bayes_loss_utility_mixture(loss_f, mu_true, mu_m1, mu_m2, sigma_m1, sigma_m2, factor, mu, sigma):\n",
        "    x = np.linspace(-7, 7, 1000, endpoint=True)\n",
        "    y_1 = gaussian(x, mu_m1, sigma_m1)\n",
        "    y_2 = gaussian(x, mu_m2, sigma_m2)\n",
        "    prior = y_1 * factor + y_2 * (1.0 - factor)\n",
        "    likelihood = gaussian(x, mu, sigma)\n",
        "\n",
        "    posterior = np.multiply(prior, likelihood)\n",
        "    posterior = posterior / (posterior.sum() * (x[1] - x[0]))\n",
        "\n",
        "    loss = calc_loss_func(loss_f, mu_true, x)\n",
        "\n",
        "    plot_bayes_loss_utility(x, prior, likelihood, posterior, loss, loss_f)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def plot_bayes_loss_utility(x, prior, likelihood, posterior, loss, loss_f):\n",
        "\n",
        "    mean, median, mode = calc_mean_mode_median(x, posterior)\n",
        "    expected_loss = calc_expected_loss(loss_f, posterior, x)\n",
        "    min_expected_loss = x[np.argmin(expected_loss)]\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.title(\"Prior and Likelihood\")\n",
        "    plt.plot(x, prior, c='b', label='prior')\n",
        "    plt.fill_between(x, prior, color='b', alpha=0.2)\n",
        "    plt.plot(x, likelihood, c='r', label='likelihood')\n",
        "    plt.fill_between(x, likelihood, color='r', alpha=0.2)\n",
        "    plt.yticks([])\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.xlabel('x')\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.title(\"Posterior\")\n",
        "    plt.plot(x, posterior, c='k', label='posterior')\n",
        "    plt.fill_between(x, posterior, color='k', alpha=0.1)\n",
        "    plt.axvline(mean, ls='dashed', color='red', label='Mean')\n",
        "    plt.axvline(median, ls='dashdot', color='blue', label='Median')\n",
        "    plt.axvline(mode, ls='dotted', color='green', label='Mode')\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.yticks([])\n",
        "    plt.xlabel('x')\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.title(loss_f)\n",
        "    plt.plot(x, loss, c='c', label=loss_f)\n",
        "    # plt.fill_between(x, loss, color='c', alpha=0.2)\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('x')\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.title(\"expected loss\")\n",
        "    plt.plot(x, expected_loss, c='y', label='$\\mathbb{E}[L]$')\n",
        "    # plt.fill_between(x, expected_loss, color='y', alpha=0.2)\n",
        "    plt.axvline(min_expected_loss, ls='dashed', color='red', label='$Min~ \\mathbb{E}[Loss]$')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('$\\mathbb{E}[L]$')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def loss_plot_switcher(what_to_plot):\n",
        "    if what_to_plot == \"Gaussian\":\n",
        "        widget = interact(plot_loss_utility_gaussian,\n",
        "            loss_f = widgets.Dropdown(\n",
        "                options=[\"Mean Squared Error\", \"Absolute Error\", \"Zero-One Loss\"], \n",
        "                value=\"Mean Squared Error\", description=\"Loss: \"),\n",
        "            mu = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ_estimate\", continuous_update=False),\n",
        "            sigma = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_estimate\", continuous_update=False),\n",
        "            mu_true = FloatSlider(min=-3.0, max=3.0, step=0.01, value=0.0, description=\"µ_true\", continuous_update=False))\n",
        "    elif what_to_plot == \"Mixture of Gaussians\":\n",
        "        widget = interact(plot_loss_utility_mixture,\n",
        "            mu1 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ_est_1\", continuous_update=False),\n",
        "            mu2 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ_est_2\", continuous_update=False),\n",
        "            sigma1 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_est_1\", continuous_update=False),\n",
        "            sigma2 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_est_2\", continuous_update=False),\n",
        "            factor = FloatSlider(min=0.0, max=1.0, step=0.01, value=0.5, description=\"π\", continuous_update=False),\n",
        "            mu_true = FloatSlider(min=-3.0, max=3.0, step=0.01, value=0.0, description=\"µ_true\", continuous_update=False),\n",
        "            loss_f = widgets.Dropdown(\n",
        "                options=[\"Mean Squared Error\", \"Absolute Error\", \"Zero-One Loss\"], \n",
        "                value=\"Mean Squared Error\", description=\"Loss: \"))\n",
        "\n",
        "\n",
        "def plot_prior_switcher(what_to_plot):\n",
        "    if what_to_plot == \"Gaussian\":\n",
        "        widget = interact(plot_utility_gaussian,\n",
        "                  mu1 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ_prior\", continuous_update=False),\n",
        "                  mu2 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.5, description=\"µ_likelihood\", continuous_update=False),\n",
        "                  sigma1 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_prior\", continuous_update=False),\n",
        "                  sigma2 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_likelihood\", continuous_update=False),\n",
        "                  mu_g = fixed(1.0),\n",
        "                  mu_c = fixed(-1.0),\n",
        "                  sigma_g = fixed(0.5),\n",
        "                  sigma_c = fixed(value=0.5),\n",
        "                  plot_utility_row=fixed(False))\n",
        "    elif what_to_plot == \"Uniform\":\n",
        "        widget = interact(plot_utility_uniform,\n",
        "                  mu = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.5, description=\"µ_likelihood\", continuous_update=False),\n",
        "                  sigma = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_likelihood\", continuous_update=False),\n",
        "                  mu_g = fixed(1.0),\n",
        "                  mu_c = fixed(-1.0),\n",
        "                  sigma_g = fixed(0.5),\n",
        "                  sigma_c = fixed(value=0.5),\n",
        "                  plot_utility_row=fixed(False))\n",
        "    elif what_to_plot == \"Gamma\":\n",
        "        widget = interact(plot_utility_gamma,\n",
        "                  alpha = FloatSlider(min=1.0, max=10.0, step=0.1, value=2.0, description=\"α_prior\", continuous_update=False),\n",
        "                  beta = FloatSlider(min=0.5, max=2.0, step=0.01, value=1.0, description=\"β_prior\", continuous_update=False),\n",
        "                  offset = FloatSlider(min=-6.0, max=2.0, step=0.1, value=0.0, description=\"offset\", continuous_update=False),\n",
        "                  mu = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.5, description=\"µ_likelihood\", continuous_update=False),\n",
        "                  sigma = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_likelihood\", continuous_update=False),\n",
        "                  mu_g = fixed(1.0),\n",
        "                  mu_c = fixed(-1.0),\n",
        "                  sigma_g = fixed(0.5),\n",
        "                  sigma_c = fixed(value=0.5),\n",
        "                  plot_utility_row=fixed(False))\n",
        "    elif what_to_plot == \"Mixture of Gaussians\":\n",
        "        widget = interact(plot_utility_mixture,\n",
        "                  mu_m1 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ_mix_1\", continuous_update=False),\n",
        "                  mu_m2 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.5, description=\"µ_mix_1\", continuous_update=False),\n",
        "                  sigma_m1 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_mix_1\", continuous_update=False),\n",
        "                  sigma_m2 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_mix_2\", continuous_update=False),\n",
        "                  factor = FloatSlider(min=0.0, max=1.0, step=0.01, value=0.5, description=\"π\", continuous_update=False),\n",
        "                  mu = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.5, description=\"µ_likelihood\", continuous_update=False),\n",
        "                  sigma = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_likelihood\", continuous_update=False),\n",
        "                  mu_g = fixed(1.0),\n",
        "                  mu_c = fixed(-1.0),\n",
        "                  sigma_g = fixed(0.5),\n",
        "                  sigma_c = fixed(value=0.5),\n",
        "                  plot_utility_row=fixed(False))\n",
        "        \n",
        "        \n",
        "def plot_bayes_loss_utility_switcher(what_to_plot):\n",
        "    if what_to_plot == \"Gaussian\":\n",
        "        widget = interact(plot_bayes_loss_utility_gaussian,\n",
        "                  mu1 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ_prior\", continuous_update=False),\n",
        "                  mu2 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.5, description=\"µ_likelihood\", continuous_update=False),\n",
        "                  sigma1 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_prior\", continuous_update=False),\n",
        "                  sigma2 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_likelihood\", continuous_update=False),\n",
        "                  mu_true = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ_true\", continuous_update=False),\n",
        "                  loss_f = widgets.Dropdown(\n",
        "                      options=[\"Mean Squared Error\", \"Absolute Error\", \"Zero-One Loss\"], \n",
        "                      value=\"Mean Squared Error\", description=\"Loss: \"))\n",
        "    elif what_to_plot == \"Uniform\":\n",
        "        widget = interact(plot_bayes_loss_utility_uniform,\n",
        "                  mu = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.5, description=\"µ_likelihood\", continuous_update=False),\n",
        "                  sigma = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_likelihood\", continuous_update=False),\n",
        "                  mu_true = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ_true\", continuous_update=False),\n",
        "                  loss_f = widgets.Dropdown(\n",
        "                      options=[\"Mean Squared Error\", \"Absolute Error\", \"Zero-One Loss\"], \n",
        "                      value=\"Mean Squared Error\", description=\"Loss: \"))\n",
        "    elif what_to_plot == \"Gamma\":\n",
        "        widget = interact(plot_bayes_loss_utility_gamma,\n",
        "                  alpha = FloatSlider(min=1.0, max=10.0, step=0.1, value=2.0, description=\"α_prior\", continuous_update=False),\n",
        "                  beta = FloatSlider(min=0.5, max=2.0, step=0.01, value=1.0, description=\"β_prior\", continuous_update=False),\n",
        "                  offset = FloatSlider(min=-6.0, max=2.0, step=0.1, value=0.0, description=\"offset\", continuous_update=False),\n",
        "                  mu = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.5, description=\"µ_likelihood\", continuous_update=False),\n",
        "                  sigma = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_likelihood\", continuous_update=False),\n",
        "                  mu_true = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ_true\", continuous_update=False),\n",
        "                  loss_f = widgets.Dropdown(\n",
        "                      options=[\"Mean Squared Error\", \"Absolute Error\", \"Zero-One Loss\"], \n",
        "                      value=\"Mean Squared Error\", description=\"Loss: \"))\n",
        "    elif what_to_plot == \"Mixture of Gaussians\":\n",
        "        widget = interact(plot_bayes_loss_utility_mixture,\n",
        "                  mu_m1 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ_mix_1\", continuous_update=False),\n",
        "                  mu_m2 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.5, description=\"µ_mix_1\", continuous_update=False),\n",
        "                  sigma_m1 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_mix_1\", continuous_update=False),\n",
        "                  sigma_m2 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_mix_2\", continuous_update=False),\n",
        "                  factor = FloatSlider(min=0.0, max=1.0, step=0.01, value=0.5, description=\"π\", continuous_update=False),\n",
        "                  mu = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.5, description=\"µ_likelihood\", continuous_update=False),\n",
        "                  sigma = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_likelihood\", continuous_update=False),\n",
        "                  mu_true = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ_true\", continuous_update=False),\n",
        "                  loss_f = widgets.Dropdown(\n",
        "                      options=[\"Mean Squared Error\", \"Absolute Error\", \"Zero-One Loss\"], \n",
        "                      value=\"Mean Squared Error\", description=\"Loss: \"))\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmFeqjqa9tF",
        "cellView": "form"
      },
      "source": [
        "# @title Helper functions\n",
        "\n",
        "def gaussian(x, μ, σ):\n",
        "    return np.exp(-((x - μ) / σ)**2 / 2) / np.sqrt(2 * np.pi * σ**2)\n",
        "\n",
        "\n",
        "def gamma_pdf(x, α, β):\n",
        "    return gamma_distribution.pdf(x, a=α, scale=1/β)\n",
        "\n",
        "\n",
        "def mvn2d(x, y, mu1, mu2, sigma1, sigma2, cov12):\n",
        "    mvn = multivariate_normal([mu1, mu2], [[sigma1**2, cov12], [cov12, sigma2**2]])\n",
        "    return mvn.pdf(np.dstack((x, y)))\n",
        "\n",
        "\n",
        "def product_guassian(mu1, mu2, sigma1, sigma2):\n",
        "    J_1, J_2 = 1/sigma1**2, 1/sigma2**2\n",
        "    J_3 = J_1 + J_2\n",
        "    mu_prod = (J_1*mu1/J_3) + (J_2*mu2/J_3)\n",
        "    sigma_prod = np.sqrt(1/J_3)\n",
        "    return mu_prod, sigma_prod\n",
        "\n",
        "\n",
        "def reverse_product(mu3, sigma3, mu1, mu2):\n",
        "    J_3 = 1/sigma3**2\n",
        "    J_1 = J_3 * (mu3 - mu2) / (mu1 - mu2)\n",
        "    J_2 = J_3 * (mu3 - mu1) / (mu2 - mu1)\n",
        "    sigma1, sigma2 = 1/np.sqrt(J_1), 1/np.sqrt(J_2)\n",
        "    return sigma1, sigma2\n",
        "\n",
        "\n",
        "def calc_mean_mode_median(x, y):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    pdf = y * (x[1] - x[0])\n",
        "    # Calc mode of an arbitrary function\n",
        "    mode = x[np.argmax(pdf)]\n",
        "\n",
        "    # Calc mean of an arbitrary function\n",
        "    mean = np.multiply(x, pdf).sum()\n",
        "\n",
        "    # Calc median of an arbitrary function\n",
        "    cdf = np.cumsum(pdf)\n",
        "    idx = np.argmin(np.abs(cdf - 0.5))\n",
        "    median = x[idx]\n",
        "\n",
        "    return mean, median, mode\n",
        "\n",
        "\n",
        "\n",
        "def calc_expected_loss(loss_f, posterior, x):\n",
        "    dx = x[1] - x[0]\n",
        "    expected_loss = np.zeros_like(x)\n",
        "    for i in np.arange(x.shape[0]):\n",
        "        loss = calc_loss_func(loss_f, x[i], x) # or mse or zero_one_loss\n",
        "        expected_loss[i] = np.sum(loss * posterior) * dx\n",
        "    return expected_loss\n",
        "\n",
        "def plot_mixture_prior(x, gaussian1, gaussian2, combined):\n",
        "    \"\"\"\n",
        "    DO NOT EDIT THIS FUNCTION !!!\n",
        "\n",
        "    Plots a prior made of a mixture of gaussians\n",
        "\n",
        "    Args:\n",
        "      x (numpy array of floats):         points at which the likelihood has been evaluated\n",
        "      gaussian1 (numpy array of floats): normalized probabilities for Gaussian 1 evaluated at each `x`\n",
        "      gaussian2 (numpy array of floats): normalized probabilities for Gaussian 2 evaluated at each `x`\n",
        "      posterior (numpy array of floats): normalized probabilities for the posterior evaluated at each `x`\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(x, gaussian1, '--b', LineWidth=2, label='Gaussian 1')\n",
        "    ax.plot(x, gaussian2, '-.b', LineWidth=2, label='Gaussian 2')\n",
        "    ax.plot(x, combined, '-r', LineWidth=2, label='Gaussian Mixture')\n",
        "    ax.legend()\n",
        "    ax.set_ylabel('Probability')\n",
        "    ax.set_xlabel('Orientation (Degrees)')\n",
        "\n",
        "def gaussian_mixture(mu1, mu2, sigma1, sigma2, factor):\n",
        "    assert 0.0 < factor < 1.0\n",
        "    x = np.linspace(-7.0, 7.0, 1000, endpoint=True)\n",
        "    y_1 = gaussian(x, mu1, sigma1)\n",
        "    y_2 = gaussian(x, mu2, sigma2)\n",
        "    mixture = y_1 * factor + y_2 * (1.0 - factor)\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, y_1, c='deepskyblue', label='p(x)', linewidth=3.0)\n",
        "    plt.fill_between(x, y_1, color='deepskyblue', alpha=0.2)\n",
        "    plt.plot(x, y_2, c='aquamarine', label='p(y)', linewidth=3.0)\n",
        "    plt.fill_between(x, y_2, color='aquamarine', alpha=0.2)\n",
        "    plt.plot(x, mixture, c='b', label='$\\pi \\cdot p(x) + (1-\\pi) \\cdot p(y)$',  linewidth=3.0)\n",
        "    plt.fill_between(x, mixture, color='b', alpha=0.2)\n",
        "    plt.yticks([])\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    # plt.ylabel('$f(x)$')\n",
        "    plt.xlabel('x')\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ3o_EHA1DNj"
      },
      "source": [
        "# Section 1: Astrocat!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AILoNnT-a9tH",
        "cellView": "form",
        "outputId": "5b342f4e-0693-4b51-b561-ede076fe06b2"
      },
      "source": [
        "# @title Video 1: Astrocat!\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id='doyA7RqbiPA', width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "video"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video available at https://youtube.com/watch?v=doyA7RqbiPA\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7f8b1d189790>"
            ],
            "text/html": "\n        <iframe\n            width=\"854\"\n            height=\"480\"\n            src=\"https://www.youtube.com/embed/doyA7RqbiPA?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        ",
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5EFGNSZC1xD"
      },
      "source": [
        "Remember, in this example, you can think of yourself as a scientist trying to decide where we belive Astrocat is, how to select a point estimate based on possible errors, and how to account for the uncertainty we have about the location of the satellite and the space mouse. In fact, this is the kind of problem real scientists working to control remote satellites face! However, we can also think of this as what your brain has does when it wants to determine a target to make a movement or hit a tennis ball! A number of classic experiments use this kind of framing to study how *optimal* human decisions or movements are! Some examples are in the further reading document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuSRwJf1B0OQ"
      },
      "source": [
        "# Section 2: Probability distribution of Astrocat location\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U7Ci7Foa9tI",
        "cellView": "form"
      },
      "source": [
        "# @title Video 2: Gaussian distributions\n",
        "from IPython.display import YouTubeVideo\n",
        "# video = YouTubeVideo(id='GdIwJWsW9-s', width=854, height=480, fs=1)\n",
        "# print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "# video"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmUq5NoaCDZj"
      },
      "source": [
        "We are going to think first about how Ground Control should estimate his position. We won't consider measurements yet, just how to represent the uncertainty we might have in general. We are now dealing with a continuous distribution - Astrocat's location can be any real number. In the last tutorial, we were dealing with a discrete distribution - the fish were either on one side or the other. \n",
        "\n",
        "So how do we represent the probability of each possible point (an infinite number) where the Astrocat could be? \n",
        "The Bayesian approach can be used on any probability distribution. While many variables in the world require representation using complex or unknown (e.g. empirical) distributions, we will be using the Gaussian distributions or extensions of it.\n",
        "\n",
        "The important concepts intoduced in this section are:\n",
        "\n",
        "1. The Gaussian distribution\n",
        "3. Mixtures of Gaussians"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCstEIvGB3F6"
      },
      "source": [
        "## Section 2.1: The Gaussian distribution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3t3GfV_BmMK"
      },
      "source": [
        "One distribution we will use throughout this tutorial is the **Gaussian distribution**, which is also sometimes called the normal distribution. \n",
        "\n",
        "This is a special, and commonly used, distribution for a couple reasons. It is actually the focus of one of the most important theorems in statistics: the Central Limit Theorem. This theorem tells us that if you sum a large number of samples of a variable, that sum is normally distributed *no matter what* the original distribution over a variable was. This is a bit too in-depth for us to get into now but check out links in the Bonus for more information. Additionally, Gaussians have some really nice mathematical properties that permit simple closed-form solutions to several important problems. As we will see later in this tutorial, we can extend Gaussians to be even more flexible and well approximate other distributions using mistures of Gaussians. In short, the Gaussian is probably the most important continous distribution to understand and use.g\n",
        "\n",
        "\n",
        "Gaussians have two parameters. The **mean** $\\mu$, which sets the location of its center. Its \"scale\" or spread is controlled by its **standard deviation** $\\sigma$ or its square, the **variance** $\\sigma^2$. These can be a bit easy to mix up: make sure you are careful about whether you are referring to/using standard deviation or variance.\n",
        "\n",
        "The equation for a Gaussian distribution on a variable $x$ is:\n",
        "\n",
        "$$\n",
        "\\mathcal{N}(\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right)\n",
        "$$\n",
        "\n",
        "In our example, $x$ is the location of the Astrocat in one direction. $\\mathcal{N}(\\mu,\\sigma^2)$ is a standard notation to refer to a **N**ormal (Gaussian) distribution. For example, $\\mathcal{N}(0, 1)$ denotes a Gaussian distribution with mean 0 and variance 1. The exact form of this equation is not particularly intuitive, but we will see how mean and standard deviation values affect the probability distribution.\n",
        "\n",
        "\n",
        "We won't implement a Gaussian distribution in code here but please refer to the pre-reqs refresher W0D5 T1 to do this if you need further clarification.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "DTGeaeVZLrLh"
      },
      "source": [
        "# @markdown Execute this cell to enable the function `gaussian`\n",
        "def gaussian(x, μ, σ):\n",
        "    return np.exp(-((x - μ) / σ)**2 / 2) / np.sqrt(2 * np.pi * σ**2)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmitvcV6a9tJ"
      },
      "source": [
        "### Interactive Demo 2.1: Exploring Gaussian parameters:\n",
        "\n",
        "Let's explore how the parameters of a Gaussian affect the distribution. Play with the demo below by changing the mean $\\mu$ and standard deviation $\\sigma$.\n",
        "\n",
        "Discuss the following:\n",
        "\n",
        "1. What does increasing $\\mu$ do? What does increasing $\\sigma$ do?\n",
        "2. If you wanted to know the probabilty of an event happing at $0$, can you find two different $\\mu$ and $\\sigma$ values that produce the same probabilty of an event at $0$?\n",
        "3. How many Gaussian's could produce the same probabilty at $0$?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OI1jBhLa9tJ",
        "cellView": "form",
        "colab": {
          "referenced_widgets": [
            "7aa497625c374242b5ef5e94bdc8587d"
          ]
        },
        "outputId": "9c8dc10d-8c13-4423-d006-7123833183f7"
      },
      "source": [
        "# @markdown Execute this cell to enable the widget\n",
        "widget = interact(plot_gaussian,\n",
        "                     μ = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.0, continuous_update=False),\n",
        "                     σ = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, continuous_update=False))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "interactive(children=(FloatSlider(value=0.0, continuous_update=False, description='μ', max=4.0, min=-4.0, step…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c5be668ba244a088a7be6d666279c0b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBY-bTNGIKKd"
      },
      "source": [
        "# to_remove explanation\n",
        "\n",
        "#. 1. Increasing u moves the distribution to the right along the x-axis. The center\n",
        "#.    of the distribution equals u - which makes sense as this is the mean! Increasing\n",
        "#.    the standard deviation makes the distribution wider.\n",
        "\n",
        "#.  2. Yes, you can! For example, keep the standard deviation the same and move the mean \n",
        "#.     from -2 to 2. At both of these, the probability at 0 is the same because the distribution\n",
        "#.     is symmetrical.\n",
        "\n",
        "#.  3. There are an infinite number of Gaussians (combinations of mean & standard deviation)\n",
        "#.     that could produce the same probability at 0"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAFMx5nMa9tK"
      },
      "source": [
        "## Section 2.2: Mixtures of Gaussians\n",
        "\n",
        "What if our continuous distribution isn't well described by a single bump? For example, what if the Astrocat is often either in one place or another - a Gaussian distribution would not capture this well! We need a multimodal distribution. Luckily, we can extend Gaussians into a *mixture of Gaussians*, which are more complex distributions.  \n",
        "\n",
        "In a Gaussian mixture distribution, you are essentially adding two or more weighted standard Gaussian distributions (and then normalizing so everything integrates to 1). Each standard Gaussian involved is described, as normal, by its mean and standard deviation. Additional parameters in a mixture of Gaussians are the weights you put on each Gaussian (π). The following demo should help clarify how a mixture of Gaussians relates to the standard Gaussian components. We will not cover the derivation here but you can work it out as a bonus exercise.\n",
        "\n",
        "Mixture distributions are a common tool in Bayesian modeling and an important tool in general.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47PQTKPtLhfx"
      },
      "source": [
        "### Interactive Demo 2.2: Exploring Gaussian mixtures\n",
        "\n",
        "We will examing a mixture of two Gaussians. We will have one weighting parameter, π, that tells us how to weight one of the Gaussians. The other is weighted by 1 - π. \n",
        "\n",
        "Use the following widget to experiment with the parameters of each Gaussian and the mixing weight ($\\pi$) to undersand how the mixture of Gaussian distribution behaves.\n",
        "\n",
        "Discuss the following questions:\n",
        "\n",
        "1. What does increasing the weight $\\pi$ do to the mixture distribution (dark blue)? \n",
        "2. How can you make the two bumps of the mixture distribution further apart?\n",
        "3. Can you make the mixture distribution have just one bump (like a Gaussian)?\n",
        "4. Any other shapes you can make the mixture distribution resemble other than one nicely rounded bump or two separate bumps?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoUoeB5Ka9tL",
        "cellView": "form",
        "colab": {
          "referenced_widgets": [
            "9169c877d65f4775843f61885859e7b0"
          ]
        },
        "outputId": "11294dd7-64c8-4067-8a74-155ee9496ae2"
      },
      "source": [
        "# @markdown Execute this cell to enable the widget\n",
        "widget = interact(gaussian_mixture,\n",
        "            mu1 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=1.0, description=\"µ_1\", continuous_update=False),\n",
        "            mu2 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-1.0, description=\"µ_2\", continuous_update=False),\n",
        "            sigma1 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_1\", continuous_update=False),\n",
        "            sigma2 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_2\", continuous_update=False),\n",
        "            factor = FloatSlider(min=0.1, max=0.9, step=0.01, value=0.5, description=\"π\", continuous_update=False))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "interactive(children=(FloatSlider(value=1.0, continuous_update=False, description='µ_1', max=4.0, min=-4.0, st…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8efde42bb3ee4bdbb3b180c0bb75c493"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Weg1p48yPwlJ"
      },
      "source": [
        "# to_remove explanation\n",
        "\n",
        "#.  1) Increasing the weight parameter makes the mixture distribution more closely \n",
        "#.   resemble p(x). This makes sense because it is weighting p(x) in the sum of Gaussians.\n",
        "\n",
        "\n",
        "#.  2) You can move the two bumps of the mixture model further apart by making the means\n",
        "#.    u_1 and u_2 of the two Gaussians more different (having one at -4 and one at 4 for\n",
        "#.    example)\n",
        "\n",
        "\n",
        "#.  3) If you make the means of the two Gaussians very similar, the mixture will resemble \n",
        "#.     a single Gaussian (u_1 = 0.25, u_2 = 0.3 for example)\n",
        "\n",
        "#.  4) You can make a bunch of shapes if the two Gaussian components overlap at all.\n",
        "#.     If they're completely separated, you'll just get two Gaussian looking bumps\n",
        "#.     "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnYT7EA2a9tL"
      },
      "source": [
        "# Section 3: Utility \n",
        "\n",
        "We want to know where Astrocat is. If we were asked to provide the coordinates, for example to display them for Ground Control or to note them in a log, we are not going to provide the whole probability distribution! We will give a single set of coordinates, but we first need to estimate those coordinates. Just like in the last tutorial, this may not be as easy as just what is most likely: we want to know how good or bad it is if we guess a certain location and the Astrocat is in another. \n",
        "\n",
        "\n",
        "As we have seen, utility represents the gain (or if negative, loss) for us if we take a certain action for a certain value of the hidden state. In our continuous example, we need a function to be able to define the utility with respect to all possible continuous values of the state. Our action here is our guess of the Astrocat location.\n",
        "\n",
        "We are going to explore this for the Gaussian distribution, where our estimate is $\\hat{\\mu}$ and the true hidden state we are interested in is $\\mu$. \n",
        "\n",
        "A loss function determines the \"cost\" (or penalty) of estimating $\\hat \\mu$ when the true or correct quantity is really $\\mu$ (this is essentially the cost of the error between the true hidden state we are interested in: $\\mu$ and our estimate: $\\hat \\mu$). A loss function is equivalent to a negative utility function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg496HU2VUyq"
      },
      "source": [
        "## Section 3.1: Standard loss functions\n",
        "\n",
        "There are lots of different possible loss functions. We will focus on three: **mean-squared error** where the loss is the different between truth and estimate squared, **absolute error** where the loss is the absolute difference between truth and estimate, and **Zero-one Loss** where the loss is 1 unless we're exactly right (the estimate equals the truth). We can represent these with the following formulas:\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\textrm{Mean Squared Error} &=& (\\mu - \\hat{\\mu})^2 \\\\ \n",
        "\\textrm{Absolute Error} &=& \\big|\\mu - \\hat{\\mu}\\big| \\\\ \n",
        "\\textrm{Zero-One Loss} &=& \\begin{cases}\n",
        "                            0,& \\textrm{if } \\mu = \\hat{\\mu} \\\\\n",
        "                            1,              & \\textrm{otherwise}\n",
        "                            \\end{cases}\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "We will now explore how these different loss functions change our expected utility!\n",
        "\n",
        "Check out the next cell to see the implementation of each loss in the function `calc_loss_func`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "leV7Mh4lT_c6"
      },
      "source": [
        "# @markdown Execute this cell to enable the function `calc_loss_func`\n",
        "\n",
        "def calc_loss_func(loss_f, mu_true, x):\n",
        "    error = x - mu_true\n",
        "    if loss_f == \"Mean Squared Error\":\n",
        "        loss = (error)**2\n",
        "    elif loss_f == \"Absolute Error\":\n",
        "        loss = np.abs(error)\n",
        "    elif loss_f == \"Zero-One Loss\":\n",
        "        loss = (np.abs(error) >= 0.03).astype(np.float)\n",
        "    return loss\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkDdBE6CmOhF"
      },
      "source": [
        "### Interactive demo 3: Exploring Loss with different distributions\n",
        "\n",
        "Let's see how our loss functions interact with probability distributions to affect expected utility and consequently, the action we take.\n",
        "\n",
        "Play with the widget below and discuss the following:\n",
        "\n",
        "1. With a Gaussian distribution, does the peak of the expected utility ever change position on the x-axis for the three different losses? This peak denotes the action we would choose (the location we would guess) so in other words, would the different choices of loss function affect our action?\n",
        "2. With a mixture of Gaussian distribution with two bumps, does the peak of the expected loss ever change position on the x-axis for the three different losses?\n",
        "3.  Find paramters for a mixture of Gaussians that results in the mean, mode, and median all being distinct (not equal to one another). With this distribution, how does the peak of the expected utility correspond to the mean/median/mode of the probability distribution for each of the three losses?\n",
        "4. When the mixture of Gaussians has two peeks that are exactly the same height, how many modes are there?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU2hvdcfmOhF",
        "cellView": "form",
        "colab": {
          "referenced_widgets": [
            "0d2aea01bc1740718d8b439896ed45a6"
          ]
        },
        "outputId": "e26fee6b-5038-441d-f752-6e961e468c9c"
      },
      "source": [
        "# @markdown Execute this cell to enable the widget\n",
        "\n",
        "widget = interact(loss_plot_switcher,\n",
        "                  what_to_plot = widgets.Dropdown(\n",
        "                      options=[\"Gaussian\", \"Mixture of Gaussians\"], \n",
        "                      value=\"Gaussian\", description=\"Distribution: \"))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "interactive(children=(Dropdown(description='Distribution: ', options=('Gaussian', 'Mixture of Gaussians'), val…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d1a1164fb874b1d83630b7019b0a2e5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQrJxdU4Yf-G"
      },
      "source": [
        "# to_remove explanation\n",
        "\n",
        "#.  1) No, no matter what parameters we choose for the Gaussian, the peak of the expected\n",
        "#.    utility is the same. In other words, we would choose the same action (provide the same\n",
        "#.    location estimate) for all 3 estimates.\n",
        "\n",
        "#.  2) Yes, the peak of expected utility is in different locations for each loss when using\n",
        "#.     a mixture of Gaussians distribution. \n",
        "\n",
        "#.  3) When using mean-squared error, the peak is at the location of the mean. For \n",
        "#.     absolute error, the peak is located at the median. And for zero-one loss, the \n",
        "#.     peaks are at the two mode values.\n",
        "\n",
        "#.  4) When a distribution has more than one maximum, it is multi-modal! This means\n",
        "#.     it can have more than one mode. You will only ever have one mean and one median.\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc4m_DFdZ_AR"
      },
      "source": [
        "You can see that what coordinates you would provide for Astrocat aren't necessarily easy to guess just from the probability distribution. You need the concept of utility/loss and a specific loss function to determine what estimate you should give.\n",
        "\n",
        "For symetric distributions, you will find that the mean, median and mode are the same. However, for distributions with *skew*, like the Gamma distribution or the Exponential distribution, these will be different. You will be able to explore more distributions as priors below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfsjNwKMa9tJ"
      },
      "source": [
        "## Interactive Demo: Multiplying Gaussians \n",
        "\n",
        "We have implemented the multiplication of two Gaussians for you. Using the following widget, we are going to think about the information and combination of two Gaussians. It is important to remember, this isn't combining the underlying random variables! In our case, imagine we want to find the location least likely to contain the satellite or space mouse. This would be the center (average) of the two locations. Because we have uncertainty, we need to weight our uncertianty in thinking about the most likely place. Or imagine you want to know how much information is gained combining (averaging) the response of two neurons that represent locations in sensory space (think: how much information is shared by their receptive fields). In any case where we need to multiply two Gaussian distributions, we will also be weighting the information they each have about their center.\n",
        "\n",
        "Remember:\n",
        "\n",
        "$$\n",
        "\\mu_{3} = a\\mu_{1} + (1-a)\\mu_{2}\n",
        "$$\n",
        "$$\n",
        "\\sigma_{3}^{-2} = \\sigma_{1}^{-2} + \\sigma_{2}^{-2}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "a = \\frac{\\sigma_{1}^{-2}}{\\sigma_{1}^{-2} + \\sigma_{2}^{-2}}\n",
        "$$\n",
        "\n",
        "Questions:\n",
        "\n",
        "1. What is your uncertainty (how much information) do you have about $\\mu_{3}$ with $\\mu_{1} = -2, \\mu_{2} = 2, \\sigma_{1} = \\sigma_{2} = 0.5$?\n",
        "2. What happens to your estimate of $\\mu_{3}$ as $\\sigma_{2} \\to \\infty$? (In this case, $\\sigma$ only goes to 11... but that should be loud enough.)\n",
        "3. What is the differene in your estimate of $\\mu_{3}$ if $\\sigma_{1} = \\sigma_{2} = 11$? What has changed from the first example?\n",
        "4. Set $\\mu_{1} = -4, \\mu_{2} = 4$ and change the $\\sigma$s so that $\\mu_{3}$ is close to $2$. How many $\\sigma$s will produce the same $\\mu_{3}$?\n",
        "5. Continuing, if you set $\\mu_{1} = 0$, what $\\sigma$ do you need to change so $\\mu_{3} ~= 2$?\n",
        "6. If $\\sigma_{1} = \\sigma_{2} = 0.1$, how much information do you have about the average?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUaAFKMaa9tK",
        "colab": {
          "referenced_widgets": [
            "d99215b344f34280a644b633e274baf3"
          ]
        },
        "outputId": "3b748d39-5c78-4a84-ea3a-2232ac74adf9"
      },
      "source": [
        "widget = interact(plot_information,\n",
        "                  mu1 = FloatSlider(min=-5.0, max=-0.51, step=0.01, value=-2.0, description=\"µ_1\",continuous_update=False),\n",
        "                  mu2 = FloatSlider(min=0.5, max=5.01, step=0.01, value=2.0, description=\"µ_2\",continuous_update=False),\n",
        "                  sigma1 = FloatSlider(min=0.1, max=11.01, step=0.01, value=1.0, description=\"σ_1\", continuous_update=False),\n",
        "                  sigma2 = FloatSlider(min=0.1, max=11.01, step=0.01, value=1.0, description=\"σ_2\", continuous_update=False)\n",
        "                  )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "interactive(children=(FloatSlider(value=-2.0, continuous_update=False, description='µ_1', max=-0.51, min=-5.0,…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97a9809ae7a643ebac2e5b07d1972aac"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to_remove explination\n",
        "\n",
        "#. 1) Information is ~ 1/variance, so the new information you have is roughly 1/(0.5^2 + 0.5^2)\n",
        "#.    (compared to 1/0.5^2) for each original measurement.\n",
        "\n",
        "#. 2) The estimate will be almost entirely dependent on the mu_{1}! There is almost no\n",
        "#.    information from mu_{2}.\n",
        "\n",
        "#. 3) Because the variances are the same, the amount of information you have about the center\n",
        "#.    is lower (very low in fact), but the mean doesn't change!\n",
        "\n",
        "#. 4) There are an infinite number of variances that will produce the same (relative) weighting.\n",
        "#.    The only thing that matters is the relative means and relative variances!\n",
        "\n",
        "#. 5) This is the same intuition, it's the relative weightings that matter, so you can only\n",
        "#.    think about the result (in this case the variance of the second Gaussian) relative to\n",
        "#.    the first.\n",
        "\n",
        "#. 6) As the variances -> zero, the amount of information goes to infinity!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeIeWJvsOnP7"
      },
      "source": [
        "## Section 3.2: A more complex loss function\n",
        "\n",
        "The loss functions were just explore were fairly simple and are often used. However, life can be complicated and in this case, Astrocat cares about both being near the space mouse and avoiding the satellite. This means we need a more complex loss function that captures this! \n",
        "\n",
        "We know that we want to estimte Astrocat to be closer to the mouse, which is safe and Astrocat likes, but further away from the satellite, which is dangerous! So, rather than thinking about the *Loss* function, we will consider a generalized utility function that considers gains and losses that *matter* to Astrocat!\n",
        "\n",
        "In this case, we can assume that depending on our uncertainty about Astrocat's probable location, we may want to 'guess' that Astrocat is close to 'good' parts of space and further from 'bad' parts of space. We will model these utilities as Gaussian gain and loss regions--and we can assume they width of the Gaussian comes from our uncertainty over where the Space Mouse and satellite are.\n",
        "\n",
        "Let's explore how this works in the next interactive demo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyKZ9EAQayo8"
      },
      "source": [
        "### Interactive demo 3.2: Complicated cat costs\n",
        "\n",
        "Now that we have explored *Loss* functions that can be used to determine both formal *estimators* and our expected loss given our error, we are going to see what happens to our estimates if we use a generalized utility function.\n",
        "\n",
        "Questions:\n",
        "\n",
        "1. As you change the $\\mu$ of Astrocat, what happens to the expected utility?\n",
        "2. Can the EU be exactly zero everywhere?\n",
        "3. Can the EU be zero in a region around Astrocat but positive and negative elsewhere?\n",
        "4. As our uncertainty about Astrocat's position increases, what happens to the expected utility?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkjK51i9a9tM",
        "cellView": "form",
        "colab": {
          "referenced_widgets": [
            "c04e19f6f8ef4146a71929f905924110"
          ]
        },
        "outputId": "cb69cd3c-cd0e-46fb-b33b-1916e44dd730"
      },
      "source": [
        "# @markdown Execute this cell to enable the widget\n",
        "\n",
        "widget = interact(plot_simple_utility_gaussian,\n",
        "                  mu = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ\", continuous_update=False),\n",
        "                  sigma = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ\", continuous_update=False),\n",
        "                  mu_g = FloatSlider(min=-4.0, max=4.0, step=0.01, value=1.0, description=\"µ_gain\", continuous_update=False),\n",
        "                  mu_c = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-1.0, description=\"µ_cost\", continuous_update=False),\n",
        "                  sigma_g = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_gain\", continuous_update=False),\n",
        "                  sigma_c = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_cost\", continuous_update=False))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "interactive(children=(FloatSlider(value=-0.5, continuous_update=False, description='µ', max=4.0, min=-4.0, ste…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfa9876aaf6e4a5db96a0accc8a2a8e0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_kRk4ZGVphp"
      },
      "source": [
        "# to_remove explanation\n",
        "\n",
        "#. 1) As Astrocat's mean get closer to the mean of the gain (or loss), the EU become dominated\n",
        "#.    by only the gain or loss.\n",
        "\n",
        "#. 2) Only if the mean and variances of both the gain and loss regions are exactly the same.\n",
        "# .   (Set one of the variances 0.01 more than the other to see this.)\n",
        "\n",
        "#. 3) If the variances of the gain and loss function are small enough relative to the position\n",
        "#.    of Astrocat, there will be a 'neutural' region. As the variances increase, this will go away.\n",
        "\n",
        "#. 4) As the uncertainty of Astrocat's location increases (relative to the gain and loss variances).\n",
        "#.    there will be a continuous increase in utility from the peak of the loss region to the peak of the\n",
        "#.    gain region. Also, this will depend on the mean of Astrocat's distribution! The larger the variance\n",
        "#.    the more sensitive the expected utility is to the gains and losses!"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0to3dQjna9tM"
      },
      "source": [
        "# Section 4: Correlation and marginalization\n",
        "\n",
        "In this section we will explore a two dimensional Gaussian, often defined as a two-dimension vector of Gaussian random variables. This is, in essence, the joint distribution of two Gaussian random variables.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY_TZXVaXNoh"
      },
      "source": [
        "## Section 4.1: Correlation\n",
        "\n",
        "If the two variables in a two dimensional Gaussian are independent, looking at one tells us nothing about the other. But what if the the two variables are correlated (covary)?\n",
        "\n",
        "The covariance of two Gaussians is:\n",
        "\n",
        "$$\n",
        "\\sigma_{XY} = E[(X-\\mu_{X})(Y-\\mu_{Y})]\n",
        "$$\n",
        "\n",
        "The correlation is the covariance normalized, so that it goes between -1 (exactly anticorrelated) to 1 (exactly correlated).\n",
        "\n",
        "$$\n",
        "\\rho_{XY} = \\frac{\\sigma_{XY}}{\\sigma_{X}\\sigma_{Y}}\n",
        "$$\n",
        "\n",
        "These are key concepts and while we are considering two hidden states (or two random variables), they extend to $N$ dimensional vectors of Gaussian random variables. You will find these used all over computational neuroscience.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVIMVEw3XVxM"
      },
      "source": [
        "### Interactive demo 4.1: Covarying 2D Gaussian\n",
        "\n",
        "Let's explore this 2D Gaussian (i.e. joint distribution of two Gaussians).\n",
        "\n",
        "Use the following widget to think about the following questions:\n",
        "\n",
        "1. If these variables represent hidden states we care about, what does observering one tell us about the other?\n",
        "2. How does the shape of the distribution change when we change the means? The variances? The correlation?\n",
        "3. If we want to isolate one or the other hidden state distributions, what do we need to do? (Hint: think about Tutorial 1.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlxISLZCa9tN",
        "cellView": "form",
        "colab": {
          "referenced_widgets": [
            "e3d602f4d7dc42d7a14ac816c8e23e43"
          ]
        },
        "outputId": "5f14c99f-6944-47b1-8104-75d8a8fb1afa"
      },
      "source": [
        "# @markdown Execute the cell to enable the widget\n",
        "widget = interact(plot_mvn2d,\n",
        "                  mu1 = FloatSlider(min=-1.0, max=1.0, step=0.01, value=0.0, description=\"µ_1\", continuous_update=False),\n",
        "                  mu2 = FloatSlider(min=-1.0, max=1.0, step=0.01, value=0.0, description=\"µ_2\", continuous_update=False),\n",
        "                  sigma1 = FloatSlider(min=0.1, max=1.5, step=0.01, value=0.5, description=\"σ_1\", continuous_update=False),\n",
        "                  sigma2 = FloatSlider(min=0.1, max=1.5, step=0.01, value=0.5, description=\"σ_2\", continuous_update=False),\n",
        "                  corr = FloatSlider(min=-0.99, max=0.99, step=0.01, value=0.0, description=\"ρ\", continuous_update=False))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "interactive(children=(FloatSlider(value=0.0, continuous_update=False, description='µ_1', max=1.0, min=-1.0, st…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9afa0baed34e4d0ab446aced300c8d73"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4keOek6fbWxU"
      },
      "source": [
        "# to_remove explanation\n",
        "\n",
        "#. 1) The higher the correlation, the more shared information there is. So, the probabilities of the  \n",
        "#.    second hidden state are more dependent on the first (and vice versa).\n",
        "\n",
        "#. 2) The means control only the location! The variances determine the spread in X and Y. The\n",
        "#.    correlation is the only factor that controls the degree of the 'rotation', where we can think\n",
        "#.    about the correlation as forcing the distribution to be more along one of the diagonals or ther\n",
        "#.    other.\n",
        "\n",
        "#. 3) We would need to marginalize! We will do this next."
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6958VJrOnP9"
      },
      "source": [
        "## Section 4.2: Marginalization and information\n",
        "\n",
        "We learned in Tutorial 1 that if we want to measure the probability of one or another variable, we need to average over the other. When we extend this to the correlated Gaussians we just played with, marginalization works the same way. Let's say that the two variables reflect Astrocat's position in space (in two dimensions). If we want to get our uncertainy about Astrocat's X or Y position, we need to marinalize. However, let's imagine we have a measurement from one of the variables, for example X, and we want to understand the uncertainy we have in Y. We can then calculate the conditional probability $P(Y|X=x)$. You can explore the relationship between these two concepts in the following interactive demo.\n",
        "\n",
        "But first, let's remember that we can also think about the amount of uncertainty as inversely proportional to the amount of information we have about each variable. This is important, because the joint information is determined by the correlation. For our Bayesian approach, the intuition that is important is that we can also think about the mutual information between the prior and the likelihood following a measurement.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm0EBw0qXjHQ"
      },
      "source": [
        "### Interactive demo 4.2: Marginalizing 2D Gaussians\n",
        "\n",
        "Use the following widget to think consider the following questions:\n",
        "\n",
        "1. When is the marginal distribution the same as the conditional probability distribution? Why?\n",
        "2. If $\\rho$ is large, how much information can we gain (in addition) looking at both variables vs just considering one?\n",
        "3. If $\\rho$ is close to zero, but the variances of the two variables are very different, what happens to the conditional probability compared to the marginals? As $\\rho$ changes?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv8Y-n3Ma9tN",
        "cellView": "form",
        "colab": {
          "referenced_widgets": [
            "50b54658ca5d42aaa80d83fa1eba6f70"
          ]
        },
        "outputId": "776b7e38-ed85-4b69-882d-007132988089"
      },
      "source": [
        "# @markdown Execute this cell to enable the widget\n",
        "widget = interact(plot_marginal,\n",
        "                  sigma1 = FloatSlider(min=0.1, max=1.1, step=0.01, value=0.5, description=\"σ_x\", continuous_update=False),\n",
        "                  sigma2 = FloatSlider(min=0.1, max=1.1, step=0.01, value=0.5, description=\"σ_y\", continuous_update=False),\n",
        "                  c_x = FloatSlider(min=-1.0, max=1.0, step=0.01, value=0.0, description=\"Cx\", continuous_update=False),\n",
        "                  c_y = FloatSlider(min=-1.0, max=1.0, step=0.01, value=0.0, description=\"Cy\", continuous_update=False),\n",
        "                  corr = FloatSlider(min=-1.0, max=1.0, step=0.01, value=0.0, description=\"ρ\", continuous_update=False))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "interactive(children=(FloatSlider(value=0.5, continuous_update=False, description='σ_x', max=1.1, min=0.1, ste…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "febceb15097241dda1a0d857238be316"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gzn4CjTxTcTC"
      },
      "source": [
        "# to_remove explanation\n",
        "\n",
        "#. 1) The conditional probability distribution is using a measurement to restrict the likely value of\n",
        "#.    one of the variables. If there is correlation, this will also effect what we know (conditionally)\n",
        "#.    about the other! However, the marginal probability *only* depends on the direction along\n",
        "#.    which we are marginalizing. So, when the conditional probabiltiy is based on a measurement at the\n",
        "#.    means, it is the same as marginalization, as there is no additional information. A Further note\n",
        "#.    is that we can also marginalize along other directions (e.g. a diagonal), but we are not exploring\n",
        "#.    this here.\n",
        "\n",
        "#. 2) The larger the correlation, the more shared information. So the more we gain about the\n",
        "#.    second variable (or hidden state) by measuring a value from the other.\n",
        "\n",
        "#. 3) The variable (hidden state) with the lower variance will produce a narrower\n",
        "#.    conditional probabilty for the other variable! As you shift the correlation, you will see\n",
        "#.    small changes in the variable with the low variance shifting the conditional mean of the \n",
        "#.    variable with the large variance! (So, if X has low variance, changing CY has a big effect.)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DQCv8Zma9tO"
      },
      "source": [
        "# Section 5: Bayes' theorem for continuous distributions\n",
        "\n",
        "## The Gausian example\n",
        "\n",
        "Bayes' rule tells us how to combine two sources of information: the prior (e.g., a noisy representation of Ground Control's expectations about where Astrocat is) and the likelihood (e.g., a noisy representation of the Astrocat after taking a measurement), to obtain a posterior distribution (our belief distribution) taking into account both pieces of information. Remember Bayes' rule:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\text{Posterior} = \\frac{ \\text{Likelihood} \\times \\text{Prior}}{ \\text{Normalization constant}}\n",
        "\\end{eqnarray}\n",
        "\n",
        "When both the prior and likelihood are Gaussians, this translates into the following form:\n",
        "\n",
        "$$\n",
        "\\begin{array}{rcl}\n",
        "\\text{Likelihood} &=& \\mathcal{N}(\\mu_{likelihood},\\sigma_{likelihood}^2) \\\\\n",
        "\\text{Prior} &=& \\mathcal{N}(\\mu_{prior},\\sigma_{prior}^2) \\\\\n",
        "\\text{Posterior} &=& \\mathcal{N}\\left( \\frac{\\sigma^2_{likelihood}\\mu_{prior}+\\sigma^2_{prior}\\mu_{likelihood}}{\\sigma^2_{likelihood}+\\sigma^2_{prior}}, \\frac{\\sigma^2_{likelihood}\\sigma^2_{prior}}{\\sigma^2_{likelihood}+\\sigma^2_{prior}} \\right) \\\\\n",
        "&\\propto& \\mathcal{N}(\\mu_{likelihood},\\sigma_{likelihood}^2) \\times \\mathcal{N}(\\mu_{prior},\\sigma_{prior}^2)\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "In these equations, $\\mathcal{N}(\\mu,\\sigma^2)$ denotes a Gaussian distribution with parameters $\\mu$ and $\\sigma^2$:\n",
        "$$\n",
        "\\mathcal{N}(\\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\; \\exp \\bigg( \\frac{-(x-\\mu)^2}{2\\sigma^2} \\bigg)\n",
        "$$\n",
        "\n",
        "Let's consider the following questions using the following interactive demo:\n",
        "\n",
        "1. For a Gaussian posterior, explain how the information seems to be combining. (Hint: think about the prior exercises!)\n",
        "2. What is the difference between the posterior here and the Gaussian that represented the averate of two Gaussian's in the exercise above?\n",
        "3. How should we think about the relative weighting of information between the prior and posterio?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18XfQvk2a9tO",
        "colab": {
          "referenced_widgets": [
            "5eac338ffe044532b4884efd98d6ddbf"
          ]
        },
        "outputId": "618a3e8a-5e9a-4a47-b65b-a0c48224e543"
      },
      "source": [
        "widget = interact(plot_bayes,\n",
        "                  mu1 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ_prior\", continuous_update=False),\n",
        "                  mu2 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.5, description=\"µ_likelihood\", continuous_update=False),\n",
        "                  sigma1 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_prior\", continuous_update=False),\n",
        "                  sigma2 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_likelihood\", continuous_update=False))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "interactive(children=(FloatSlider(value=-0.5, continuous_update=False, description='µ_prior', max=4.0, min=-4.…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c597ed1ae26e4a099bc7258471304ed2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E76-AHQfTcTD"
      },
      "source": [
        "# to_remove explination"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDuI85NOa9tP"
      },
      "source": [
        "## Exploring priors\n",
        "\n",
        "What would happen if we had a different prior distribution for Astrocat's location? Bayes' Rule works exactly the same way if our prior is not a Guassian (though the analytical solution may be far more complex or impossible). Let's look at how the posterior behaves if we have a different prior over Astrocat's location.\n",
        "\n",
        "Consider the following questions:\n",
        "\n",
        "1. Why does the posterior not look Gaussian when you use a non-Gaussian prior?\n",
        "2. What does having a flat prior mean?\n",
        "3. How does the Gamma prior behave differently than the others?\n",
        "4. From what you know, can you imagine the likelihood being something other than a Gaussian?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlnOtfSva9tS",
        "colab": {
          "referenced_widgets": [
            "70e72958fe27422e8e6b70d1015e0a84"
          ]
        },
        "outputId": "db645cb9-02a5-4280-8efe-5317d3fe1098"
      },
      "source": [
        "widget = interact(plot_prior_switcher,\n",
        "                  what_to_plot = widgets.Dropdown(\n",
        "                      options=[\"Gaussian\", \"Mixture of Gaussians\", \"Uniform\", \"Gamma\"], \n",
        "                      value=\"Gaussian\", description=\"Prior: \"))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "interactive(children=(Dropdown(description='Prior: ', options=('Gaussian', 'Mixture of Gaussians', 'Uniform', …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ccd3fbf55c54f3fb35f0b96744c64d1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZHW3Y4WTcTE"
      },
      "source": [
        "# to_remove explination"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToCxtWwha9tT"
      },
      "source": [
        "# Section 6: Bayesian decisions\n",
        "\n",
        "## Bayesian estimation on the posterior\n",
        "\n",
        "Now that we understand that the posterior can be something other than a Gaussian, let's revisit **Loss** functions. In this case, we can see that the posterior can take many forms. If \n",
        "\n",
        " \n",
        "Questions:\n",
        "\n",
        "1. If were to have a bi-modal prior, how do the different loss functions potentially inform us differently about what we learn?\n",
        "2. Why do the different loss functions behavior differently with respect to the shape of the posterior? When do they produce different expected loss?\n",
        "3. For the mixture of Gaussians, describe the situtations where the expected loss will look different from the Guassian case.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG6104KTmOhI",
        "colab": {
          "referenced_widgets": [
            "3ec6fd7d66a4432cbc01aa039bffc28c"
          ]
        },
        "outputId": "48b32037-b84c-4073-d2b5-99919cb55f9c"
      },
      "source": [
        "widget = interact(plot_bayes_loss_utility_switcher,\n",
        "                  what_to_plot = widgets.Dropdown(\n",
        "                      options=[\"Gaussian\", \"Mixture of Gaussians\", \"Uniform\", \"Gamma\"], \n",
        "                      value=\"Gaussian\", description=\"Prior: \"))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "interactive(children=(Dropdown(description='Prior: ', options=('Gaussian', 'Mixture of Gaussians', 'Uniform', …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e28689eb465451c8e9b77a7ac4ea8aa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu2LWGfuTcTF"
      },
      "source": [
        "# to_remove explination"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpOhf8aia9tM"
      },
      "source": [
        "## Bayesian decisions\n",
        "\n",
        "Finally, we can combine everything we have learned so far! Now, let's imagine we have just received a new measurement of Astrocat's location. We need to think about where want to decide Astrocat is, so that we can decide how far to tell Astrocat to move. However, we want to account for the satellite and Space Mouse location in this estimation. If we make an error towards the satellite, it's worse than towards Space Mouse.\n",
        "\n",
        "Questions:\n",
        "\n",
        "1. If you have a weak prior and likelihood, how much are you relying on the utility function to guide your estimation?\n",
        "2. If you get a good measurement, that is a likelihood with low variance, how much does this help?\n",
        "3. Which of the factors are most important in making your decision?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNo6jiJqa9tU",
        "colab": {
          "referenced_widgets": [
            "dfa16e73602a4f66ace58f0e799b70bf"
          ]
        },
        "outputId": "5e33931e-8c88-472f-f7c9-6bdb39abbc0b"
      },
      "source": [
        "widget = interact(plot_utility_gaussian,\n",
        "                  mu1 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-0.5, description=\"µ_prior\", continuous_update=False),\n",
        "                  mu2 = FloatSlider(min=-4.0, max=4.0, step=0.01, value=0.5, description=\"µ_likelihood\", continuous_update=False),\n",
        "                  sigma1 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_prior\", continuous_update=False),\n",
        "                  sigma2 = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_likelihood\", continuous_update=False),\n",
        "                  mu_g = FloatSlider(min=-4.0, max=4.0, step=0.01, value=1.0, description=\"µ_gain\", continuous_update=False),\n",
        "                  mu_c = FloatSlider(min=-4.0, max=4.0, step=0.01, value=-1.0, description=\"µ_cost\", continuous_update=False),\n",
        "                  sigma_g = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_gain\", continuous_update=False),\n",
        "                  sigma_c = FloatSlider(min=0.1, max=2.0, step=0.01, value=0.5, description=\"σ_cost\", continuous_update=False),\n",
        "                  plot_utility_row=fixed(True))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "interactive(children=(FloatSlider(value=-0.5, continuous_update=False, description='µ_prior', max=4.0, min=-4.…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9635227217de4ec482b21ed54755bbad"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyRgMPw9TcTF"
      },
      "source": [
        "# to_remove explination"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a63c9FKUTcTF"
      },
      "source": [
        "---\n",
        "# Summary\n",
        "\n",
        "In this tutorial, you extended your exploration of Bayes Rule and the Bayesian approach in the context of finding and choosling a location for Astrocat.\n",
        "\n",
        "Specifically, we covered:\n",
        "\n",
        "* The Gaussian distribution and it's properties\n",
        "\n",
        "* That the likelihood is the probability of the measurement given some hidden state\n",
        "\n",
        "* Information shared between Gaussians (via multiplication of PDFs and via two-dimensional distributions)\n",
        "\n",
        "* That how the prior and likelihood interact to create the posterior, the probability of the hidden state given a measurement, depends on how they covary\n",
        "\n",
        "* That utility is the gain from each action and state pair, and the expected utility for an action is the sum of the utility for all state pairs, weighted by the probability of that state happening. You can then choose the action with highest expected utility.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTP-05ltTcTF"
      },
      "source": [
        "# [outro]"
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}