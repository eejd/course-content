{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/W2D1-postcourse-bugfix/tutorials/W2D1_BayesianStatistics/W2D1_Tutorial4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# NMA 2020 W2D1 -- (Bonus) Tutorial 4: Bayesian Decision Theory & Cost functions\n",
    "__Content creators:__ Vincent Valton, Konrad Kording, with help from Matthew Krause\n",
    "\n",
    "__Content reviewers:__ Matthew Krause, Jesse Livezey, Karolina Stosio, Saeed Salehi"
   ]
  },
  {
   "source": [
    "# Bonus Section: Multimodal Priors\n",
    "\n",
    "\n",
    "**Only do this if the first half-hour has not yet passed.**\n",
    "\n",
    "The preceeding exercises used a Gaussian prior, implying that participants expected the stimulus to come from a single location, though they might not know precisely where. However, suppose the subjects actually thought that sound might come from one of two distinct locations. Perhaps they can see two speakers (and know that speakers often emit noise). \n",
    "\n",
    "We could model this using a Gaussian prior with a large $\\sigma$ that covers both locations, but that would also make every point in between seem likely too.A better approach is to adjust the form of the prior so that it better matches the participants' experiences/expectations. In this optional exercise, we will build a bimodal (2-peaked) prior out of Gaussians and examine the resulting posterior and its peaks. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Exercise 3: Implement and test a multimodal prior\n",
    "\n",
    "* Complete the `bimodal_prior` function below to create a bimodal prior, comprised of the sum of two Gaussians with means $\\mu = -3$ and $\\mu = 3$. Use $\\sigma=1$ for both Gaussians. Be sure to normalize the result so it is a proper probability distribution. \n",
    "\n",
    "* In Exercise 2, we used the mean location to summarize the posterior distribution. This is not always the best choice, especially for multimodal distributions. What is the mean of our new prior? Is it a particularly likely location for the stimulus? Instead, we will use the posterior **mode** to summarize the distribution. The mode is the *location* of the most probable part of the distribution. Complete `posterior_mode` below, to find it. (Hint: `np.argmax` returns the *index* of the largest element in an array).\n",
    "\n",
    "* Run the provided simulation and plotting code. Observe what happens to the posterior as the likelihood gets closer to the different peaks of the prior.\n",
    "* Notice what happens to the posterior when the likelihood is exactly in between the two modes of the prior (i.e., $\\mu_{Likelihood} = 0$)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Exercise 2B: Finding the posterior analytically\n",
    "\n",
    "[If you are running short on time, feel free to skip the coding exercise below].\n",
    "\n",
    "As you may have noticed from the interactive demo, the product of two Gaussian distributions, like our prior and likelihood, remains a Gaussian, regardless of the parameters. We can directly compute the  parameters of that Gaussian from the means and variances of the prior and likelihood. For example, the posterior mean is given by:\n",
    "\n",
    "$$ \\mu_{posterior} = \\frac{\\mu_{auditory} \\cdot \\frac{1}{\\sigma_{auditory}^2} + \\mu_{visual} \\cdot \\frac{1}{\\sigma_{visual}^2}}{1/\\sigma_{auditory}^2 + 1/\\sigma_{visual}^2} \n",
    "$$\n",
    "\n",
    "This formula is a special case for two Gaussians, but is a very useful one because:\n",
    "*   The posterior has the same form (here, a normal distribution) as the prior, and\n",
    "*   There is simple, closed-form expression for its parameters.\n",
    "\n",
    "When these properties hold, we call them **conjugate distributions** or **conjugate priors** (for a particular likelihood). Working with conjugate distributions is very convenient; otherwise, it is often necessary to use computationally-intensive numerical methods to combine the prior and likelihood. \n",
    "\n",
    "In this exercise, we ask you to verify that property.  To do so, we will hold our auditory likelihood constant as an $\\mathcal{N}(3, 1.5)$ distribution, while considering visual priors with different means ranging from $\\mu=-10$ to $\\mu=10$. For each prior,\n",
    "\n",
    "* Compute the posterior distribution using the function you wrote in Exercise 2A. Next, find its mean. The mean of a probability distribution is $\\int_x p(x) dx$ or $\\sum_x x\\cdot p(x)$. \n",
    "* Compute the analytical posterior mean from auditory and visual using the equation above.\n",
    "* Use the provided plotting code to plot both estimates of the mean. \n",
    "\n",
    "Are the estimates of the posterior mean the same in both cases? \n",
    "\n",
    "Using these results, try to predict the posterior mean for the combination of a $\\mathcal{N}(-4,4)$ prior and and $\\mathcal{N}(4, 2)$ likelihood. Use the widget above to check your prediction. You can enter values directly by clicking on the numbers to the right of each slider; $\\sqrt{2} \\approx 1.41$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Tutorial Objectives\n",
    "\n",
    "*This tutorial is optional! Please do not feel pressured to finish it!*\n",
    "\n",
    "In the previous tutorials, we investigated the posterior, which describes  beliefs based on a combination of current evidence and prior experience. This tutorial focuses on Bayesian Decision Theory, which combines the posterior with **cost functions** that allow us to quantify the potential impact of making a decision or choosing an action based on that posterior. Cost functions are therefore critical for turning probabilities into actions!\n",
    "\n",
    "In Tutorial 3, we used the mean of the posterior $p(x | \\tilde x)$ as a proxy for the response $\\hat x$ for the participants. What prompted us to use the mean of the posterior as a **decision rule**? In this tutorial we will see how different common decision rules such as the choosing the mean, median or mode of the posterior distribution correspond to minimizing different cost functions.\n",
    "\n",
    "In this tutorial, you will\n",
    "  1. Implement three commonly-used cost functions: mean-squared error, absolute error, and zero-one loss\n",
    "  2. Discover the concept of expected loss, and\n",
    "  3. Choose optimal locations on the posterior that minimize these cost functions. You will verify that it these locations can be found analytically as well as empirically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "colab_type": "code",
    "outputId": "824205d4-e193-45ab-d748-bbd5c015f970"
   },
   "outputs": [],
   "source": [
    "#@title Video 1: Introduction\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id='z2DF4H_sa-k', width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "Please execute the cell below to initialize the notebook environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "--- \n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "#@title Figure Settings\n",
    "import ipywidgets as widgets\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# @title Helper Functions\n",
    "\n",
    "def my_gaussian(x_points, mu, sigma):\n",
    "  \"\"\"Returns un-normalized Gaussian estimated at points `x_points`\n",
    "\n",
    "  DO NOT EDIT THIS FUNCTION !!!\n",
    "\n",
    "  Args :\n",
    "    x_points (numpy array of floats) - points at which the gaussian is evaluated\n",
    "    mu (scalar) - mean of the Gaussian\n",
    "    sigma (scalar) - std of the gaussian\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats): un-normalized Gaussian (i.e. without constant) evaluated at `x`\n",
    "  \"\"\"\n",
    "  return np.exp(-(x_points-mu)**2/(2*sigma**2))\n",
    "\n",
    "def visualize_loss_functions(mse=None, abse=None, zero_one=None):\n",
    "  \"\"\"Visualize loss functions\n",
    "    Args:\n",
    "      - mse (func) that returns mean-squared error\n",
    "      - abse: (func) that returns absolute_error\n",
    "      - zero_one: (func) that returns zero-one loss\n",
    "    All functions should be of the form f(x, x_hats). See Exercise #1.\n",
    "\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "\n",
    "  x = np.arange(-3, 3.25, 0.25)\n",
    "\n",
    "  fig, ax = plt.subplots(1)\n",
    "\n",
    "  if mse is not None:\n",
    "    ax.plot(x, mse(0, x), linewidth=2, label=\"Mean Squared Error\")\n",
    "  if abse is not None:\n",
    "    ax.plot(x, abse(0, x), linewidth=2, label=\"Absolute Error\")\n",
    "  if zero_one_loss is not None:\n",
    "    ax.plot(x, zero_one_loss(0, x), linewidth=2, label=\"Zero-One Loss\")\n",
    "\n",
    "  ax.set_ylabel('Cost')\n",
    "  ax.set_xlabel('Predicted Value ($\\hat{x}$)')\n",
    "  ax.set_title(\"Loss when the true value $x$=0\")\n",
    "  ax.legend()\n",
    "  plt.show()\n",
    "\n",
    "def moments_myfunc(x_points, function):\n",
    "    \"\"\"Returns the mean, median and mode of an arbitrary function\n",
    "\n",
    "    DO NOT EDIT THIS FUNCTION !!!\n",
    "\n",
    "    Args :\n",
    "      x_points (numpy array of floats) - x-axis values\n",
    "      function (numpy array of floats) - y-axis values of the function evaluated at `x_points`\n",
    "\n",
    "    Returns:\n",
    "       (tuple of 3 scalars): mean, median, mode\n",
    "    \"\"\"\n",
    "\n",
    "    # Calc mode of an arbitrary function\n",
    "    mode = x_points[np.argmax(function)]\n",
    "\n",
    "    # Calc mean of an arbitrary function\n",
    "    mean = np.sum(x_points * function)\n",
    "\n",
    "    # Calc median of an arbitrary function\n",
    "    cdf_function = np.zeros_like(x_points)\n",
    "    accumulator = 0\n",
    "    for i in np.arange(x.shape[0]):\n",
    "        accumulator = accumulator + posterior[i]\n",
    "        cdf_function[i] = accumulator\n",
    "    idx = np.argmin(np.abs(cdf_function - 0.5))\n",
    "    median = x_points[idx]\n",
    "\n",
    "    return mean, median, mode\n",
    "\n",
    "def loss_plot(x, loss, min_loss, loss_label, show=False, ax=None):\n",
    "  if not ax:\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "  ax.plot(x, loss, '-C1', linewidth=2, label=loss_label)\n",
    "  ax.axvline(min_loss, ls='dashed', color='C1', label='Minimum')\n",
    "  ax.set_ylabel('Expected Loss')\n",
    "  ax.set_xlabel('Orientation (Degrees)')\n",
    "  ax.legend()\n",
    "\n",
    "  if show:\n",
    "    plt.show()\n",
    "\n",
    "def loss_plot_subfigures(x,\n",
    "              MSEloss, min_MSEloss, loss_MSElabel,\n",
    "              ABSEloss, min_ABSEloss, loss_ABSElabel,\n",
    "              ZeroOneloss, min_01loss, loss_01label):\n",
    "\n",
    "  fig_w, fig_h = plt.rcParams.get('figure.figsize')\n",
    "  fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(fig_w*2, fig_h*2), sharex=True)\n",
    "\n",
    "  ax[0, 0].plot(x, MSEloss, '-C1', linewidth=2, label=loss_MSElabel)\n",
    "  ax[0, 0].axvline(min_MSEloss, ls='dashed', color='C1', label='Minimum')\n",
    "  ax[0, 0].set_ylabel('Expected Loss')\n",
    "  ax[0, 0].set_xlabel('Orientation (Degrees)')\n",
    "  ax[0, 0].set_title(\"Mean Squared Error\")\n",
    "  ax[0, 0].legend()\n",
    "\n",
    "  pmoments_plot(x, posterior, ax=ax[1,0])\n",
    "\n",
    "  ax[0, 1].plot(x, ABSEloss, '-C0', linewidth=2, label=loss_ABSElabel)\n",
    "  ax[0, 1].axvline(min_ABSEloss, ls='dashdot', color='C0', label='Minimum')\n",
    "  ax[0, 1].set_ylabel('Expected Loss')\n",
    "  ax[0, 1].set_xlabel('Orientation (Degrees)')\n",
    "  ax[0, 1].set_title(\"Absolute Error\")\n",
    "  ax[0, 1].legend()\n",
    "\n",
    "  pmoments_plot(x, posterior, ax=ax[1,1])\n",
    "\n",
    "\n",
    "  ax[0, 2].plot(x, ZeroOneloss, '-C2', linewidth=2, label=loss_01label)\n",
    "  ax[0, 2].axvline(min_01loss, ls='dotted', color='C2', label='Minimum')\n",
    "  ax[0, 2].set_ylabel('Expected Loss')\n",
    "  ax[0, 2].set_xlabel('Orientation (Degrees)')\n",
    "  ax[0, 2].set_title(\"0-1 Loss\")\n",
    "  ax[0, 2].legend()\n",
    "\n",
    "  pmoments_plot(x, posterior, ax=ax[1,2])\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "def pmoments_plot(x, posterior,\n",
    "                  prior=None, likelihood=None, show=False, ax=None):\n",
    "\n",
    "  if not ax:\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "  if prior:\n",
    "    ax.plot(x, prior, '-C1', linewidth=2, label='Prior')\n",
    "  if likelihood:\n",
    "    ax.plot(x, likelihood, '-C0', linewidth=2, label='Likelihood')\n",
    "  ax.plot(x, posterior, '-C2', linewidth=4, label='Posterior')\n",
    "\n",
    "  mean, median, mode = moments_myfunc(x, posterior)\n",
    "\n",
    "  ax.axvline(mean, ls='dashed', color='C1', label='Mean')\n",
    "  ax.axvline(median, ls='dashdot', color='C0', label='Median')\n",
    "  ax.axvline(mode, ls='dotted', color='C2', label='Mode')\n",
    "  ax.set_ylabel('Probability')\n",
    "  ax.set_xlabel('Orientation (Degrees)')\n",
    "  ax.legend()\n",
    "\n",
    "  if show:\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def generate_example_pdfs():\n",
    "  \"\"\"Generate example probability distributions as in T2\"\"\"\n",
    "  x=np.arange(-5, 5, 0.01)\n",
    "\n",
    "  prior_mean = 0\n",
    "  prior_sigma1 = .5\n",
    "  prior_sigma2 = 3\n",
    "  prior1 = my_gaussian(x, prior_mean, prior_sigma1)\n",
    "  prior2 = my_gaussian(x, prior_mean, prior_sigma2)\n",
    "\n",
    "  alpha = 0.05\n",
    "  prior_combined = (1-alpha) * prior1 + (alpha * prior2)\n",
    "  prior_combined = prior_combined / np.sum(prior_combined)\n",
    "\n",
    "  likelihood_mean = -2.7\n",
    "  likelihood_sigma = 1\n",
    "  likelihood = my_gaussian(x, likelihood_mean, likelihood_sigma)\n",
    "  likelihood = likelihood / np.sum(likelihood)\n",
    "\n",
    "  posterior = prior_combined * likelihood\n",
    "  posterior = posterior / np.sum(posterior)\n",
    "\n",
    "  return x, prior_combined, likelihood, posterior\n",
    "\n",
    "def plot_posterior_components(x, prior, likelihood, posterior):\n",
    "  with plt.xkcd():\n",
    "    fig = plt.figure()\n",
    "    plt.plot(x, prior, '-C1', linewidth=2, label='Prior')\n",
    "    plt.plot(x, likelihood, '-C0', linewidth=2, label='Likelihood')\n",
    "    plt.plot(x, posterior, '-C2', linewidth=4, label='Posterior')\n",
    "    plt.legend()\n",
    "    plt.title('Sample Output')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The Posterior Distribution\n",
    "\n",
    "This notebook will use a model similar to the puppet & puppeteer sound experiment developed in Tutorial 2, but with different probabilities for $p_{common}$, $p_{independent}$, $\\sigma_{common}$ and $\\sigma_{independent}$. Specifically, our model will consist of these components, combined according to Bayes' rule:\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\textrm{Prior} &=& \\begin{cases} \\mathcal{N_{common}}(0, 0.5) & 95\\% \\textrm{ weight}\\\\\n",
    "                                 \\mathcal{N_{independent}}(0, 3.0) &  5\\% \\textrm{ weight} \\\\\n",
    "                    \\end{cases}\\\\\\\\\n",
    "\\textrm{Likelihood} &= &\\mathcal{N}(-2.7, 1.0)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "We will use this posterior as an an example through this notebook. Please run the cell below to import and plot the model. You do not need to edit anything. These parameter values were deliberately chosen for illustration purposes: there is nothing intrinsically special about them, but they make several of the exercises easier. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "outputId": "ef01110e-546a-487c-e1db-b13cd08292f3"
   },
   "outputs": [],
   "source": [
    "x, prior, likelihood, posterior = generate_example_pdfs()\n",
    "plot_posterior_components(x, prior, likelihood, posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Exercise 2: Finding the expected loss empirically via integration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Section 3: Analytical Solutions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "colab_type": "code",
    "outputId": "a2058e1a-71c5-45a7-833d-6074b9512727"
   },
   "outputs": [],
   "source": [
    "#@title Video 3: Analytical Solutions\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id='wmDD51N9rs0', width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In the previous exercise, we found the minimum expected loss via brute-force: we searched over all possible values of $x$ and found the one that minimized each of our loss functions. This is feasible for our small toy example, but can quickly become intractable. \n",
    "\n",
    "Fortunately, the three loss functions examined in this tutorial have are minimized at specific points on the posterior, corresponding to the itss mean, median, and mode. To verify this property, we have replotted the loss functions from Exercise 2 below, with the posterior on the same scale beneath. The mean, median, and mode are marked on the posterior. \n",
    "\n",
    "Which loss form corresponds to each summary statistics? \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "colab_type": "code",
    "outputId": "5c045166-97d9-4a3b-cc3a-963bb8d560d6"
   },
   "outputs": [],
   "source": [
    "loss_plot_subfigures(x,\n",
    "                    ExpectedLoss_MSE, min_MSE, f\"Mean Squared Error = {min_MSE:.2f}\",\n",
    "                    ExpectedLoss_ABSE, min_ABSE, f\"Absolute Error = {min_ABSE:.2f}\",\n",
    "                    ExpectedLoss_01, min_01, f\"Zero-One Error = {min_01:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "outputId": "704d3a6a-ea3a-4b1a-f83c-579585e9a4bf"
   },
   "outputs": [],
   "source": [
    "#to_remove explanation\n",
    "\"\"\"\n",
    "As you might recall from W1D3, the mean minimizes the mean-squared error.\n",
    "Absolute error is minimized by the median, while zero-one loss is minimized\n",
    "at the posterior's mode.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Section 4: Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "colab_type": "code",
    "outputId": "e5748d72-880b-4891-bc52-476eb10a6c42"
   },
   "outputs": [],
   "source": [
    "#@title Video 4: Outro\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id='3nTvamDVx2s', width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In this tutorial, we learned about three kinds of cost functions: mean-squared error, absolute error, and zero-one loss. We used expected loss to quantify the results of making a decision, and showed that optimizing under different cost functions led us to choose different locations on the posterior. Finally, we found that these optimal locations can be identified analytically, sparing us from a brute-force search. \n",
    "\n",
    "Here are some additional questions to ponder:\n",
    "*   Suppose your professor offered to grade your work with a zero-one loss or mean square error. \n",
    "    * When might you choose each?\n",
    "    * Which would be easier to learn from?\n",
    "* All of the loss functions we considered are symmetrical. Are there situations where an asymmetrical loss function might make sense? How about a negative one?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D1_Tutorial4",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}