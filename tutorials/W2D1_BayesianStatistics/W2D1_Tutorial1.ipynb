{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/W2D1-postcourse-bugfix/tutorials/W2D1_BayesianStatistics/W2D1_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Neuromatch Academy: Week 3, Day 1, Tutorial 1\n",
    "# Bayes with a binary hidden state\n",
    "\n",
    "__Content creators:__ [insert your name here]\n",
    "\n",
    "__Content reviewers:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Tutorial Objectives\n",
    "This is the first in a series of two core tutorials on Bayesian statistics. In these tutorials, we will explore the fundemental concepts of the Bayesian approach from two perspectives. This tutorial will work through an example of Bayesian inference and decision making using a binary hidden state. The second main tutorial extends these concepts to a continuous hidden state. In the next days, each of these basic ideas will be extended--first through time as we consider what happens when we infere a hidden state using multiple observations and when the hidden state changes across time. In the third day, we will introduce the notion of how to use inference and decisions to select actions for optimal control. For this tutorial, you will be introduced to our binary state fishing problem!\n",
    "\n",
    "This notebook will introduce the fundamental building blocks for Bayesian statistics: \n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "colab_type": "code",
    "outputId": "bccac551-e778-4f58-ea61-cdd93ffd4cde"
   },
   "outputs": [],
   "source": [
    "#@title Video 1: Introduction to Bayesian Statistics\n",
    "from IPython.display import YouTubeVideo\n",
    "# video = YouTubeVideo(id='K4sSKZtk-Sc', width=854, height=480, fs=1)\n",
    "# print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "# video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup  \n",
    "Please execute the cells below to initialize the notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from matplotlib import transforms\n",
    "from matplotlib import gridspec\n",
    "from scipy.optimize import fsolve\n",
    "\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "#@title Figure Settings\n",
    "import ipywidgets as widgets       # interactive display\n",
    "from ipywidgets import GridspecLayout\n",
    "from IPython.display import clear_output\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def compute_marginal(px, py, cor):\n",
    "    # calculate 2x2 joint probabilities given marginals p(x=1), p(y=1) and correlation\n",
    "    p11 = px*py + cor*np.sqrt(px*py*(1-px)*(1-py))\n",
    "    p01 = px - p11\n",
    "    p10 = py - p11\n",
    "    p00 = 1.0 - p11 - p01 - p10\n",
    "    return np.asarray([[p00, p01], [p10, p11]])\n",
    "# test\n",
    "# print(compute_marginal(0.4, 0.6, -0.8))\n",
    "\n",
    "def compute_cor_range(px,py):\n",
    "    # Calculate the allowed range of correlation values given marginals p(x=1) and p(y=1)\n",
    "    def p11(corr):\n",
    "        return px*py + corr*np.sqrt(px*py*(1-px)*(1-py))\n",
    "    def p01(corr):\n",
    "        return px - p11(corr)\n",
    "    def p10(corr):\n",
    "        return py - p11(corr)\n",
    "    def p00(corr):\n",
    "        return 1.0 - p11(corr) - p01(corr) - p10(corr)\n",
    "    Cmax = min(fsolve(p01, 0.0), fsolve(p10, 0.0))\n",
    "    Cmin = max(fsolve(p11, 0.0), fsolve(p00, 0.0))\n",
    "    return Cmin, Cmax\n",
    "\n",
    "def plot_joint_probs(P, ):\n",
    "    assert np.all(P >= 0), \"probabilities should be >= 0\"\n",
    "    # normalize if not\n",
    "    P = P / np.sum(P)\n",
    "    marginal_y = np.sum(P,axis=1)\n",
    "    marginal_x = np.sum(P,axis=0)\n",
    "\n",
    "    # definitions for the axes\n",
    "    left, width = 0.1, 0.65\n",
    "    bottom, height = 0.1, 0.65\n",
    "    spacing = 0.005\n",
    "\n",
    "    # start with a square Figure\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "    joint_prob = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom + height + spacing, width, 0.2]\n",
    "    rect_histy = [left + width + spacing, bottom, 0.2, height]\n",
    "\n",
    "    rect_x_cmap = plt.cm.Blues\n",
    "    rect_y_cmap = plt.cm.Reds\n",
    "\n",
    "    # Show joint probs and marginals\n",
    "    ax = fig.add_axes(joint_prob)\n",
    "    ax_x = fig.add_axes(rect_histx, sharex=ax)\n",
    "    ax_y = fig.add_axes(rect_histy, sharey=ax)\n",
    "\n",
    "    # Show joint probs and marginals\n",
    "    ax.matshow(P,vmin=0., vmax=1., cmap='Greys')\n",
    "    ax_x.bar(0, marginal_x[0], facecolor=rect_x_cmap(marginal_x[0]))\n",
    "    ax_x.bar(1, marginal_x[1], facecolor=rect_x_cmap(marginal_x[1]))\n",
    "    ax_y.barh(0, marginal_y[0], facecolor=rect_y_cmap(marginal_y[0]))\n",
    "    ax_y.barh(1, marginal_y[1], facecolor=rect_y_cmap(marginal_y[1]))\n",
    "    # set limits\n",
    "    ax_x.set_ylim([0,1])\n",
    "    ax_y.set_xlim([0,1])\n",
    "\n",
    "    # show values \n",
    "    ind = np.arange(2)\n",
    "    x,y = np.meshgrid(ind,ind)\n",
    "    for i,j in zip(x.flatten(), y.flatten()):\n",
    "        c = f\"{P[i,j]:.2f}\"\n",
    "        ax.text(j,i, c, va='center', ha='center', color='black')\n",
    "    for i in ind:\n",
    "        v = marginal_x[i]\n",
    "        c = f\"{v:.2f}\"\n",
    "        ax_x.text(i, v +0.1, c, va='center', ha='center', color='black')\n",
    "        v = marginal_y[i]\n",
    "        c = f\"{v:.2f}\"\n",
    "        ax_y.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
    "\n",
    "    # set up labels\n",
    "    ax.xaxis.tick_bottom()\n",
    "    ax.yaxis.tick_left()\n",
    "    ax.set_xticks([0,1])\n",
    "    ax.set_yticks([0,1])\n",
    "    ax.set_xticklabels(['R','B'])\n",
    "    ax.set_yticklabels(['0','1'])\n",
    "    ax.set_xlabel('color')\n",
    "    ax.set_ylabel('size')\n",
    "    ax_x.axis('off')   \n",
    "    ax_y.axis('off')   \n",
    "    return fig\n",
    "# test\n",
    "# P = np.random.rand(2,2)\n",
    "# P = np.asarray([[0.9, 0.8], [0.4, 0.1]])\n",
    "# P = P / np.sum(P)\n",
    "# fig = plot_joint_probs(P)\n",
    "# plt.show(fig)\n",
    "# plt.close(fig)\n",
    "\n",
    "def plot_prior_likelihood(ps, cor):\n",
    "    likelihood = np.asarray([[cor, 1-cor],[1-cor,cor]])\n",
    "    assert 0.0 <= ps <= 1.0\n",
    "    assert 0.0 <= cor <= 1.0\n",
    "    prior = np.asarray([1-ps, ps])\n",
    "\n",
    "    posterior = likelihood * prior.reshape(1,2)\n",
    "\n",
    "    fig, ax_all = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    ax_prior, ax_likelihood, ax_posterior = ax_all\n",
    "    ax_prior.set_title(\"prior distribution\", pad=15)\n",
    "    ax_likelihood.set_title(\"likelihood\")\n",
    "    ax_posterior.set_title(\"posterior distribution\")\n",
    "\n",
    "    rect_colormap = plt.cm.Blues\n",
    "\n",
    "    # Show posterior probs and marginals\n",
    "    ax_prior.bar(0, prior[0], facecolor = rect_colormap(prior[0]))\n",
    "    ax_prior.bar(1, prior[1], facecolor = rect_colormap(prior[1]))\n",
    "    ax_likelihood.matshow(likelihood, vmin=0., vmax=1., cmap='Reds')\n",
    "    ax_posterior.matshow(posterior, vmin=0., vmax=1., cmap='Greys')\n",
    "\n",
    "    for ax in ax_all:\n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_yticks([0, 1])\n",
    "        ax.set_xticklabels([0, 1])\n",
    "        ax.set_yticklabels([0, 1])\n",
    "    ax_posterior.xaxis.set_ticks_position('bottom')\n",
    "    ax_likelihood.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "    # show values \n",
    "    ind = np.arange(2)\n",
    "    x,y = np.meshgrid(ind,ind)\n",
    "    for i,j in zip(x.flatten(), y.flatten()):\n",
    "        c = f\"{posterior[i,j]:.2f}\"\n",
    "        ax_posterior.text(j,i, c, va='center', ha='center', color='black')\n",
    "    for i,j in zip(x.flatten(), y.flatten()):\n",
    "        c = f\"{likelihood[i,j]:.2f}\"\n",
    "        ax_likelihood.text(j,i, c, va='center', ha='center', color='black')\n",
    "    for i in ind:\n",
    "        v = prior[i]\n",
    "        c = f\"{v:.2f}\"\n",
    "        ax_prior.text(i, v +0.05, c, va='center', ha='center', color='black')\n",
    "\n",
    "    # set up labels\n",
    "    ax_prior.set_xlabel(\"s\")\n",
    "    ax_likelihood.set_xlabel(\"s\")\n",
    "    ax_likelihood.set_ylabel(\"m\")\n",
    "    ax_posterior.set_xlabel(\"s\")\n",
    "    ax_posterior.set_ylabel(\"m\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "# fig = plot_prior_likelihood(0.5, 0.3)\n",
    "# plt.show(fig)\n",
    "# plt.close(fig)\n",
    "\n",
    "def plot_prior_likelihood_utility(ps, p_a_s1, p_a_s0, loss_s, gain_s):\n",
    "    likelihood = np.asarray([[p_a_s1, 1-p_a_s1],[1-p_a_s0,p_a_s0]]).T\n",
    "    assert 0.0 <= ps <= 1.0\n",
    "    assert 0.0 <= p_a_s1 <= 1.0\n",
    "    assert 0.0 <= p_a_s0 <= 1.0\n",
    "    prior = np.asarray([1-ps, ps])\n",
    "\n",
    "    utility = np.array([[-loss_s, loss_s-1], [gain_s, 1-gain_s]]).T\n",
    "    posterior = likelihood * prior.reshape(1,2)\n",
    "    posterior, likelihood = posterior.T, likelihood.T\n",
    "    expected = np.multiply(utility, posterior)\n",
    "\n",
    "    # definitions for the axes\n",
    "    left, width = 0.05, 0.16\n",
    "    bottom, height = 0.05, 0.9\n",
    "    padding = 0.04\n",
    "    small_width = 0.1\n",
    "    left_space = left + small_width + padding\n",
    "    added_space = padding + width\n",
    "\n",
    "    fig = plt.figure(figsize=(17, 3))\n",
    "\n",
    "    rect_prior = [left, bottom, small_width, height]\n",
    "    rect_likelihood = [left_space, bottom , width, height]\n",
    "    rect_posterior = [left_space + added_space, bottom , width, height]\n",
    "    rect_utility = [left_space + 2*added_space, bottom , width, height]\n",
    "    rect_expected = [left_space + 3*added_space, bottom , width, height]\n",
    "\n",
    "    ax_likelihood = fig.add_axes(rect_likelihood)\n",
    "    ax_prior = fig.add_axes(rect_prior, sharey=ax_likelihood)\n",
    "    ax_posterior = fig.add_axes(rect_posterior, sharey=ax_likelihood)\n",
    "    ax_utility = fig.add_axes(rect_utility, sharey=ax_posterior)\n",
    "    ax_expected = fig.add_axes(rect_expected, sharey=ax_utility)\n",
    "\n",
    "    ax_prior.set_title(\"prior distribution\", pad=15)\n",
    "    ax_likelihood.set_title(\"likelihood\")\n",
    "    ax_posterior.set_title(\"posterior distribution\")\n",
    "    ax_utility.set_title(\"utility function\")\n",
    "    ax_expected.set_title(\"expected utility\")\n",
    "\n",
    "    rect_colormap = plt.cm.Blues\n",
    "\n",
    "    # Show posterior probs and marginals\n",
    "    ax_prior.barh(0, prior[0], facecolor = rect_colormap(prior[0]))\n",
    "    ax_prior.barh(1, prior[1], facecolor = rect_colormap(prior[1]))\n",
    "    ax_likelihood.matshow(likelihood, vmin=0., vmax=1., cmap='Reds')\n",
    "    ax_posterior.matshow(posterior, vmin=0., vmax=1., cmap='Greys')\n",
    "    ax_utility.matshow(utility, vmin=0., vmax=1., cmap='cool')\n",
    "    ax_expected.matshow(expected, vmin=0., vmax=1., cmap='Wistia')\n",
    "\n",
    "    for ax in [ax_prior, ax_likelihood, ax_posterior, ax_expected]:\n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_yticks([0, 1])\n",
    "        ax.set_xticklabels([0, 1])\n",
    "        ax.set_yticklabels([0, 1])\n",
    "\n",
    "    ax_utility.set_xticks([0, 1])\n",
    "    ax_utility.set_yticks([0, 1])\n",
    "    ax_utility.set_xticklabels([\"loss\", \"gain\"])\n",
    "    # ax_utility.set_yticklabels([0, 1])\n",
    "\n",
    "    ax_posterior.xaxis.set_ticks_position('bottom')\n",
    "    ax_likelihood.xaxis.set_ticks_position('bottom')\n",
    "    ax_utility.xaxis.set_ticks_position('bottom')\n",
    "    ax_expected.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "    ax_prior.set_xlim([1, 0])\n",
    "\n",
    "    # show values \n",
    "    ind = np.arange(2)\n",
    "    x,y = np.meshgrid(ind,ind)\n",
    "    for i,j in zip(x.flatten(), y.flatten()):\n",
    "        c = f\"{posterior[i,j]:.2f}\"\n",
    "        ax_posterior.text(j,i, c, va='center', ha='center', color='black')\n",
    "    for i,j in zip(x.flatten(), y.flatten()):\n",
    "        c = f\"{likelihood[i,j]:.2f}\"\n",
    "        ax_likelihood.text(j,i, c, va='center', ha='center', color='black')\n",
    "    for i,j in zip(x.flatten(), y.flatten()):\n",
    "        c = f\"{utility[i,j]:.2f}\"\n",
    "        ax_utility.text(j,i, c, va='center', ha='center', color='black')\n",
    "    for i,j in zip(x.flatten(), y.flatten()):\n",
    "        c = f\"{expected[i,j]:.2f}\"\n",
    "        ax_expected.text(j,i, c, va='center', ha='center', color='black')\n",
    "    for i in ind:\n",
    "        v = prior[i]\n",
    "        c = f\"{v:.2f}\"\n",
    "        ax_prior.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
    "\n",
    "\n",
    "    # # show values \n",
    "    # ind = np.arange(2)\n",
    "    # x,y = np.meshgrid(ind,ind)\n",
    "    # for i,j in zip(x.flatten(), y.flatten()):\n",
    "    #     c = f\"{P[i,j]:.2f}\"\n",
    "    #     ax.text(j,i, c, va='center', ha='center', color='white')\n",
    "    # for i in ind:\n",
    "    #     v = marginal_x[i]\n",
    "    #     c = f\"{v:.2f}\"\n",
    "    #     ax_x.text(i, v +0.2, c, va='center', ha='center', color='black')\n",
    "    #     v = marginal_y[i]\n",
    "    #     c = f\"{v:.2f}\"\n",
    "    #     ax_y.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
    "\n",
    "    # set up labels\n",
    "    ax_prior.set_xlabel(\"m\")\n",
    "    ax_likelihood.set_xlabel(\"m\")\n",
    "    ax_likelihood.set_ylabel(\"s\")\n",
    "    ax_posterior.set_xlabel(\"m\")\n",
    "    ax_posterior.set_ylabel(\"s\")\n",
    "    ax_utility.set_ylabel(\"s\")\n",
    "    # ax_expected.set_xlabel(\"m\")\n",
    "    ax_expected.set_ylabel(\"s\")\n",
    "    ax_utility.set_xlabel(\"a\")\n",
    "    ax_prior.axis('off')\n",
    "    return fig\n",
    "\n",
    "\n",
    "# fig = plot_prior_likelihood(0.5, 0.3)\n",
    "# plt.show(fig)\n",
    "# plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Section 1: The Binary hidden state\n",
    "\n",
    "You were just introduced to the binary hidden state problem we are going to explore. You are watching a person fishing on a dock and you want to determine where the fish are so you can decide where to fish. Remember, you can either think of your self as a scientist conducting an experiment or as a brain trying to make a decision. The Bayesian approach is the same!\n",
    "\n",
    "In this section, we are going to walk though what it means to take a measurement (also often called an observation) and how to think about what it tells you about the probability of the hidden state we are interested in. Then we are going to think about what happens if you act on a guess about the hidden state.\n"
   ]
  },
  {
   "source": [
    "## Video : Observations and costs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Exercise 1: What is a likelihood?\n",
    "We know fish like to school together. On different days the fish are mostly on the left or right, but you don’t know what the case is today. Let’s assume that on a given day all the fish are on one side only. So, we have no prior knowledge, but we can still know something about what catching a fish means for the likelihood of the fish being on one side or the other. You know that if you fish on the side of the dock where the fish are, you have a 50% chance that you catch a fish. Otherwise you catch a fish with only 10% probability. Calculate the following probabilities by hand.\n",
    "\n",
    "1. We showed the P(m|s) if you fish on the right side of the dock. What are the probabilities on the left side?\n",
    "2. What does a single measurement tell you?\n",
    "3. What is the difference in the likelihood if you know the $P(s = left) = 0.3$?\n",
    "\n",
    "To explore what happens as you change the (prior) probability of where the fish are today, $P(s)$, and you change the likelihood function, $P(m|s)$, use the widget below.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_widget = widgets.FloatSlider(0.3, description='ρ', min=0.0, max=1.0, step=0.01, disabled=False)\n",
    "\n",
    "@widgets.interact(\n",
    "    cor=cor_widget,\n",
    ")\n",
    "def make_prior_likelihood_plot(cor):\n",
    "    fig = plot_prior_likelihood(0.5,cor)\n",
    "    plt.show(fig)\n",
    "    plt.close(fig)\n",
    "    return None\n"
   ]
  },
  {
   "source": [
    "## Exercise 2: Utility and Loss (gain) functions\n",
    "\n",
    "Fish are much easier to catch on the left side of the dock, as there are no submarines, but you also know you are going to get sunburnt if you fish on the side were there are no fish. Let’s say you don’t know anything about where the fish are (no fishing yet today)\n",
    "\n",
    "| Utility: U(s,a)   | a = left   | a = right  |\n",
    "| ----------------- |----------|----------|\n",
    "| s = Left          | 2          | -3         |\n",
    "| s = right         | -2         |  1         |\n",
    "\n",
    "1. What should cause you to choose left or right?\n",
    "2. What changes after you have an observation?\n",
    "3. Calculate the utility of fishing on the right and left side of the dock if you have no measurement and no prior information (50/50 probabilty the fish are on either side).\n",
    "3. You observe someone fish on the right side and catch a fish, using the utilities we described in the video, what is the utility you should expect?\n",
    "\n",
    "Let's ask the question in a different way, what if we just wanted to know how close or far off our inference is about the location of the fish today. To do this, we must decide how much cost we incur if we are incorrect in our inference. In this case, we call the utility function a Loss function, similiar to the Loss functions you have encountered already. The expected loss is: $\\sum_{s}u(s,a)p(s)$, which allows us to ask how badly we expect our inference to be given a probabilty over the hidden state $s$.\n",
    "\n",
    "1. Assume your Loss function is the the squared error $(a-s)^2$, what is expected loss if $P(s=left)$ is .5? if it is .3?\n",
    "\n",
    "To explore what happens if you change the likelifhood and utility functions, use the widget below:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_widget = widgets.FloatSlider(0.5, description='p(s)', min=0.0, max=1.0, step=0.01)\n",
    "p_a_s1_widget = widgets.FloatSlider(0.5, description='p(fish | a = s)', min=0.0, max=1.0, step=0.01)\n",
    "p_a_s0_widget = widgets.FloatSlider(0.5, description='p(fish | a != s)', min=0.0, max=1.0, step=0.01)\n",
    "loss_s_widget = widgets.FloatSlider(0.5, description='loss (a = s)', min=0.0, max=1.0, step=0.01)\n",
    "gain_s_widget = widgets.FloatSlider(0.5, description='gain (a = s)', min=0.0, max=1.0, step=0.01)\n",
    "\n",
    "@widgets.interact(\n",
    "    ps=ps_widget,\n",
    "    p_a_s1=p_a_s1_widget,\n",
    "    p_a_s0=p_a_s0_widget,\n",
    "    loss_s=loss_s_widget,\n",
    "    gain_s=gain_s_widget,\n",
    ")\n",
    "def make_prior_likelihood_utility_plot(ps, p_a_s1, p_a_s0, loss_s, gain_s):\n",
    "    fig = plot_prior_likelihood_utility(ps, p_a_s1, p_a_s0, loss_s, gain_s)\n",
    "    plt.show(fig)\n",
    "    plt.close(fig)\n",
    "    return None\n"
   ]
  },
  {
   "source": [
    "# Section 2: Correlation and marginalization\n",
    "\n",
    "In this section we are going to think about the amount of information shared between two variables. We want to know how much information you gain when you observe one variable (take a measurement) if you know something about another. The fundemental concept is the same if we think about two attributes, for example the size and color of the fish, or the prior information and the likelihood."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Video : Correlation and marginalization"
   ]
  },
  {
   "source": [
    "## Exercise\n",
    "\n",
    "To understand the information between two variables, let's first consider the size and color of the fish.\n",
    "\n",
    "| p (x & y)   | y = sliver   | y = gold  |\n",
    "| ----------------- |----------|----------|\n",
    "| x = small          | 0.4          | 0.2         |\n",
    "| x = large         | 0.1         |  0.3         |\n",
    "\n",
    "We want to know what the probability of catching a small fish or a silver fish. To do this, we need to marginalize--or average out--the variable we are not intersted in across the rows or columns. For example, the $P(x = small) = \\sum_y{P(x = small \\& y)}$.\n",
    "\n",
    "1. Calculate the probability of catching a small fish, a large fish, a silver fish or a gold fish.\n",
    "2. Calculate the probability of catching a small fish OR a gold fish. (Hint: $P(A\\ \\textrm{or}\\ B) = P(A) + P(B) - P(A\\ \\textrm{and}\\ B)$)\n",
    "3. Calculate the probability of catching a small gold fish or a large silver fish.\n",
    "\n",
    "The relationship between the marginal probabilities and the joint probabilities is determined by the correlation between the two variables. To understand the way we calculate the correlation, we need to review the definition of covariance and correlation.\n",
    "\n",
    "Covariance:\n",
    "$cov(X,Y) = \\sigma_{XY} = E[(X - \\mu_{x})(Y - \\mu_{y})] = E[X]E[Y] - \\mu_{x}\\mu_{y}$\n",
    "\n",
    "Correlation:\n",
    "$\\rho_{XY} = \\frac{cov(Y,Y)}{\\sqrt{V(X)V(Y)}} = \\frac{\\sigma_{XY}}{\\sigma_{X}\\sigma_{Y}}$\n",
    "\n",
    "Use the widget below and answer the following questions:\n",
    "\n",
    "1. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridspecLayout(2,2)\n",
    "\n",
    "cor_widget = widgets.FloatSlider(0.3, description='ρ', min=-1, max=1, step=0.01)\n",
    "px_widget = widgets.FloatSlider(0.5, description='p(x)', min=0.01, max=0.99, step=0.01)\n",
    "py_widget = widgets.FloatSlider(0.5, description='p(y)', min=0.01, max=0.99, step=0.01)\n",
    "gs[0,0] = cor_widget\n",
    "gs[0,1] = px_widget\n",
    "gs[1,0] = py_widget\n",
    "\n",
    "\n",
    "@widgets.interact(\n",
    "    px=px_widget,\n",
    "    py=py_widget,\n",
    "    cor=cor_widget,\n",
    ")\n",
    "def make_corr_plot(px, py, cor):\n",
    "    Cmin, Cmax = compute_cor_range(px, py) #allow correlation values\n",
    "    cor_widget.min, cor_widget.max = Cmin+0.01, Cmax-0.01\n",
    "    if cor_widget.value > Cmax:\n",
    "        cor_widget.value = Cmax\n",
    "    if cor_widget.value < Cmin:\n",
    "        cor_widget.value = Cmin\n",
    "    cor = cor_widget.value\n",
    "    P = compute_marginal(px,py,cor)\n",
    "    # print(P)\n",
    "    fig = plot_joint_probs(P) \n",
    "    plt.show(fig)\n",
    "    plt.close(fig)\n",
    "    return None\n",
    "\n",
    "# gs[1,1] = make_corr_plot()"
   ]
  },
  {
   "source": [
    "## Exercise\n",
    "\n",
    "Let's return to fishing on the dock! We know the likelihoods if we take a measurement on the left or right side of the dock. We need to determine the likelihood given an assumption about the (prior) probility the fish are on the left or right using $\\mathcal{L}(m|s) = P(m|s)$. Assume $P(s = left) = 0.5$.\n",
    "\n",
    "1. Calcualte the likelihood of the fish being on the left side (the hidden state) if you see the person fishing on the right, but not catching a fish.\n",
    "2. Calcualte the likelihood of the fish being on the left side if you see the person fishing on the left and catch a fish.\n",
    "\n",
    "Now assume that $P(s = left) = 0.3$.\n",
    "\n",
    "3. Calcualte the likelihood of the fish being on the right if you see the person catch a fish on the left.\n",
    "4. Calcualte the likelihood of the fish being on the right if you see the person catch a fish on the right.\n",
    "\n",
    "You can "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Section 3. Bayes' Theorem and the Posterior"
   ]
  },
  {
   "source": [
    "## Video : Bayes' Theorem"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "colab_type": "code",
    "outputId": "f27d7154-1f82-4dd7-d21d-1d1ae4c9ae8c"
   },
   "outputs": [],
   "source": [
    "#@title Video 2: Bayes' theorem\n",
    "from IPython.display import YouTubeVideox\n",
    "# video = YouTubeVideo(id='ewQPHQMcdBs', width=854, height=480, fs=1)\n",
    "# print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now we can calcualte the full posterior distribution. The difference between the evidence or marginal likelihood,\n",
    "\n",
    "$\\mathcal{L}(s) = E_{s}[\\mathcal{L}(m|s)] = \\sum_s{P(m|s)P(s)}$\n",
    "\n",
    "and the posterior probability of the hidden state, $s$, given a measurement, $m$, is the partiction function (or normalizing constant) that ensures we produce a full probability distribution. This means that we can use this posterior as a complete probability distribution for future compututations! We often call the posterior probability distribution our *belief*, $b$, about the hidden state.\n",
    "\n",
    "$b = P(s|m) = \\frac{P(m|s)P(s)}{P(m)}$\n",
    "\n",
    "However, there is a reason that the likelihood function is an important concept: for many complicated cases, like those we might be using to model behavioral or brain inferences, the partition function can be intractable or extremely complex to calculate. This is why we often need to be careful to choose probability distributions were we can analytically calculate the posterior probability or numerical approximation is reliable. But the important thing to remember is that you can compare likelihoods, as we have seen during model fitting and model comparision, because the relative likelihoods are independent of the partition function.\n",
    "\n",
    "Let's calculate the posterior probability distribution (our belief about the hidden state). Assume $P(s = left) = 0.3$\n",
    "\n",
    "1. Calculate the posterior probability distribution if you see the person catch a fish on the right.\n",
    "2. Calculate the posterior probability distribution if you see the person try to fish on the right but not catch a fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Interactive Demo: What affects the posterior?\n",
    "\n",
    "Now that we can play with the effects of *Bayes rule*, let's vary the parameters of the prior to see how changing the prior and likelihood affect the posterior. \n",
    "\n",
    "**Hit the Play button or Ctrl+Enter in the cell below** and play with the sliders to get an intuition for how the means and standard deviations of prior and likelihood influence the posterior.\n",
    "\n",
    "When does the prior have the strongest influence over the posterior? When is it the weakest?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_widget = widgets.FloatSlider(0.3, description='ρ', min=0.0, max=1.0, step=0.01, disabled=False)\n",
    "ps_widget = widgets.FloatSlider(0.5, description='p(s)', min=0.0, max=1.0, step=0.01)\n",
    "\n",
    "@widgets.interact(\n",
    "    ps=ps_widget,\n",
    "    cor=cor_widget,\n",
    ")\n",
    "def make_prior_likelihood_plot(ps,cor):\n",
    "    fig = plot_prior_likelihood(ps,cor)\n",
    "    plt.show(fig)\n",
    "    plt.close(fig)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "colab_type": "code",
    "outputId": "85d7fbcc-3e73-4fa8-efcf-fd9163f1cad0"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "video = YouTubeVideo(id='AbXorOLBrws', width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Section 3: Bayesian decisions\n",
    "\n",
    "We will explore how to consider the expected utility of an action based on our belief (the posterior distribution) about where we think the fish are. Now we have all the components of a Bayesian decision: our prior information, the likelihood given a measurement, the posterior distribution (belief) and our utility (the gains and losses). This allows us to consider the relationship between the true value of the hidden state, $s$, and what we *expect* to get if we take action, $a$, based on our beleif!\n",
    "\n",
    "Let's use the following widget to think about the relationship between these probability distributions and utility function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_widget = widgets.FloatSlider(0.5, description='p(s)', min=0.0, max=1.0, step=0.01)\n",
    "p_a_s1_widget = widgets.FloatSlider(0.5, description='p(fish | a = s)', min=0.0, max=1.0, step=0.01)\n",
    "p_a_s0_widget = widgets.FloatSlider(0.5, description='p(fish | a != s)', min=0.0, max=1.0, step=0.01)\n",
    "loss_s_widget = widgets.FloatSlider(0.5, description='loss (a = s)', min=0.0, max=1.0, step=0.01)\n",
    "gain_s_widget = widgets.FloatSlider(0.5, description='gain (a = s)', min=0.0, max=1.0, step=0.01)\n",
    "\n",
    "@widgets.interact(\n",
    "    ps=ps_widget,\n",
    "    p_a_s1=p_a_s1_widget,\n",
    "    p_a_s0=p_a_s0_widget,\n",
    "    loss_s=loss_s_widget,\n",
    "    gain_s=gain_s_widget,\n",
    ")\n",
    "def make_prior_likelihood_utility_plot(ps, p_a_s1, p_a_s0, loss_s, gain_s):\n",
    "    fig = plot_prior_likelihood_utility(ps, p_a_s1, p_a_s0, loss_s, gain_s)\n",
    "    plt.show(fig)\n",
    "    plt.close(fig)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": []
  },
  {
   "source": [
    "# End of tutorial 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D1_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "name": "python3710jvsc74a57bd03e19903e646247cead5404f55ff575624523d45cf244c3f93aaf5fa10367032a",
   "display_name": "Python 3.7.10 64-bit ('nma': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}